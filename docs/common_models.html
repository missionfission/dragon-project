<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>common_models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>common_models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import argparse
import sys

import numpy as np

# rnn_config = {rnn_type : &#34;lstm&#34;, encoder_n_hidden : 1024, encoder_pre_rnn_layers : 2, encoder_stack_time_factor : 2, encoder_post_rnn_layers : 3, pred_n_hidden : 320, pred_rnn_layers : 2, forget_gate_bias : 1.0, joint_n_hidden : 512, dropout:0.32}
import toml
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.models as models

from dlrm.dlrm_s_pytorch import DLRM_Net, dash_separated_floats, dash_separated_ints
from ir.trace import trace
from models.rnnt import RNNT
from models.ssd_rs34 import SSD_R34
from models.Unet import Generic_UNet

# from transformers import (
#     DPRConfig,
#     DPRContextEncoder,
#     DPRQuestionEncoder,
#     DPRReader,
#     DPRReaderTokenizer,
# )


def dlrm_graph():
    import dlrm.dlrm_data_pytorch as dp

    # ### parse arguments ###
    parser = argparse.ArgumentParser(
        description=&#34;Train Deep Learning Recommendation Model (DLRM)&#34;
    )
    # model related parameters
    parser.add_argument(&#34;--arch-sparse-feature-size&#34;, type=int, default=64)
    parser.add_argument(
        &#34;--arch-embedding-size&#34;, type=dash_separated_ints, default=&#34;4-3-2&#34;
    )
    # j will be replaced with the table number
    parser.add_argument(
        &#34;--arch-mlp-bot&#34;, type=dash_separated_ints, default=&#34;13-512-256-64&#34;
    )
    parser.add_argument(
        &#34;--arch-mlp-top&#34;, type=dash_separated_ints, default=&#34;512-512-256-1&#34;
    )
    parser.add_argument(
        &#34;--arch-interaction-op&#34;, type=str, choices=[&#34;dot&#34;, &#34;cat&#34;], default=&#34;dot&#34;
    )
    parser.add_argument(&#34;--arch-interaction-itself&#34;, action=&#34;store_true&#34;, default=False)
    # embedding table options
    parser.add_argument(&#34;--md-flag&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--md-threshold&#34;, type=int, default=200)
    parser.add_argument(&#34;--md-temperature&#34;, type=float, default=0.3)
    parser.add_argument(&#34;--md-round-dims&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--qr-flag&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--qr-threshold&#34;, type=int, default=200)
    parser.add_argument(&#34;--qr-operation&#34;, type=str, default=&#34;mult&#34;)
    parser.add_argument(&#34;--qr-collisions&#34;, type=int, default=4)
    # activations and loss
    parser.add_argument(&#34;--activation-function&#34;, type=str, default=&#34;relu&#34;)
    parser.add_argument(&#34;--dataset-multiprocessing&#34;, type=bool, default=False)
    parser.add_argument(&#34;--loss-function&#34;, type=str, default=&#34;mse&#34;)  # or bce or wbce
    parser.add_argument(
        &#34;--loss-weights&#34;, type=dash_separated_floats, default=&#34;1.0-1.0&#34;
    )  # for wbce
    parser.add_argument(&#34;--loss-threshold&#34;, type=float, default=0.0)  # 1.0e-7
    parser.add_argument(&#34;--round-targets&#34;, type=bool, default=False)
    # data
    parser.add_argument(&#34;--data-size&#34;, type=int, default=1)
    parser.add_argument(&#34;--num-batches&#34;, type=int, default=1)
    parser.add_argument(
        &#34;--data-generation&#34;, type=str, default=&#34;random&#34;
    )  # synthetic or dataset
    parser.add_argument(&#34;--data-trace-file&#34;, type=str, default=&#34;./input/dist_emb_j.log&#34;)
    parser.add_argument(&#34;--data-set&#34;, type=str, default=&#34;kaggle&#34;)  # or terabyte
    parser.add_argument(&#34;--raw-data-file&#34;, type=str, default=&#34;&#34;)
    parser.add_argument(&#34;--processed-data-file&#34;, type=str, default=&#34;&#34;)
    parser.add_argument(&#34;--data-randomize&#34;, type=str, default=&#34;total&#34;)  # or day or none
    parser.add_argument(&#34;--data-trace-enable-padding&#34;, type=bool, default=False)
    parser.add_argument(&#34;--max-ind-range&#34;, type=int, default=-1)
    parser.add_argument(&#34;--data-sub-sample-rate&#34;, type=float, default=0.0)  # in [0, 1]
    parser.add_argument(&#34;--num-indices-per-lookup&#34;, type=int, default=10)
    parser.add_argument(&#34;--num-indices-per-lookup-fixed&#34;, type=bool, default=False)
    parser.add_argument(&#34;--num-workers&#34;, type=int, default=0)
    parser.add_argument(&#34;--memory-map&#34;, action=&#34;store_true&#34;, default=False)
    # training
    parser.add_argument(&#34;--mini-batch-size&#34;, type=int, default=1)
    parser.add_argument(&#34;--nepochs&#34;, type=int, default=1)
    parser.add_argument(&#34;--learning-rate&#34;, type=float, default=0.01)
    parser.add_argument(&#34;--print-precision&#34;, type=int, default=5)
    parser.add_argument(&#34;--numpy-rand-seed&#34;, type=int, default=123)
    parser.add_argument(&#34;--sync-dense-params&#34;, type=bool, default=True)
    # inference
    parser.add_argument(&#34;--inference-only&#34;, action=&#34;store_true&#34;, default=False)
    # onnx
    parser.add_argument(&#34;--save-onnx&#34;, action=&#34;store_true&#34;, default=False)
    # gpu
    parser.add_argument(&#34;--use-gpu&#34;, action=&#34;store_true&#34;, default=False)
    # debugging and profiling
    parser.add_argument(&#34;--print-freq&#34;, type=int, default=1)
    parser.add_argument(&#34;--test-freq&#34;, type=int, default=-1)
    parser.add_argument(&#34;--test-mini-batch-size&#34;, type=int, default=-1)
    parser.add_argument(&#34;--test-num-workers&#34;, type=int, default=-1)
    parser.add_argument(&#34;--print-time&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--debug-mode&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--enable-profiling&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--plot-compute-graph&#34;, action=&#34;store_true&#34;, default=False)
    # store/load model
    parser.add_argument(&#34;--save-model&#34;, type=str, default=&#34;&#34;)
    parser.add_argument(&#34;--load-model&#34;, type=str, default=&#34;&#34;)
    # mlperf logging (disables other output and stops early)
    parser.add_argument(&#34;--mlperf-logging&#34;, action=&#34;store_true&#34;, default=False)
    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107
    parser.add_argument(&#34;--mlperf-acc-threshold&#34;, type=float, default=0.0)
    # stop at target AUC Terabyte (no subsampling) 0.8025
    parser.add_argument(&#34;--mlperf-auc-threshold&#34;, type=float, default=0.0)
    parser.add_argument(&#34;--mlperf-bin-loader&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--mlperf-bin-shuffle&#34;, action=&#34;store_true&#34;, default=False)
    # LR policy
    parser.add_argument(&#34;--lr-num-warmup-steps&#34;, type=int, default=0)
    parser.add_argument(&#34;--lr-decay-start-step&#34;, type=int, default=0)
    parser.add_argument(&#34;--lr-num-decay-steps&#34;, type=int, default=0)
    global args

    args = parser.parse_args([])

    # if args.mlperf_logging:
    #     print(&#34;command line args: &#34;, json.dumps(vars(args)))

    ### some basic setup ###
    np.random.seed(args.numpy_rand_seed)
    np.set_printoptions(precision=args.print_precision)
    torch.set_printoptions(precision=args.print_precision)
    torch.manual_seed(args.numpy_rand_seed)

    if args.test_mini_batch_size &lt; 0:
        # if the parameter is not set, use the training batch size
        args.test_mini_batch_size = args.mini_batch_size
    if args.test_num_workers &lt; 0:
        # if the parameter is not set, use the same parameter for training
        args.test_num_workers = args.num_workers

    use_gpu = args.use_gpu and torch.cuda.is_available()
    if use_gpu:
        torch.cuda.manual_seed_all(args.numpy_rand_seed)
        torch.backends.cudnn.deterministic = True
        device = torch.device(&#34;cuda&#34;, 0)
        ngpus = torch.cuda.device_count()  # 1
        print(&#34;Using {} GPU(s)...&#34;.format(ngpus))
    else:
        device = torch.device(&#34;cpu&#34;)
        print(&#34;Using CPU...&#34;)

    # ### prepare training data ###
    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=&#34;-&#34;)

    # input data
    if args.data_generation == &#34;dataset&#34;:

        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
        nbatches = args.num_batches if args.num_batches &gt; 0 else len(train_ld)
        nbatches_test = len(test_ld)

        ln_emb = train_data.counts
        # enforce maximum limit on number of vectors per embedding
        if args.max_ind_range &gt; 0:
            ln_emb = np.array(
                list(
                    map(
                        lambda x: x if x &lt; args.max_ind_range else args.max_ind_range,
                        ln_emb,
                    )
                )
            )
        m_den = train_data.m_den
        ln_bot[0] = m_den
    else:
        # input and target at random
        # ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=&#34;-&#34;)
        ln_emb = np.array(
            [
                9980333,
                36084,
                17217,
                7378,
                20134,
                3,
                7112,
                1442,
                61,
                9758201,
                1333352,
                313829,
                10,
                2208,
                11156,
                122,
                4,
                970,
                14,
                9994222,
                7267859,
                9946608,
                415421,
                12420,
                101,
                36,
            ]
        )

        m_den = ln_bot[0]
        train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
        nbatches = 1
    print(&#34;Num batches &#34;, nbatches)
    # ### parse command line arguments ###
    m_spa = args.arch_sparse_feature_size
    num_fea = ln_emb.size + 1  # num sparse + num dense features
    m_den_out = ln_bot[ln_bot.size - 1]
    if args.arch_interaction_op == &#34;dot&#34;:
        # approach 1: all
        # num_int = num_fea * num_fea + m_den_out
        # approach 2: unique
        if args.arch_interaction_itself:
            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out
        else:
            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out
    elif args.arch_interaction_op == &#34;cat&#34;:
        num_int = num_fea * m_den_out
    else:
        sys.exit(
            &#34;ERROR: --arch-interaction-op=&#34;
            + args.arch_interaction_op
            + &#34; is not supported&#34;
        )
    arch_mlp_top_adjusted = str(num_int) + &#34;-&#34; + args.arch_mlp_top
    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=&#34;-&#34;)

    # sanity check: feature sizes and mlp dimensions must match
    if m_den != ln_bot[0]:
        sys.exit(
            &#34;ERROR: arch-dense-feature-size &#34;
            + str(m_den)
            + &#34; does not match first dim of bottom mlp &#34;
            + str(ln_bot[0])
        )
    if args.qr_flag:
        if args.qr_operation == &#34;concat&#34; and 2 * m_spa != m_den_out:
            sys.exit(
                &#34;ERROR: 2 arch-sparse-feature-size &#34;
                + str(2 * m_spa)
                + &#34; does not match last dim of bottom mlp &#34;
                + str(m_den_out)
                + &#34; (note that the last dim of bottom mlp must be 2x the embedding dim)&#34;
            )
        if args.qr_operation != &#34;concat&#34; and m_spa != m_den_out:
            sys.exit(
                &#34;ERROR: arch-sparse-feature-size &#34;
                + str(m_spa)
                + &#34; does not match last dim of bottom mlp &#34;
                + str(m_den_out)
            )
    else:
        if m_spa != m_den_out:
            sys.exit(
                &#34;ERROR: arch-sparse-feature-size &#34;
                + str(m_spa)
                + &#34; does not match last dim of bottom mlp &#34;
                + str(m_den_out)
            )
    if num_int != ln_top[0]:
        sys.exit(
            &#34;ERROR: # of feature interactions &#34;
            + str(num_int)
            + &#34; does not match first dimension of top mlp &#34;
            + str(ln_top[0])
        )

    # ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1
    ndevices = -1
    # ### construct the neural network specified above ###
    # # WARNING: to obtain exactly the same initialization for
    # # the weights we need to start from the same random seed.
    # # np.random.seed(args.numpy_rand_seed)
    m_spa = 64
    ln_emb = np.array(
        [
            9980333,
            36084,
            17217,
            7378,
            20134,
            3,
            7112,
            1442,
            61,
            9758201,
            1333352,
            313829,
            10,
            2208,
            11156,
            122,
            4,
            970,
            14,
            9994222,
            7267859,
            9946608,
            415421,
            12420,
            101,
            36,
        ]
    )
    ln_bot = np.array([13, 512, 256, 64])
    ln_top = np.array([415, 512, 512, 256, 1])

    dlrm = DLRM_Net(
        m_spa,
        ln_emb,
        ln_bot,
        ln_top,
        arch_interaction_op=args.arch_interaction_op,
        arch_interaction_itself=args.arch_interaction_itself,
        sigmoid_bot=-1,
        sigmoid_top=ln_top.size - 2,
        sync_dense_params=args.sync_dense_params,
        loss_threshold=args.loss_threshold,
        ndevices=ndevices,
        qr_flag=args.qr_flag,
        qr_operation=args.qr_operation,
        qr_collisions=args.qr_collisions,
        qr_threshold=args.qr_threshold,
        md_flag=args.md_flag,
        md_threshold=args.md_threshold,
    )
    # print(train_ld)
    for j, (X, lS_o, lS_i, T) in enumerate(train_ld):
        pass
    dlrm_graph = trace(dlrm, (X, lS_o, lS_i))
    print(dlrm_graph)
    return dlrm_graph


def resnet_18_graph():
    for name, model in models.__dict__.items():
        #     print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            continue
        if &#34;resnet18&#34; in name:
            print(name)
            model = model().eval()
            inputs = torch.randn(1, 3, 224, 224)
            resnet_graph = trace(model, inputs)
            break
    return resnet_graph


def resnet_50_graph():
    for name, model in models.__dict__.items():
        #     print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            continue
        if &#34;resnet50&#34; in name:
            print(name)
            model = model().eval()
            inputs = torch.randn(64, 3, 224, 224)
            resnet_graph = trace(model, inputs)
            break
    return resnet_graph


def vggnet_graph():
    for name, model in models.__dict__.items():
        #     print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            continue
        if &#34;vgg11&#34; in name and &#34;vgg11_bn&#34; not in name:
            inputs = torch.randn(1, 3, 224, 224)
            vgg11_graph = trace(model().eval(), inputs)
            # print(vgg11_graph)
            break
    return vgg11_graph


def bert_graph():
    from transformers import BertModel, BertConfig

    # model.configMM

    # tokenizer = torch.hub.load(&#39;huggingface/pytorch-pretrained-BERT&#39;, &#39;tokenizer&#39;, &#39;bert-base-cased&#39;, do_basic_tokenize=False)

    # Tokenized input
    # text = &#34;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&#34;
    # tokenized_text = tokenizer.tokenize(text)
    # indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    # print(indexed_tokens)
    ### Get the hidden states computed by `bertModel`
    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)
    segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
    indexed_tokens = [
        101,
        2627,
        1108,
        3104,
        1124,
        15703,
        136,
        102,
        3104,
        1124,
        15703,
        1108,
        170,
        16797,
        8284,
        102,
    ]

    # Convert inputs to PyTorch tensors
    segments_tensors = torch.tensor([segments_ids])
    tokens_tensor = torch.tensor([indexed_tokens])

    # model = torch.hub.load(&#39;huggingface/pytorch-pretrained-BERT&#39;, &#39;model&#39;, &#39;bert-base-cased&#39;)
    configuration = BertConfig()
    model = BertModel(configuration)
    model.eval()

    model(tokens_tensor)
    bert_graph = trace(model, tokens_tensor)
    return bert_graph


def dpr_graph():

    segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
    indexed_tokens = [
        101,
        2627,
        1108,
        3104,
        1124,
        15703,
        136,
        102,
        3104,
        1124,
        15703,
        1108,
        170,
        16797,
        8284,
        102,
    ]

    # Convert inputs to PyTorch tensors
    segments_tensors = torch.tensor([segments_ids])
    tokens_tensor = torch.tensor([indexed_tokens])
    configuration = DPRConfig()
    contextencoder_graph = trace(DPRContextEncoder(configuration).eval(), tokens_tensor)
    questionencoder_graph = trace(
        DPRQuestionEncoder(configuration).eval(), tokens_tensor
    )
    reader_graph = trace(DPRReader(configuration).eval(), tokens_tensor)
    return contextencoder_graph, questionencoder_graph, reader_graph


def gpt2_graph():
    from transformers import GPT2Model, GPT2Config

    segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
    indexed_tokens = [
        101,
        2627,
        1108,
        3104,
        1124,
        15703,
        136,
        102,
        3104,
        1124,
        15703,
        1108,
        170,
        16797,
        8284,
        102,
    ]

    # Convert inputs to PyTorch tensors
    segments_tensors = torch.tensor([segments_ids])
    tokens_tensor = torch.tensor([indexed_tokens])
    configuration = GPT2Config()
    model = GPT2Model(configuration)
    model.eval()

    model(tokens_tensor)
    gpt2_graph = trace(model, tokens_tensor)
    return gpt2_graph


def alexnet_graph():
    import torchvision.models as models

    for name, model in models.__dict__.items():
        #             print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            #             print(name.islower())
            continue
        #         print(name)
        if &#34;alexnet&#34; in name:
            model = model().eval()
            inputs = torch.randn(1, 3, 224, 224)
            alexnet_graph = trace(model, inputs)
            break
    return alexnet_graph


class LSTMTagger(nn.Module):
    def __init__(
        self, embedding_dim=24, hidden_dim=2048, vocab_size=32768, tagset_size=1024
    ):
        super(LSTMTagger, self).__init__()
        self.hidden_dim = hidden_dim

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        # The linear layer that maps from hidden state space to tag space
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_space, dim=1)
        return tag_scores


def langmodel_graph():

    # torch.manual_seed(1)
    # lstm = nn.LSTM(793470, 1024)  # Input dim is 3, output dim is 3
    # inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5
    # # initialize the hidden state.
    # hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))
    # inputs = torch.cat(inputs).view(len(inputs), 1, -1)
    # hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state
    # # out, hidden = lstm(inputs, hidden)

    # LAYERS[&#34;embed1&#34;] = FCLayer(1, 793470, 1)
    # LAYERS[&#34;act_1&#34;] = FCLayer(2048, 32768, 1)
    # LAYERS[&#34;proj_1&#34;] = FCLayer(8192, 1024, 1)
    inputs = torch.randn(2048)  # make a sequence of length 5
    model1 = nn.Sequential(nn.Linear(2048, 32768))
    model2 = nn.Sequential(nn.Linear(8192, 1024))

    langmod_graph1 = trace(model1.eval(), inputs)
    inputs = torch.randn(8192)  # make a sequence of length 5
    langmod_graph2 = trace(model2.eval(), inputs)
    return langmod_graph1, langmod_graph2


class InitWeights_He(object):
    def __init__(self, neg_slope=1e-2):
        self.neg_slope = neg_slope

    def __call__(self, module):
        if (
            isinstance(module, nn.Conv3d)
            or isinstance(module, nn.Conv2d)
            or isinstance(module, nn.ConvTranspose2d)
            or isinstance(module, nn.ConvTranspose3d)
        ):
            module.weight = nn.init.kaiming_normal_(module.weight, a=self.neg_slope)
            if module.bias is not None:
                module.bias = nn.init.constant_(module.bias, 0)


def Unet():
    conv_op = nn.Conv3d
    dropout_op = nn.Dropout3d
    norm_op = nn.InstanceNorm3d

    inputs = torch.randn(1, 160, 224, 224)

    norm_op_kwargs = {&#34;eps&#34;: 1e-5, &#34;affine&#34;: True}
    dropout_op_kwargs = {&#34;p&#34;: 0, &#34;inplace&#34;: True}
    net_nonlin = nn.LeakyReLU
    net_nonlin_kwargs = {&#34;negative_slope&#34;: 1e-2, &#34;inplace&#34;: True}
    net_num_pool_op_kernel_sizes = [
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
    ]
    net_conv_kernel_sizes = [
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
    ]
    # loaded automatically from plans_file
    model = Generic_UNet(
        160,
        24,
        16,
        len(net_num_pool_op_kernel_sizes),
        1,
        2,
        conv_op,
        norm_op,
        norm_op_kwargs,
        dropout_op,
        dropout_op_kwargs,
        net_nonlin,
        net_nonlin_kwargs,
        True,
        False,
        lambda x: x,
        InitWeights_He(1e-2),
        net_num_pool_op_kernel_sizes,
        net_conv_kernel_sizes,
        False,
        True,
        True,
    )

    unet_Graph = trace(model.eval(), inputs)
    # return unet_Graph
    return 0


# featurizer_config = dict({sample_rate : 16000, window_size : 0.02, window_stride : 0.01, window : &#34;hann&#34;, features : 80, n_fft : 512,frame_splicing : 3, dither : 0.00001, feat_type : &#34;logfbank&#34;, normalize_transcripts : true, trim_silence : true, pad_to : 0})


def speech2text_model():
    config = toml.load(&#34;rnnt.toml&#34;)
    featurizer_config = config[&#34;input_eval&#34;]
    model = RNNT(feature_config=featurizer_config, rnnt=config[&#34;rnnt&#34;], num_classes=29)
    seq_length, batch_size, feature_length = 157, 1, 240
    inp = torch.randn([seq_length, batch_size, feature_length])
    feature_length = torch.LongTensor([seq_length])
    x_padded, x_lens = model.encoder(inp, feature_length)
    speech2text_graph = trace(model.eval(), (inp, feature_length))
    return speech2text_graph


def objectdetection_model():
    model = SSD_R34().model
    inputs = torch.randn(1, 3, 1200, 1200)
    objectdetection_graph = trace(model.eval(), inputs)
    return objectdetection_graph


# 224x224
# 1200x1200
# 224x224x160,16
# bert max_seq_len=384</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="common_models.Unet"><code class="name flex">
<span>def <span class="ident">Unet</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Unet():
    conv_op = nn.Conv3d
    dropout_op = nn.Dropout3d
    norm_op = nn.InstanceNorm3d

    inputs = torch.randn(1, 160, 224, 224)

    norm_op_kwargs = {&#34;eps&#34;: 1e-5, &#34;affine&#34;: True}
    dropout_op_kwargs = {&#34;p&#34;: 0, &#34;inplace&#34;: True}
    net_nonlin = nn.LeakyReLU
    net_nonlin_kwargs = {&#34;negative_slope&#34;: 1e-2, &#34;inplace&#34;: True}
    net_num_pool_op_kernel_sizes = [
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
        [2, 2],
    ]
    net_conv_kernel_sizes = [
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
        [3, 3],
    ]
    # loaded automatically from plans_file
    model = Generic_UNet(
        160,
        24,
        16,
        len(net_num_pool_op_kernel_sizes),
        1,
        2,
        conv_op,
        norm_op,
        norm_op_kwargs,
        dropout_op,
        dropout_op_kwargs,
        net_nonlin,
        net_nonlin_kwargs,
        True,
        False,
        lambda x: x,
        InitWeights_He(1e-2),
        net_num_pool_op_kernel_sizes,
        net_conv_kernel_sizes,
        False,
        True,
        True,
    )

    unet_Graph = trace(model.eval(), inputs)
    # return unet_Graph
    return 0</code></pre>
</details>
</dd>
<dt id="common_models.alexnet_graph"><code class="name flex">
<span>def <span class="ident">alexnet_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alexnet_graph():
    import torchvision.models as models

    for name, model in models.__dict__.items():
        #             print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            #             print(name.islower())
            continue
        #         print(name)
        if &#34;alexnet&#34; in name:
            model = model().eval()
            inputs = torch.randn(1, 3, 224, 224)
            alexnet_graph = trace(model, inputs)
            break
    return alexnet_graph</code></pre>
</details>
</dd>
<dt id="common_models.bert_graph"><code class="name flex">
<span>def <span class="ident">bert_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bert_graph():
    from transformers import BertModel, BertConfig

    # model.configMM

    # tokenizer = torch.hub.load(&#39;huggingface/pytorch-pretrained-BERT&#39;, &#39;tokenizer&#39;, &#39;bert-base-cased&#39;, do_basic_tokenize=False)

    # Tokenized input
    # text = &#34;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&#34;
    # tokenized_text = tokenizer.tokenize(text)
    # indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    # print(indexed_tokens)
    ### Get the hidden states computed by `bertModel`
    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)
    segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
    indexed_tokens = [
        101,
        2627,
        1108,
        3104,
        1124,
        15703,
        136,
        102,
        3104,
        1124,
        15703,
        1108,
        170,
        16797,
        8284,
        102,
    ]

    # Convert inputs to PyTorch tensors
    segments_tensors = torch.tensor([segments_ids])
    tokens_tensor = torch.tensor([indexed_tokens])

    # model = torch.hub.load(&#39;huggingface/pytorch-pretrained-BERT&#39;, &#39;model&#39;, &#39;bert-base-cased&#39;)
    configuration = BertConfig()
    model = BertModel(configuration)
    model.eval()

    model(tokens_tensor)
    bert_graph = trace(model, tokens_tensor)
    return bert_graph</code></pre>
</details>
</dd>
<dt id="common_models.dlrm_graph"><code class="name flex">
<span>def <span class="ident">dlrm_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dlrm_graph():
    import dlrm.dlrm_data_pytorch as dp

    # ### parse arguments ###
    parser = argparse.ArgumentParser(
        description=&#34;Train Deep Learning Recommendation Model (DLRM)&#34;
    )
    # model related parameters
    parser.add_argument(&#34;--arch-sparse-feature-size&#34;, type=int, default=64)
    parser.add_argument(
        &#34;--arch-embedding-size&#34;, type=dash_separated_ints, default=&#34;4-3-2&#34;
    )
    # j will be replaced with the table number
    parser.add_argument(
        &#34;--arch-mlp-bot&#34;, type=dash_separated_ints, default=&#34;13-512-256-64&#34;
    )
    parser.add_argument(
        &#34;--arch-mlp-top&#34;, type=dash_separated_ints, default=&#34;512-512-256-1&#34;
    )
    parser.add_argument(
        &#34;--arch-interaction-op&#34;, type=str, choices=[&#34;dot&#34;, &#34;cat&#34;], default=&#34;dot&#34;
    )
    parser.add_argument(&#34;--arch-interaction-itself&#34;, action=&#34;store_true&#34;, default=False)
    # embedding table options
    parser.add_argument(&#34;--md-flag&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--md-threshold&#34;, type=int, default=200)
    parser.add_argument(&#34;--md-temperature&#34;, type=float, default=0.3)
    parser.add_argument(&#34;--md-round-dims&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--qr-flag&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--qr-threshold&#34;, type=int, default=200)
    parser.add_argument(&#34;--qr-operation&#34;, type=str, default=&#34;mult&#34;)
    parser.add_argument(&#34;--qr-collisions&#34;, type=int, default=4)
    # activations and loss
    parser.add_argument(&#34;--activation-function&#34;, type=str, default=&#34;relu&#34;)
    parser.add_argument(&#34;--dataset-multiprocessing&#34;, type=bool, default=False)
    parser.add_argument(&#34;--loss-function&#34;, type=str, default=&#34;mse&#34;)  # or bce or wbce
    parser.add_argument(
        &#34;--loss-weights&#34;, type=dash_separated_floats, default=&#34;1.0-1.0&#34;
    )  # for wbce
    parser.add_argument(&#34;--loss-threshold&#34;, type=float, default=0.0)  # 1.0e-7
    parser.add_argument(&#34;--round-targets&#34;, type=bool, default=False)
    # data
    parser.add_argument(&#34;--data-size&#34;, type=int, default=1)
    parser.add_argument(&#34;--num-batches&#34;, type=int, default=1)
    parser.add_argument(
        &#34;--data-generation&#34;, type=str, default=&#34;random&#34;
    )  # synthetic or dataset
    parser.add_argument(&#34;--data-trace-file&#34;, type=str, default=&#34;./input/dist_emb_j.log&#34;)
    parser.add_argument(&#34;--data-set&#34;, type=str, default=&#34;kaggle&#34;)  # or terabyte
    parser.add_argument(&#34;--raw-data-file&#34;, type=str, default=&#34;&#34;)
    parser.add_argument(&#34;--processed-data-file&#34;, type=str, default=&#34;&#34;)
    parser.add_argument(&#34;--data-randomize&#34;, type=str, default=&#34;total&#34;)  # or day or none
    parser.add_argument(&#34;--data-trace-enable-padding&#34;, type=bool, default=False)
    parser.add_argument(&#34;--max-ind-range&#34;, type=int, default=-1)
    parser.add_argument(&#34;--data-sub-sample-rate&#34;, type=float, default=0.0)  # in [0, 1]
    parser.add_argument(&#34;--num-indices-per-lookup&#34;, type=int, default=10)
    parser.add_argument(&#34;--num-indices-per-lookup-fixed&#34;, type=bool, default=False)
    parser.add_argument(&#34;--num-workers&#34;, type=int, default=0)
    parser.add_argument(&#34;--memory-map&#34;, action=&#34;store_true&#34;, default=False)
    # training
    parser.add_argument(&#34;--mini-batch-size&#34;, type=int, default=1)
    parser.add_argument(&#34;--nepochs&#34;, type=int, default=1)
    parser.add_argument(&#34;--learning-rate&#34;, type=float, default=0.01)
    parser.add_argument(&#34;--print-precision&#34;, type=int, default=5)
    parser.add_argument(&#34;--numpy-rand-seed&#34;, type=int, default=123)
    parser.add_argument(&#34;--sync-dense-params&#34;, type=bool, default=True)
    # inference
    parser.add_argument(&#34;--inference-only&#34;, action=&#34;store_true&#34;, default=False)
    # onnx
    parser.add_argument(&#34;--save-onnx&#34;, action=&#34;store_true&#34;, default=False)
    # gpu
    parser.add_argument(&#34;--use-gpu&#34;, action=&#34;store_true&#34;, default=False)
    # debugging and profiling
    parser.add_argument(&#34;--print-freq&#34;, type=int, default=1)
    parser.add_argument(&#34;--test-freq&#34;, type=int, default=-1)
    parser.add_argument(&#34;--test-mini-batch-size&#34;, type=int, default=-1)
    parser.add_argument(&#34;--test-num-workers&#34;, type=int, default=-1)
    parser.add_argument(&#34;--print-time&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--debug-mode&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--enable-profiling&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--plot-compute-graph&#34;, action=&#34;store_true&#34;, default=False)
    # store/load model
    parser.add_argument(&#34;--save-model&#34;, type=str, default=&#34;&#34;)
    parser.add_argument(&#34;--load-model&#34;, type=str, default=&#34;&#34;)
    # mlperf logging (disables other output and stops early)
    parser.add_argument(&#34;--mlperf-logging&#34;, action=&#34;store_true&#34;, default=False)
    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107
    parser.add_argument(&#34;--mlperf-acc-threshold&#34;, type=float, default=0.0)
    # stop at target AUC Terabyte (no subsampling) 0.8025
    parser.add_argument(&#34;--mlperf-auc-threshold&#34;, type=float, default=0.0)
    parser.add_argument(&#34;--mlperf-bin-loader&#34;, action=&#34;store_true&#34;, default=False)
    parser.add_argument(&#34;--mlperf-bin-shuffle&#34;, action=&#34;store_true&#34;, default=False)
    # LR policy
    parser.add_argument(&#34;--lr-num-warmup-steps&#34;, type=int, default=0)
    parser.add_argument(&#34;--lr-decay-start-step&#34;, type=int, default=0)
    parser.add_argument(&#34;--lr-num-decay-steps&#34;, type=int, default=0)
    global args

    args = parser.parse_args([])

    # if args.mlperf_logging:
    #     print(&#34;command line args: &#34;, json.dumps(vars(args)))

    ### some basic setup ###
    np.random.seed(args.numpy_rand_seed)
    np.set_printoptions(precision=args.print_precision)
    torch.set_printoptions(precision=args.print_precision)
    torch.manual_seed(args.numpy_rand_seed)

    if args.test_mini_batch_size &lt; 0:
        # if the parameter is not set, use the training batch size
        args.test_mini_batch_size = args.mini_batch_size
    if args.test_num_workers &lt; 0:
        # if the parameter is not set, use the same parameter for training
        args.test_num_workers = args.num_workers

    use_gpu = args.use_gpu and torch.cuda.is_available()
    if use_gpu:
        torch.cuda.manual_seed_all(args.numpy_rand_seed)
        torch.backends.cudnn.deterministic = True
        device = torch.device(&#34;cuda&#34;, 0)
        ngpus = torch.cuda.device_count()  # 1
        print(&#34;Using {} GPU(s)...&#34;.format(ngpus))
    else:
        device = torch.device(&#34;cpu&#34;)
        print(&#34;Using CPU...&#34;)

    # ### prepare training data ###
    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=&#34;-&#34;)

    # input data
    if args.data_generation == &#34;dataset&#34;:

        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
        nbatches = args.num_batches if args.num_batches &gt; 0 else len(train_ld)
        nbatches_test = len(test_ld)

        ln_emb = train_data.counts
        # enforce maximum limit on number of vectors per embedding
        if args.max_ind_range &gt; 0:
            ln_emb = np.array(
                list(
                    map(
                        lambda x: x if x &lt; args.max_ind_range else args.max_ind_range,
                        ln_emb,
                    )
                )
            )
        m_den = train_data.m_den
        ln_bot[0] = m_den
    else:
        # input and target at random
        # ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=&#34;-&#34;)
        ln_emb = np.array(
            [
                9980333,
                36084,
                17217,
                7378,
                20134,
                3,
                7112,
                1442,
                61,
                9758201,
                1333352,
                313829,
                10,
                2208,
                11156,
                122,
                4,
                970,
                14,
                9994222,
                7267859,
                9946608,
                415421,
                12420,
                101,
                36,
            ]
        )

        m_den = ln_bot[0]
        train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)
        nbatches = 1
    print(&#34;Num batches &#34;, nbatches)
    # ### parse command line arguments ###
    m_spa = args.arch_sparse_feature_size
    num_fea = ln_emb.size + 1  # num sparse + num dense features
    m_den_out = ln_bot[ln_bot.size - 1]
    if args.arch_interaction_op == &#34;dot&#34;:
        # approach 1: all
        # num_int = num_fea * num_fea + m_den_out
        # approach 2: unique
        if args.arch_interaction_itself:
            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out
        else:
            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out
    elif args.arch_interaction_op == &#34;cat&#34;:
        num_int = num_fea * m_den_out
    else:
        sys.exit(
            &#34;ERROR: --arch-interaction-op=&#34;
            + args.arch_interaction_op
            + &#34; is not supported&#34;
        )
    arch_mlp_top_adjusted = str(num_int) + &#34;-&#34; + args.arch_mlp_top
    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=&#34;-&#34;)

    # sanity check: feature sizes and mlp dimensions must match
    if m_den != ln_bot[0]:
        sys.exit(
            &#34;ERROR: arch-dense-feature-size &#34;
            + str(m_den)
            + &#34; does not match first dim of bottom mlp &#34;
            + str(ln_bot[0])
        )
    if args.qr_flag:
        if args.qr_operation == &#34;concat&#34; and 2 * m_spa != m_den_out:
            sys.exit(
                &#34;ERROR: 2 arch-sparse-feature-size &#34;
                + str(2 * m_spa)
                + &#34; does not match last dim of bottom mlp &#34;
                + str(m_den_out)
                + &#34; (note that the last dim of bottom mlp must be 2x the embedding dim)&#34;
            )
        if args.qr_operation != &#34;concat&#34; and m_spa != m_den_out:
            sys.exit(
                &#34;ERROR: arch-sparse-feature-size &#34;
                + str(m_spa)
                + &#34; does not match last dim of bottom mlp &#34;
                + str(m_den_out)
            )
    else:
        if m_spa != m_den_out:
            sys.exit(
                &#34;ERROR: arch-sparse-feature-size &#34;
                + str(m_spa)
                + &#34; does not match last dim of bottom mlp &#34;
                + str(m_den_out)
            )
    if num_int != ln_top[0]:
        sys.exit(
            &#34;ERROR: # of feature interactions &#34;
            + str(num_int)
            + &#34; does not match first dimension of top mlp &#34;
            + str(ln_top[0])
        )

    # ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1
    ndevices = -1
    # ### construct the neural network specified above ###
    # # WARNING: to obtain exactly the same initialization for
    # # the weights we need to start from the same random seed.
    # # np.random.seed(args.numpy_rand_seed)
    m_spa = 64
    ln_emb = np.array(
        [
            9980333,
            36084,
            17217,
            7378,
            20134,
            3,
            7112,
            1442,
            61,
            9758201,
            1333352,
            313829,
            10,
            2208,
            11156,
            122,
            4,
            970,
            14,
            9994222,
            7267859,
            9946608,
            415421,
            12420,
            101,
            36,
        ]
    )
    ln_bot = np.array([13, 512, 256, 64])
    ln_top = np.array([415, 512, 512, 256, 1])

    dlrm = DLRM_Net(
        m_spa,
        ln_emb,
        ln_bot,
        ln_top,
        arch_interaction_op=args.arch_interaction_op,
        arch_interaction_itself=args.arch_interaction_itself,
        sigmoid_bot=-1,
        sigmoid_top=ln_top.size - 2,
        sync_dense_params=args.sync_dense_params,
        loss_threshold=args.loss_threshold,
        ndevices=ndevices,
        qr_flag=args.qr_flag,
        qr_operation=args.qr_operation,
        qr_collisions=args.qr_collisions,
        qr_threshold=args.qr_threshold,
        md_flag=args.md_flag,
        md_threshold=args.md_threshold,
    )
    # print(train_ld)
    for j, (X, lS_o, lS_i, T) in enumerate(train_ld):
        pass
    dlrm_graph = trace(dlrm, (X, lS_o, lS_i))
    print(dlrm_graph)
    return dlrm_graph</code></pre>
</details>
</dd>
<dt id="common_models.dpr_graph"><code class="name flex">
<span>def <span class="ident">dpr_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dpr_graph():

    segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
    indexed_tokens = [
        101,
        2627,
        1108,
        3104,
        1124,
        15703,
        136,
        102,
        3104,
        1124,
        15703,
        1108,
        170,
        16797,
        8284,
        102,
    ]

    # Convert inputs to PyTorch tensors
    segments_tensors = torch.tensor([segments_ids])
    tokens_tensor = torch.tensor([indexed_tokens])
    configuration = DPRConfig()
    contextencoder_graph = trace(DPRContextEncoder(configuration).eval(), tokens_tensor)
    questionencoder_graph = trace(
        DPRQuestionEncoder(configuration).eval(), tokens_tensor
    )
    reader_graph = trace(DPRReader(configuration).eval(), tokens_tensor)
    return contextencoder_graph, questionencoder_graph, reader_graph</code></pre>
</details>
</dd>
<dt id="common_models.gpt2_graph"><code class="name flex">
<span>def <span class="ident">gpt2_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gpt2_graph():
    from transformers import GPT2Model, GPT2Config

    segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
    indexed_tokens = [
        101,
        2627,
        1108,
        3104,
        1124,
        15703,
        136,
        102,
        3104,
        1124,
        15703,
        1108,
        170,
        16797,
        8284,
        102,
    ]

    # Convert inputs to PyTorch tensors
    segments_tensors = torch.tensor([segments_ids])
    tokens_tensor = torch.tensor([indexed_tokens])
    configuration = GPT2Config()
    model = GPT2Model(configuration)
    model.eval()

    model(tokens_tensor)
    gpt2_graph = trace(model, tokens_tensor)
    return gpt2_graph</code></pre>
</details>
</dd>
<dt id="common_models.langmodel_graph"><code class="name flex">
<span>def <span class="ident">langmodel_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def langmodel_graph():

    # torch.manual_seed(1)
    # lstm = nn.LSTM(793470, 1024)  # Input dim is 3, output dim is 3
    # inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5
    # # initialize the hidden state.
    # hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))
    # inputs = torch.cat(inputs).view(len(inputs), 1, -1)
    # hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state
    # # out, hidden = lstm(inputs, hidden)

    # LAYERS[&#34;embed1&#34;] = FCLayer(1, 793470, 1)
    # LAYERS[&#34;act_1&#34;] = FCLayer(2048, 32768, 1)
    # LAYERS[&#34;proj_1&#34;] = FCLayer(8192, 1024, 1)
    inputs = torch.randn(2048)  # make a sequence of length 5
    model1 = nn.Sequential(nn.Linear(2048, 32768))
    model2 = nn.Sequential(nn.Linear(8192, 1024))

    langmod_graph1 = trace(model1.eval(), inputs)
    inputs = torch.randn(8192)  # make a sequence of length 5
    langmod_graph2 = trace(model2.eval(), inputs)
    return langmod_graph1, langmod_graph2</code></pre>
</details>
</dd>
<dt id="common_models.objectdetection_model"><code class="name flex">
<span>def <span class="ident">objectdetection_model</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def objectdetection_model():
    model = SSD_R34().model
    inputs = torch.randn(1, 3, 1200, 1200)
    objectdetection_graph = trace(model.eval(), inputs)
    return objectdetection_graph</code></pre>
</details>
</dd>
<dt id="common_models.resnet_18_graph"><code class="name flex">
<span>def <span class="ident">resnet_18_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_18_graph():
    for name, model in models.__dict__.items():
        #     print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            continue
        if &#34;resnet18&#34; in name:
            print(name)
            model = model().eval()
            inputs = torch.randn(1, 3, 224, 224)
            resnet_graph = trace(model, inputs)
            break
    return resnet_graph</code></pre>
</details>
</dd>
<dt id="common_models.resnet_50_graph"><code class="name flex">
<span>def <span class="ident">resnet_50_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_50_graph():
    for name, model in models.__dict__.items():
        #     print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            continue
        if &#34;resnet50&#34; in name:
            print(name)
            model = model().eval()
            inputs = torch.randn(64, 3, 224, 224)
            resnet_graph = trace(model, inputs)
            break
    return resnet_graph</code></pre>
</details>
</dd>
<dt id="common_models.speech2text_model"><code class="name flex">
<span>def <span class="ident">speech2text_model</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def speech2text_model():
    config = toml.load(&#34;rnnt.toml&#34;)
    featurizer_config = config[&#34;input_eval&#34;]
    model = RNNT(feature_config=featurizer_config, rnnt=config[&#34;rnnt&#34;], num_classes=29)
    seq_length, batch_size, feature_length = 157, 1, 240
    inp = torch.randn([seq_length, batch_size, feature_length])
    feature_length = torch.LongTensor([seq_length])
    x_padded, x_lens = model.encoder(inp, feature_length)
    speech2text_graph = trace(model.eval(), (inp, feature_length))
    return speech2text_graph</code></pre>
</details>
</dd>
<dt id="common_models.vggnet_graph"><code class="name flex">
<span>def <span class="ident">vggnet_graph</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vggnet_graph():
    for name, model in models.__dict__.items():
        #     print(name)
        if not name.islower() or name.startswith(&#34;__&#34;) or not callable(model):
            continue
        if &#34;vgg11&#34; in name and &#34;vgg11_bn&#34; not in name:
            inputs = torch.randn(1, 3, 224, 224)
            vgg11_graph = trace(model().eval(), inputs)
            # print(vgg11_graph)
            break
    return vgg11_graph</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="common_models.InitWeights_He"><code class="flex name class">
<span>class <span class="ident">InitWeights_He</span></span>
<span>(</span><span>neg_slope=0.01)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InitWeights_He(object):
    def __init__(self, neg_slope=1e-2):
        self.neg_slope = neg_slope

    def __call__(self, module):
        if (
            isinstance(module, nn.Conv3d)
            or isinstance(module, nn.Conv2d)
            or isinstance(module, nn.ConvTranspose2d)
            or isinstance(module, nn.ConvTranspose3d)
        ):
            module.weight = nn.init.kaiming_normal_(module.weight, a=self.neg_slope)
            if module.bias is not None:
                module.bias = nn.init.constant_(module.bias, 0)</code></pre>
</details>
</dd>
<dt id="common_models.LSTMTagger"><code class="flex name class">
<span>class <span class="ident">LSTMTagger</span></span>
<span>(</span><span>embedding_dim=24, hidden_dim=2048, vocab_size=32768, tagset_size=1024)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LSTMTagger(nn.Module):
    def __init__(
        self, embedding_dim=24, hidden_dim=2048, vocab_size=32768, tagset_size=1024
    ):
        super(LSTMTagger, self).__init__()
        self.hidden_dim = hidden_dim

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        # The linear layer that maps from hidden state space to tag space
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_space, dim=1)
        return tag_scores</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="common_models.LSTMTagger.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, sentence)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, sentence):
    embeds = self.word_embeddings(sentence)
    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
    tag_scores = F.log_softmax(tag_space, dim=1)
    return tag_scores</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="common_models.Unet" href="#common_models.Unet">Unet</a></code></li>
<li><code><a title="common_models.alexnet_graph" href="#common_models.alexnet_graph">alexnet_graph</a></code></li>
<li><code><a title="common_models.bert_graph" href="#common_models.bert_graph">bert_graph</a></code></li>
<li><code><a title="common_models.dlrm_graph" href="#common_models.dlrm_graph">dlrm_graph</a></code></li>
<li><code><a title="common_models.dpr_graph" href="#common_models.dpr_graph">dpr_graph</a></code></li>
<li><code><a title="common_models.gpt2_graph" href="#common_models.gpt2_graph">gpt2_graph</a></code></li>
<li><code><a title="common_models.langmodel_graph" href="#common_models.langmodel_graph">langmodel_graph</a></code></li>
<li><code><a title="common_models.objectdetection_model" href="#common_models.objectdetection_model">objectdetection_model</a></code></li>
<li><code><a title="common_models.resnet_18_graph" href="#common_models.resnet_18_graph">resnet_18_graph</a></code></li>
<li><code><a title="common_models.resnet_50_graph" href="#common_models.resnet_50_graph">resnet_50_graph</a></code></li>
<li><code><a title="common_models.speech2text_model" href="#common_models.speech2text_model">speech2text_model</a></code></li>
<li><code><a title="common_models.vggnet_graph" href="#common_models.vggnet_graph">vggnet_graph</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="common_models.InitWeights_He" href="#common_models.InitWeights_He">InitWeights_He</a></code></h4>
</li>
<li>
<h4><code><a title="common_models.LSTMTagger" href="#common_models.LSTMTagger">LSTMTagger</a></code></h4>
<ul class="">
<li><code><a title="common_models.LSTMTagger.forward" href="#common_models.LSTMTagger.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>