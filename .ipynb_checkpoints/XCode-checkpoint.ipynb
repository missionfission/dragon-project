{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, operator, attributes, inputs, outputs, scope):\n",
    "        self.operator = operator\n",
    "        self.attributes = attributes\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.scope = scope\n",
    "\n",
    "    @property\n",
    "    def operator(self):\n",
    "        return self._operator\n",
    "\n",
    "    @operator.setter\n",
    "    def operator(self, operator):\n",
    "        self._operator = operator.lower()\n",
    "\n",
    "    @property\n",
    "    def attributes(self):\n",
    "        return self._attributes\n",
    "\n",
    "    @attributes.setter\n",
    "    def attributes(self, attributes):\n",
    "        self._attributes = attributes\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "\n",
    "    @inputs.setter\n",
    "    def inputs(self, inputs):\n",
    "        self._inputs = inputs\n",
    "\n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self._outputs\n",
    "\n",
    "    @outputs.setter\n",
    "    def outputs(self, outputs):\n",
    "        self._outputs = outputs\n",
    "\n",
    "    @property\n",
    "    def scope(self):\n",
    "        return self._scope\n",
    "\n",
    "    @scope.setter\n",
    "    def scope(self, scope):\n",
    "        self._scope = scope\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = ', '.join([str(v) for v in self.outputs])\n",
    "        text += ' = ' + self.operator\n",
    "        if self.attributes:\n",
    "            text += '[' + ', '.join(\n",
    "                [str(k) + ' = ' + str(v)\n",
    "                 for k, v in self.attributes.items()]) + ']'\n",
    "        text += '(' + ', '.join([str(v) for v in self.inputs]) + ')'\n",
    "        return text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Variable:\n",
    "    def __init__(self, name, dtype, shape=None):\n",
    "        self.name = name\n",
    "        self.dtype = dtype\n",
    "        self.shape = shape\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, name):\n",
    "        self._name = name\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self._dtype\n",
    "\n",
    "    @dtype.setter\n",
    "    def dtype(self, dtype):\n",
    "        self._dtype = dtype.lower()\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "\n",
    "    @shape.setter\n",
    "    def shape(self, shape):\n",
    "        self._shape = shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return len(self.shape)\n",
    "\n",
    "    def size(self):\n",
    "        return self.shape\n",
    "\n",
    "    def dim(self):\n",
    "        return self.ndim\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = '%' + self.name + ': ' + self.dtype\n",
    "        if self.shape is not None:\n",
    "            text += '[' + ', '.join([str(x) for x in self.shape]) + ']'\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Graph:\n",
    "    def __init__(self, name, variables, inputs, outputs, nodes):\n",
    "        self.name = name\n",
    "        self.variables = variables\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.nodes = nodes\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, name):\n",
    "        self._name = name\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self._variables\n",
    "\n",
    "    @variables.setter\n",
    "    def variables(self, variables):\n",
    "        self._variables = variables\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "\n",
    "    @inputs.setter\n",
    "    def inputs(self, inputs):\n",
    "        self._inputs = inputs\n",
    "\n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self._outputs\n",
    "\n",
    "    @outputs.setter\n",
    "    def outputs(self, outputs):\n",
    "        self._outputs = outputs\n",
    "\n",
    "    @property\n",
    "    def nodes(self):\n",
    "        return self._nodes\n",
    "\n",
    "    @nodes.setter\n",
    "    def nodes(self, nodes):\n",
    "        self._nodes = nodes\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = self.name\n",
    "        text += ' (' + '\\n'\n",
    "        text += ',\\n'.join(['\\t' + str(v) for v in self.inputs]) + '\\n'\n",
    "        text += '):' + '\\n'\n",
    "        text += '\\n'.join(['\\t' + str(x) for x in self.nodes]) + '\\n'\n",
    "        text += '\\t' + 'return ' + ', '.join([str(v) for v in self.outputs])\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "__all__ = ['flatten', 'Flatten']\n",
    "\n",
    "\n",
    "def flatten(inputs):\n",
    "    queue = deque([inputs])\n",
    "    outputs = []\n",
    "    while queue:\n",
    "        x = queue.popleft()\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            queue.extend(x)\n",
    "        elif isinstance(x, dict):\n",
    "            queue.extend(x.values())\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            outputs.append(x)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = self.model(*args, **kwargs)\n",
    "        return flatten(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.jit\n",
    "\n",
    "from torchprofile.utils.ir import Variable, Node, Graph\n",
    "\n",
    "__all__ = ['trace']\n",
    "\n",
    "\n",
    "def trace(model, args=(), kwargs=None):\n",
    "    assert kwargs is None, 'Keyword arguments are not supported for now. ' \\\n",
    "                           'Please use positional arguments instead!'\n",
    "\n",
    "    with warnings.catch_warnings(record=True):\n",
    "        graph, _ = torch.jit._get_trace_graph(Flatten(model), args, kwargs)\n",
    "\n",
    "    variables = dict()\n",
    "    for x in graph.nodes():\n",
    "        for v in list(x.inputs()) + list(x.outputs()):\n",
    "            if 'tensor' in v.type().kind().lower():\n",
    "                variables[v] = Variable(\n",
    "                    name=v.debugName(),\n",
    "                    dtype=v.type().scalarType(),\n",
    "                    shape=v.type().sizes(),\n",
    "                )\n",
    "            else:\n",
    "                variables[v] = Variable(\n",
    "                    name=v.debugName(),\n",
    "                    dtype=str(v.type()),\n",
    "                )\n",
    "\n",
    "    nodes = []\n",
    "    for x in graph.nodes():\n",
    "        node = Node(\n",
    "            operator=x.kind(),\n",
    "            attributes={\n",
    "                s: getattr(x, x.kindOf(s))(s)\n",
    "                for s in x.attributeNames()\n",
    "            },\n",
    "            inputs=[variables[v] for v in x.inputs() if v in variables],\n",
    "            outputs=[variables[v] for v in x.outputs() if v in variables],\n",
    "            scope=x.scopeName() \\\n",
    "                .replace('Flatten/', '', 1) \\\n",
    "                .replace('Flatten', '', 1),\n",
    "        )\n",
    "        nodes.append(node)\n",
    "\n",
    "    graph = Graph(\n",
    "        name=model.__class__.__module__ + '.' + model.__class__.__name__,\n",
    "        variables=[v for v in variables.values()],\n",
    "        inputs=[variables[v] for v in graph.inputs() if v in variables],\n",
    "        outputs=[variables[v] for v in graph.outputs() if v in variables],\n",
    "        nodes=nodes,\n",
    "    )\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handlers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "__all__ = ['handlers']\n",
    "\n",
    "\n",
    "def addmm(node):\n",
    "    # [n, p] = aten::addmm([n, p], [n, m], [m, p], *, *)\n",
    "    n, m = node.inputs[1].shape\n",
    "    m, p = node.inputs[2].shape\n",
    "    return n * m * p\n",
    "\n",
    "\n",
    "def addmv(node):\n",
    "    # [n] = aten::addmv([n], [n, m], [m], *, *)\n",
    "    n, m = node.inputs[1].shape\n",
    "    return n * m\n",
    "\n",
    "\n",
    "def bmm(node):\n",
    "    # [b, n, p] = aten::bmm([b, n, m], [b, m, p])\n",
    "    b, n, m = node.inputs[0].shape\n",
    "    b, m, p = node.inputs[1].shape\n",
    "    return b * n * m * p\n",
    "\n",
    "\n",
    "def matmul(node):\n",
    "    if node.inputs[0].ndim == 1 and node.inputs[1].ndim == 1:\n",
    "        # [] = aten::matmul([n], [n])\n",
    "        n = node.inputs[0].shape[0]\n",
    "        return n\n",
    "    elif node.inputs[0].ndim == 1 and node.inputs[1].ndim == 2:\n",
    "        # [m] = aten::matmul([n], [n, m])\n",
    "        n, m = node.inputs[1].shape\n",
    "        return n * m\n",
    "    elif node.inputs[0].ndim == 2 and node.inputs[1].ndim == 1:\n",
    "        # [n] = aten::matmul([n, m], [m])\n",
    "        n, m = node.inputs[0].shape\n",
    "        return n * m\n",
    "    elif node.inputs[0].ndim == 2 and node.inputs[1].ndim == 2:\n",
    "        # [n, p] = aten::matmul([n, m], [m, p])\n",
    "        n, m = node.inputs[0].shape\n",
    "        m, p = node.inputs[1].shape\n",
    "        return n * m * p\n",
    "    elif node.inputs[0].ndim == 1:\n",
    "        # [..., m] = aten::matmul([n], [..., n, m])\n",
    "        *b, n, m = node.inputs[1].shape\n",
    "        return np.prod(b) * n * m\n",
    "    elif node.inputs[1].ndim == 1:\n",
    "        # [..., n] = aten::matmul([..., n, m], [m])\n",
    "        *b, n, m = node.inputs[0].shape\n",
    "        return np.prod(b) * n * m\n",
    "    else:\n",
    "        # [..., n, p] = aten::matmul([..., n, m], [..., m, p])\n",
    "        *b, n, p = node.outputs[0].shape\n",
    "        *_, n, m = node.inputs[0].shape\n",
    "        *_, m, p = node.inputs[1].shape\n",
    "        return np.prod(b) * n * m * p\n",
    "\n",
    "\n",
    "def mul(node):\n",
    "    os = node.outputs[0].shape\n",
    "    return np.prod(os)\n",
    "\n",
    "\n",
    "def convolution(node):\n",
    "    if node.outputs[0].shape[1] == node.inputs[1].shape[0]:\n",
    "        oc, ic, *ks = node.inputs[1].shape\n",
    "    else:\n",
    "        ic, oc, *ks = node.inputs[1].shape\n",
    "    os = node.outputs[0].shape\n",
    "    return np.prod(os) * ic * np.prod(ks)\n",
    "\n",
    "\n",
    "def batch_norm(node):\n",
    "    # TODO: provide an option to not fuse `batch_norm` into `linear` or `conv`\n",
    "    return 0\n",
    "\n",
    "\n",
    "def instance_norm_or_layer_norm(node):\n",
    "    os = node.outputs[0].shape\n",
    "    return np.prod(os)\n",
    "\n",
    "\n",
    "def avg_pool_or_mean(node):\n",
    "    os = node.outputs[0].shape\n",
    "    return np.prod(os)\n",
    "\n",
    "\n",
    "handlers = (\n",
    "    ('aten::addmm', addmm),\n",
    "    ('aten::addmv', addmv),\n",
    "    ('aten::bmm', bmm),\n",
    "    ('aten::matmul', matmul),\n",
    "    ((\n",
    "        'aten::mul',\n",
    "        'aten::mul_',\n",
    "    ), mul),\n",
    "    ('aten::_convolution', convolution),\n",
    "    ('aten::batch_norm', batch_norm),\n",
    "    ((\n",
    "        'aten::instance_norm',\n",
    "        'aten::layer_norm',\n",
    "    ), instance_norm_or_layer_norm),\n",
    "    ((\n",
    "        'aten::adaptive_avg_pool1d',\n",
    "        'aten::adaptive_avg_pool2d',\n",
    "        'aten::adaptive_avg_pool3d',\n",
    "        'aten::avg_pool1d',\n",
    "        'aten::avg_pool2d',\n",
    "        'aten::avg_pool3d',\n",
    "        'aten::mean',\n",
    "    ), avg_pool_or_mean),\n",
    "    ((\n",
    "        'aten::adaptive_max_pool1d',\n",
    "        'aten::adaptive_max_pool2d',\n",
    "        'aten::adaptive_max_pool3d',\n",
    "        'aten::add',\n",
    "        'aten::add_',\n",
    "        'aten::alpha_dropout',\n",
    "        'aten::cat',\n",
    "        'aten::chunk',\n",
    "        'aten::clone',\n",
    "        'aten::constant_pad_nd',\n",
    "        'aten::contiguous',\n",
    "        'aten::div',\n",
    "        'aten::div_',\n",
    "        'aten::dropout',\n",
    "        'aten::dropout_',\n",
    "        'aten::embedding',\n",
    "        'aten::eq',\n",
    "        'aten::feature_dropout',\n",
    "        'aten::flatten',\n",
    "        'aten::gt',\n",
    "        'aten::hardtanh_',\n",
    "        'aten::int',\n",
    "        'aten::lt',\n",
    "        'aten::log_softmax',\n",
    "        'aten::max_pool1d',\n",
    "        'aten::max_pool1d_with_indices',\n",
    "        'aten::max_pool2d',\n",
    "        'aten::max_pool2d_with_indices',\n",
    "        'aten::max_pool3d',\n",
    "        'aten::max_pool3d_with_indices',\n",
    "        'aten::max_unpool1d',\n",
    "        'aten::max_unpool2d',\n",
    "        'aten::max_unpool3d',\n",
    "        'aten::ne',\n",
    "        'aten::reflection_pad1d',\n",
    "        'aten::reflection_pad2d',\n",
    "        'aten::reflection_pad3d',\n",
    "        'aten::relu',\n",
    "        'aten::relu_',\n",
    "        'aten::replication_pad1d',\n",
    "        'aten::replication_pad2d',\n",
    "        'aten::replication_pad3d',\n",
    "        'aten::rsub',\n",
    "        'aten::select',\n",
    "        'aten::sigmoid',\n",
    "        'aten::size',\n",
    "        'aten::slice',\n",
    "        'aten::softmax',\n",
    "        'aten::softshrink',\n",
    "        'aten::squeeze',\n",
    "        'aten::sub',\n",
    "        'aten::sum',\n",
    "        'aten::t',\n",
    "        'aten::tanh',\n",
    "        'aten::threshold',\n",
    "        'aten::transpose',\n",
    "        'aten::view',\n",
    "        'aten::zeros',\n",
    "        'prim::constant',\n",
    "        'prim::listconstruct',\n",
    "        'prim::listunpack',\n",
    "        'prim::numtotensor',\n",
    "    ), None),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from torchprofile.utils.trace import trace\n",
    "\n",
    "__all__ = ['profile_macs']\n",
    "\n",
    "\n",
    "def profile_macs(model, args=(), kwargs=None, reduction=sum):\n",
    "    results = dict()\n",
    "\n",
    "    graph = trace(model, args, kwargs)\n",
    "    for node in graph.nodes:\n",
    "#         print(node)\n",
    "        for operators, func in handlers:\n",
    "            if isinstance(operators, str):\n",
    "                operators = [operators]\n",
    "            if node.operator in operators:\n",
    "                if func is not None:\n",
    "                    print(node.operator)\n",
    "                    results[node] = func(node)\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
    "                node.operator))\n",
    "\n",
    "    if reduction is not None:\n",
    "        return reduction(results.values())\n",
    "    else:\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::adaptive_avg_pool2d\n",
      "aten::addmm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::adaptive_avg_pool2d\n",
      "aten::addmm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "if __name__ == '__main__':\n",
    "    for name, model in models.__dict__.items():\n",
    "        if not name.islower() or name.startswith('__') or not callable(model):\n",
    "            continue\n",
    "\n",
    "        model = model().eval()\n",
    "#       \n",
    "        if 'resnet50' in name:\n",
    "            if 'inception' not in name:\n",
    "                inputs = torch.randn(1, 3, 224, 224)\n",
    "            else:\n",
    "                inputs = torch.randn(1, 3, 299, 299)\n",
    "            macs = profile_macs(model, inputs)\n",
    "#             graph = trace(model,inputs)\n",
    "#             for node in graph.nodes:\n",
    "#                 print(node)\n",
    "#             print(graph)\n",
    "            # with open('graph_resnet18.txt', 'w') as file:\n",
    "            #     file.write(graph)\n",
    "#         print('{}: {:.4g} G'.format(name, macs / 1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaml Loaders - utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "import yaml\n",
    "from yaml import dump\n",
    "\n",
    "import yamlordereddictloader\n",
    "\n",
    "\n",
    "class accelergy_loader(yaml.SafeLoader):\n",
    "    \"\"\"\n",
    "    Accelergy yaml loader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "\n",
    "        self._root = os.path.split(stream.name)[0]\n",
    "        super(accelergy_loader, self).__init__(stream)\n",
    "\n",
    "def include_constructor(self, node):\n",
    "    \"\"\"\n",
    "    constructor:\n",
    "    parses the !include relative_file_path\n",
    "    loads the file from relative_file_path and insert the values into the original file\n",
    "    \"\"\"\n",
    "    filepath = self.construct_scalar(node)\n",
    "    if filepath[-1] == \",\":\n",
    "        filepath = filepath[:-1]\n",
    "    filename = os.path.join(self._root, filepath)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return yaml.load(f, accelergy_loader)\n",
    "\n",
    "\n",
    "def includedir_constructor(self, node):\n",
    "    \"\"\"\n",
    "    constructor:\n",
    "    parses the !includedir relative_file_path\n",
    "    loads the file from relative_file_path and insert the values into the original file\n",
    "    \"\"\"\n",
    "    filepath = self.construct_scalar(node)\n",
    "    if filepath[-1] == \",\":\n",
    "        filepath = filepath[:-1]\n",
    "    dirname = os.path.join(self._root, filepath)\n",
    "    yamllist = []\n",
    "    for filename in glob.glob(dirname + \"/*.yaml\"):\n",
    "        with open(filename, \"r\") as f:\n",
    "            yamllist.append(yaml.load(f, accelergy_loader))\n",
    "    return yamllist\n",
    "    \n",
    "    \n",
    "yaml.add_constructor(\"!include\", include_constructor, accelergy_loader)\n",
    "yaml.add_constructor(\"!includedir\", includedir_constructor, accelergy_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class accelergy_dumper(yamlordereddictloader.SafeDumper):\n",
    "    \"\"\" Accelergy yaml dumper \"\"\"\n",
    "\n",
    "    def ignore_aliases(self, _data):\n",
    "        return True\n",
    "\n",
    "\n",
    "def create_folder(directory):\n",
    "    \"\"\"\n",
    "    Checks the existence of a directory, if does not exist, create a new one\n",
    "    :param directory: path to directory under concern\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"ERROR: Creating directory. \" + directory)\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    merge_dict = deepcopy(dict1)\n",
    "    merge_dict.update(dict2)\n",
    "    return merge_dict\n",
    "\n",
    "\n",
    "def write_yaml_file(filepath, content):\n",
    "    \"\"\"\n",
    "    if file exists at filepath, overwite the file, if not, create a new file\n",
    "    :param filepath: string that specifies the destination file path\n",
    "    :param content: yaml string that needs to be written to the destination file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "    create_folder(os.path.dirname(filepath))\n",
    "    out_file = open(filepath, \"a\")\n",
    "    out_file.write(dump(content, default_flow_style=False, Dumper=accelergy_dumper))\n",
    "\n",
    "\n",
    "def get_yaml_format(content):\n",
    "    return dump(content, default_flow_style=False, Dumper=accelergy_dumper)\n",
    "\n",
    "\n",
    "def write_file(filepath, content):\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "    create_folder(os.path.dirname(filepath))\n",
    "    out_file = open(filepath, \"a\")\n",
    "    out_file.write(content)\n",
    "\n",
    "\n",
    "def remove_quotes(filepath):\n",
    "    \"\"\"\n",
    "    :param filepath: file that needs to processed\n",
    "    :return: None\n",
    "    removes the quotes inside yaml files\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        new_content = \"\"\n",
    "        f = open(filepath, \"r\")\n",
    "\n",
    "        for line in f:\n",
    "            if \"'\" in line:\n",
    "                line = line.replace(\"'\", \"\")\n",
    "                new_content += line\n",
    "        f.close()\n",
    "        os.remove(filepath)\n",
    "        newf = open(filepath, \"w\")\n",
    "        newf.write(new_content)\n",
    "        newf.close()\n",
    "\n",
    "\n",
    "def ERROR_CLEAN_EXIT(*argv):\n",
    "    msg_str = \"ERROR: \"\n",
    "    for arg in argv:\n",
    "        if type(arg) is not str:\n",
    "            print(msg_str)\n",
    "            print(arg)\n",
    "            msg_str = \"\"\n",
    "        else:\n",
    "            msg_str += arg + \" \"\n",
    "    print(msg_str)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "def WARN(*argv):\n",
    "    msg_str = \"Warn: \"\n",
    "    for arg in argv:\n",
    "        if type(arg) is not str:\n",
    "            print(msg_str)\n",
    "            print(arg)\n",
    "            msg_str = \"\"\n",
    "        else:\n",
    "            msg_str += arg + \" \"\n",
    "    print(msg_str)\n",
    "\n",
    "\n",
    "def INFO(*argv):\n",
    "    msg_str = \"Info: \"\n",
    "    for arg in argv:\n",
    "        if type(arg) is not str:\n",
    "            print(msg_str)\n",
    "            print(arg)\n",
    "            msg_str = \"\"\n",
    "        else:\n",
    "            msg_str += arg + \" \"\n",
    "    print(msg_str)\n",
    "\n",
    "\n",
    "def ASSERT_MSG(expression, msg):\n",
    "    if not expression:\n",
    "        ERROR_CLEAN_EXIT(msg)\n",
    "\n",
    "\n",
    "def add_functions_as_methods(functions):\n",
    "    def decorator(Class):\n",
    "        for function in functions:\n",
    "            setattr(Class, function.__name__, function)\n",
    "        return Class\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def register_function(sequence, function):\n",
    "    sequence.append(function)\n",
    "    return function\n",
    "\n",
    "\n",
    "def remove_brackets(name):\n",
    "    \"\"\"Removes the brackets from a component name in a list\"\"\"\n",
    "    if \"[\" not in name and \"]\" not in name:\n",
    "        return name\n",
    "    if \"[\" in name and \"]\" in name:\n",
    "        start_idx = name.find(\"[\")\n",
    "        end_idx = name.find(\"]\")\n",
    "        name = name[:start_idx] + name[end_idx + 1 :]\n",
    "        name = remove_brackets(name)\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systemstate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelergy.utils import *\n",
    "class SystemState():\n",
    "    def __init__(self):\n",
    "        self.cc_classes = {}\n",
    "        self.pc_classes = {}\n",
    "        self.arch_spec = None\n",
    "        self.hier_arch_spec = None\n",
    "        self.ccs = {}\n",
    "        self.pcs = {}\n",
    "        self.action_counts = None\n",
    "        self.plug_ins = []\n",
    "        self.ERT = None\n",
    "        self.ART = None\n",
    "        self.parser_version = None\n",
    "        self.flags = {}\n",
    "        self.energy_estimations = None\n",
    "\n",
    "    def set_flag_s(self, flag_name_val_dict):\n",
    "        self.flags.update(flag_name_val_dict)\n",
    "\n",
    "    def set_accelergy_version(self, version):\n",
    "        self.parser_version = version\n",
    "\n",
    "    def set_hier_arch_spec(self, arch_dict):\n",
    "        ASSERT_MSG(self.hier_arch_spec is None, 'interpreted input arch is set')\n",
    "        self.hier_arch_spec = arch_dict\n",
    "\n",
    "    def set_arch_spec(self, arch_spec):\n",
    "        ASSERT_MSG(self.arch_spec is None, 'architecture spec is already set')\n",
    "        self.arch_spec = arch_spec\n",
    "\n",
    "    def add_cc_class(self, cc_class):\n",
    "        cc_class_name = cc_class.get_name()\n",
    "        ASSERT_MSG(cc_class_name not in self.cc_classes, '%s compound class is already added'%(cc_class_name))\n",
    "        self.cc_classes[cc_class_name] = cc_class\n",
    "\n",
    "    def add_pc_class(self, pc_class):\n",
    "        pc_class_name = pc_class.get_name()\n",
    "        ASSERT_MSG(pc_class_name not in self.pc_classes, '%s primitive class is already added'%(pc_class_name))\n",
    "        self.pc_classes[pc_class_name] = pc_class\n",
    "\n",
    "    def add_cc(self, cc):\n",
    "        cc_name = cc.get_name()\n",
    "        ASSERT_MSG(cc_name not in self.ccs, '%s compound component is already added'%(cc_name))\n",
    "        self.ccs[cc_name] = cc\n",
    "\n",
    "    def add_pc(self, pc):\n",
    "        pc_name = pc.get_name()\n",
    "        ASSERT_MSG(pc_name not in self.ccs, '%s compound component is already added'%(pc_name))\n",
    "        self.pcs[pc_name] = pc\n",
    "\n",
    "    def add_plug_ins(self, plug_ins):\n",
    "        ASSERT_MSG(type(plug_ins) is list, 'plug in objects need to be passed in as a list')\n",
    "        self.plug_ins = plug_ins\n",
    "\n",
    "    def set_ERT(self, ERT):\n",
    "        self.ERT = ERT\n",
    "\n",
    "    def set_ART(self, ART):\n",
    "        self.ART = ART\n",
    "\n",
    "    def set_action_counts(self, action_counts):\n",
    "        self.action_counts = action_counts\n",
    "\n",
    "    def set_energy_estimations(self, energy_estimations):\n",
    "        self.energy_estimations = energy_estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class Action(object):\n",
    "    \"\"\"Action class\"\"\"\n",
    "    def __init__(self, action_def_dict):\n",
    "\n",
    "        self.action_def_dict = action_def_dict\n",
    "        self.name = action_def_dict['name']\n",
    "\n",
    "        self._arguments = None\n",
    "        self.set_arguments(action_def_dict)\n",
    "\n",
    "        # compound action contains action definitions in terms of subcomponents\n",
    "        self._subcomponents = None\n",
    "        self.set_subcomponents(action_def_dict)\n",
    "\n",
    "        # repeat has the same meaning as action share\n",
    "        if 'repeat' in action_def_dict:\n",
    "            self._action_share = action_def_dict['repeat']\n",
    "        elif 'action_share' in action_def_dict:\n",
    "            self._action_share = action_def_dict['action_share']\n",
    "        else:\n",
    "            self._action_share = None\n",
    "        # only compound actions will later set this property\n",
    "        self._primitive_list = None\n",
    "\n",
    "    def set_arguments(self, action_def_dict):\n",
    "        if 'arguments' in action_def_dict:\n",
    "            self._arguments = {}\n",
    "            for arg_name, arg_range in action_def_dict['arguments'].items():\n",
    "                self._arguments[arg_name] = arg_range\n",
    "\n",
    "    def set_subcomponents(self, action_def_dict):\n",
    "        if 'subcomponents' in action_def_dict:\n",
    "            self._subcomponents = {}\n",
    "            for subcomp in action_def_dict['subcomponents']:\n",
    "                subcompActions = []\n",
    "                for subcompAction in subcomp['actions']:\n",
    "                    subcompActions.append(Action(subcompAction))\n",
    "                self._subcomponents[subcomp['name']] = subcompActions\n",
    "\n",
    "    def set_primitive_list(self, primitive_list):\n",
    "        self._primitive_list = primitive_list\n",
    "\n",
    "    def set_action_share(self, new_action_share):\n",
    "        \"\"\"update the parsed repeat/action_share value\"\"\"\n",
    "        self._action_share = new_action_share\n",
    "\n",
    "    def set_argument(self, new_arg_dict):\n",
    "        \"\"\" update one or more argument name-val pairs\"\"\"\n",
    "        self._arguments.update(new_arg_dict)\n",
    "\n",
    "    def set_subcomps(self, defined_subcomps):\n",
    "        self._subcomponents = defined_subcomps\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_action_share(self):\n",
    "        return self._action_share\n",
    "\n",
    "    def get_arguments(self):\n",
    "        return self._arguments\n",
    "\n",
    "    def get_argument(self, arg_name):\n",
    "        return self._arguments[arg_name]\n",
    "\n",
    "    def get_subcomps(self):\n",
    "        ASSERT_MSG(self._subcomponents is not None, 'action does not have defined subcomponents')\n",
    "        return self._subcomponents\n",
    "\n",
    "    def get_primitive_list(self):\n",
    "        return self._primitive_list\n",
    "\n",
    "    def get_action_info_as_dict(self):\n",
    "        action_dict = {'name': self.name}\n",
    "        if self._subcomponents is not None: action_dict['subcomponents'] = self._subcomponents\n",
    "        if self._arguments is not None: action_dict['arguments'] = self._arguments\n",
    "        return action_dict\n",
    "\n",
    "    def get_subactions(self, subcompName):\n",
    "        ASSERT_MSG(self._subcomponents is not None and subcompName in self._subcomponents,\n",
    "                   'cannot find subactions associated with %s for action %s'%(subcompName, self.name))\n",
    "        return self._subcomponents[subcompName]\n",
    "\n",
    "    def get_arg_val(self, argName):\n",
    "        ASSERT_MSG(argName in self._arguments, 'argument name %s is not associated with action %s'%(argName, self.name))\n",
    "        return self._arguments[argName]\n",
    "\n",
    "    def set_arg(self, arg_dict):\n",
    "        self._arguments.update(arg_dict)\n",
    "\n",
    "    def flatten_action_args_into_list(self, mappingDict):\n",
    "        \"\"\" flatten an action into a list representing all possible argument value combinations\"\"\"\n",
    "\n",
    "        args = self.get_arguments()\n",
    "        if args is None:\n",
    "            return [self]  # no arguments, no need to flatten\n",
    "\n",
    "        # an action needs to be flattened into a list of actions with the same action name but different arg vals\n",
    "        total_entries = 1\n",
    "        argument_range_record = {}\n",
    "        for arg_name, arg_range in args.items():\n",
    "            ASSERT_MSG(type(arg_range) is str, '%s: argument value for action %s is not string, cannot parse range'%(arg_name,self.name))\n",
    "            ASSERT_MSG('..' in arg_range, '%s: argument value for action %s is not range, cannot parse range'%(arg_name,self.name))\n",
    "            new_arg_range = Action.map_arg_range_bounds(arg_range, mappingDict)[0]\n",
    "            startIdx, endIdx = Action.parse_arg_range(new_arg_range)\n",
    "            total_entries *= (endIdx - startIdx + 1)\n",
    "            argument_range_record[arg_name] = (startIdx, endIdx)\n",
    "\n",
    "        action_list = []\n",
    "        for entry_idx in range(total_entries):\n",
    "            offset = 1\n",
    "            arg_def = {}\n",
    "            for arg_name, range_record in argument_range_record.items():\n",
    "                arg_range = range_record[1] - range_record[0] + 1\n",
    "                arg_def[arg_name] = (entry_idx // offset) % arg_range + range_record[0]\n",
    "                offset *= arg_range\n",
    "            subcomp_list = []\n",
    "            new_action = deepcopy(self); new_action._arguments = arg_def\n",
    "            action_list.append(new_action)\n",
    "        return action_list\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_arg_range(arg_range):\n",
    "        \"\"\" Parse the start index and end index for an argument range\"\"\"\n",
    "        if type(arg_range) is not str or '..' not in arg_range:\n",
    "            ERROR_CLEAN_EXIT('cannot parse the argument range specification: ', arg_range)\n",
    "        split_sub_string = arg_range.split('..')\n",
    "        start_idx = int(split_sub_string[0])\n",
    "        end_idx = int(split_sub_string[1])\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_action_to_list_with_arg_values(action_info):\n",
    "        \"\"\"flatten actions with arguments into list\n",
    "           1) input action is fully defined with numerical ranges\n",
    "           2) output list contains a list of actions each with a possible set of argument values\n",
    "        \"\"\"\n",
    "\n",
    "        action_name = action_info['name']\n",
    "        total_entries = 1\n",
    "        argument_range_record = {}\n",
    "        for argument_name, argument_range in action_info['arguments'].items():\n",
    "            start_idx, end_idx = Action.parse_arg_range(argument_range)\n",
    "            total_entries *= (end_idx - start_idx + 1)\n",
    "            argument_range_record[argument_name] = (start_idx, end_idx)\n",
    "        expanded_list = [{'name': action_name, 'arguments':{}} for i in range(total_entries)]\n",
    "        # construct list of dictionaries that contain all the possible combination of argument values\n",
    "        for entry_idx in range(total_entries):\n",
    "            offset = 1\n",
    "            for argument_name, range_record in argument_range_record.items():\n",
    "                arg_range = range_record[1] - range_record[0] + 1\n",
    "                expanded_list[entry_idx]['arguments'][argument_name] = \\\n",
    "                    (entry_idx // offset) % arg_range + range_record[0]\n",
    "                offset *= arg_range\n",
    "        return expanded_list\n",
    "\n",
    "    @staticmethod\n",
    "    def map_arg_range_bounds(arg_range_str, attributes_dict):\n",
    "        \"\"\"\n",
    "        arguments for actions might have ranges that are specified in terms of it attributes\n",
    "        parses the argument ranges in the format int/str..int/str, where str can be arithmetic operation\n",
    "\n",
    "        :param arg_range_str: string that decribes the range of a compound action\n",
    "        :param attributes_dict: attribute name-value pairs of the compound component\n",
    "        :return: parsed argument range, whether there was binding\n",
    "        \"\"\"\n",
    "        split_sub_string = arg_range_str.split('..')\n",
    "        detect_arg_range_binding = False\n",
    "        # process the start index\n",
    "        try:\n",
    "            start_idx = int(split_sub_string[0])\n",
    "        except ValueError:\n",
    "            op_type, op1, op2 = parse_expression_for_arithmetic(split_sub_string[0], attributes_dict)\n",
    "            if op_type is not None:\n",
    "                start_idx = process_arithmetic(op1, op2, op_type)\n",
    "            else:\n",
    "                if split_sub_string[0] not in attributes_dict:\n",
    "                    ERROR_CLEAN_EXIT('cannot find mapping from', arg_range_str, 'to', attributes_dict)\n",
    "                start_idx = attributes_dict[split_sub_string[0]]\n",
    "            detect_arg_range_binding = True\n",
    "\n",
    "        # process the end index\n",
    "        try:\n",
    "            end_idx = int(split_sub_string[1])\n",
    "        except ValueError:\n",
    "            op_type, op1, op2 = parse_expression_for_arithmetic(split_sub_string[1], attributes_dict)\n",
    "            if op_type is not None:\n",
    "                end_idx = process_arithmetic(op1, op2, op_type)\n",
    "            else:\n",
    "                if split_sub_string[1] not in attributes_dict:\n",
    "                    ERROR_CLEAN_EXIT('cannot find mapping from', arg_range_str, 'to', attributes_dict)\n",
    "                end_idx = attributes_dict[split_sub_string[1]]\n",
    "            detect_arg_range_binding = True\n",
    "\n",
    "        new_arg_range_str = str(start_idx) + '..' + str(end_idx)\n",
    "\n",
    "        return new_arg_range_str, detect_arg_range_binding\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_arg_range(arg_range):\n",
    "        if type(arg_range) is not str or '..' not in arg_range:\n",
    "            ERROR_CLEAN_EXIT('cannot parse the argument range specification: ', arg_range)\n",
    "        split_sub_string = arg_range.split('..')\n",
    "        start_idx = int(split_sub_string[0])\n",
    "        end_idx   = int(split_sub_string[1])\n",
    "        return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subcomponent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subcomponent:\n",
    "    def __init__(self, comp_def):\n",
    "        self.dict_reprsentation = comp_def\n",
    "        if 'attributes' not in self.dict_reprsentation:\n",
    "            self.dict_reprsentation['attributes'] = {}\n",
    "\n",
    "    def set_name(self, name):\n",
    "        self.dict_reprsentation['name'] = name\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.dict_reprsentation['name']\n",
    "\n",
    "    def get_class_name(self):\n",
    "        return self.dict_reprsentation['class']\n",
    "\n",
    "    def get_attributes(self):\n",
    "        return self.dict_reprsentation['attributes']\n",
    "\n",
    "\n",
    "    def get_area_share(self):\n",
    "        return self.dict_reprsentation['area_share']\n",
    "\n",
    "    def add_new_attr(self, attr_dict):\n",
    "        self.dict_reprsentation['attributes'].update(attr_dict)\n",
    "\n",
    "    def set_area_share(self, area_share):\n",
    "        self.dict_reprsentation['area_share'] = area_share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "componenclass.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ComponentClass:\n",
    "    def __init__(self, class_dict):\n",
    "        self._name = class_dict['name']\n",
    "        self._default_attributes = deepcopy(class_dict['attributes'])\n",
    "\n",
    "        self._actions = {}\n",
    "        self.set_actions(class_dict['actions'])\n",
    "\n",
    "        self._subcomponents = None\n",
    "        if 'subcomponents' in class_dict:\n",
    "            self._subcomponents = {}\n",
    "            self.type = 'compound'\n",
    "            for scomp in class_dict['subcomponents']: self._subcomponents[scomp['name']] = Subcomponent(scomp)\n",
    "        else:\n",
    "            self.type = 'primitive'\n",
    "        self._primitive_type = class_dict['primitive_type'] if 'primitive_type' in class_dict else None\n",
    "\n",
    "    def set_actions(self, action_list):\n",
    "        ASSERT_MSG(type(action_list) is list,\n",
    "                   '%s class description must specify its actions in list format'%(self.get_name()))\n",
    "        for action in action_list:\n",
    "            ASSERT_MSG('name' in action, '%s class actions must contain \"name\" keys'%(self.get_name()))\n",
    "            self._actions[action['name']] = Action(action)\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Getters\n",
    "    #-----------------------------------------------------\n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def get_default_attr_to_apply(self, obj_attr_name_list):\n",
    "        attr_to_be_applied = OrderedDict()\n",
    "        for attr_name, attr_val in self._get_default_attrs().items():\n",
    "            if attr_val == \"must_specify\":\n",
    "                ASSERT_MSG(attr_name in obj_attr_name_list,\n",
    "                           \"attributes %s for compound class %s must be specified in architecture description\"\n",
    "                           %(attr_name, self.get_name()))\n",
    "            if attr_name not in obj_attr_name_list:\n",
    "                attr_to_be_applied[attr_name] = attr_val\n",
    "        return attr_to_be_applied\n",
    "\n",
    "    def _get_default_attrs(self):\n",
    "        return self._default_attributes\n",
    "\n",
    "    def _get_attr_name_list(self):\n",
    "        return list(self._default_attributes.keys())\n",
    "\n",
    "    def _get_attr_default_val(self, attrName):\n",
    "        ASSERT_MSG(attrName in self._default_attributes, 'Attribute %s cannot be found in class %s'%(attrName, self.get_name()))\n",
    "        return self._default_attributes[attrName]\n",
    "\n",
    "    def get_action_name_list(self):\n",
    "        return list(self._actions.keys())\n",
    "\n",
    "    def get_action(self, actionName):\n",
    "        ASSERT_MSG(actionName in self._actions, '%s does not exist in class %s'%(actionName, self.get_name()))\n",
    "        return self._actions[actionName]\n",
    "\n",
    "    def get_subcomponents_as_dict(self):\n",
    "        ASSERT_MSG(self._subcomponents is not None, 'component class %s does not have subcomponents' % self.get_name())\n",
    "        return self._subcomponents\n",
    "\n",
    "    def get_primitive_type(self):\n",
    "        return self._primitive_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action_counts:\n",
    "  local:\n",
    "  - action_counts:\n",
    "    - counts: 31306\n",
    "      name: idle\n",
    "    - arguments:\n",
    "        address_delta: 2\n",
    "        data_delta: 1\n",
    "      counts: 1152\n",
    "      name: fill\n",
    "    - arguments:\n",
    "        address_delta: 0\n",
    "        data_delta: 0\n",
    "      counts: 1150\n",
    "      name: read\n",
    "    - arguments:\n",
    "        address_delta: 2\n",
    "        data_delta: 1\n",
    "      counts: 2\n",
    "      name: read\n",
    "    name: eyeriss_like.weights_glb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  subtree:\n",
    "    - name: eyeriss_like\n",
    "      attributes:\n",
    "        technology: 40nm\n",
    "      local:\n",
    "        - name: weights_glb\n",
    "          class: smartbuffer_SRAM\n",
    "          attributes:\n",
    "            memory_width: 64\n",
    "            memory_depth: 1024\n",
    "            n_banks: 2\n",
    "        - name: shared_glb\n",
    "          class: smartbuffer_SRAM\n",
    "          attributes:\n",
    "            memory_width: 64\n",
    "            n_banks: 25\n",
    "            bank_depth: 512\n",
    "            memory_depth: bank_depth * n_banks\n",
    "            n_buffets: 2\n",
    "            update_fifo_depth: 2\n",
    "        - name: ifmap_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 16\n",
    "            col_id_width: 5\n",
    "        - name: weights_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 64\n",
    "        - name: psum_write_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 64\n",
    "        - name: psum_read_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 64\n",
    "            Y_X_wire_avg_length: 4mm\n",
    "      subtree:\n",
    "      - name: PE[0..167]\n",
    "        attributes:\n",
    "          memory_width: 16\n",
    "        local:\n",
    "          - name: ifmap_spad\n",
    "            class: smartbuffer_RF\n",
    "            attributes:\n",
    "              memory_depth: 12\n",
    "              buffet_manager_depth: 0\n",
    "          - name: weights_spad\n",
    "            class: smartbuffer_SRAM\n",
    "            attributes:\n",
    "              memory_depth: 224\n",
    "              buffet_manager_depth: 0\n",
    "          - name: psum_spad\n",
    "            class: smartbuffer_RF\n",
    "            attributes:\n",
    "              memory_depth: 24\n",
    "              buffet_manager_depth: 24\n",
    "              update_fifo_depth: 2\n",
    "          - name: mac\n",
    "            class: intmac\n",
    "            attributes:\n",
    "              datawidth: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    for nodes in graph.nodes():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Scheduling():\n",
    "    def __init__(self,opts=None,hwdesc=None,constrainsts=None):\n",
    "        total_cycles = 0\n",
    "          \n",
    "    def run(self, graph)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "        for nodes in graph.nodes():\n",
    "         # Also check how to schedule next node \n",
    "        node_memory_statistics(node)\n",
    "        if(memory_bandwidth<maxconstraints.bandwidth):\n",
    "            step_cycles = get_number_cycles(node, True )\n",
    "        logger.save(node,step_cycles, read_access, write_access, memory_bandwidth, ismemorybottleneck)\n",
    "        total_cycles+=step_cycles\n",
    "    logger.save(total_cycles)\n",
    "    \n",
    "    def node_memory_statistics(self):\n",
    "            read_access = 0\n",
    "            write_access = 0\n",
    "            memory_bandwidth = 0\n",
    "            \n",
    "            switch(node.type):\n",
    "                {\n",
    "                    case \"conv\" : \n",
    "                    \n",
    "                    \n",
    "                    default \"\" : \n",
    "                    \n",
    "                    \n",
    "                }\n",
    "                \n",
    "        return read_access, write_access, memory_bandwidth\n",
    "\n",
    "\n",
    "    def get_number_cycles(self, ismemorybottleneck, ):\n",
    "          if(!ismemorybottleneck):\n",
    "            return (node.computational_expense//hwdesc)\n",
    "             else:\n",
    "                return (total_memory_accesses//memory_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-ff39986a8cad>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-ff39986a8cad>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def runner():\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\"\n",
    "    logger = Logger()\n",
    "    maxconstraints = defconstraints(\"max.yaml\")  \n",
    "    minconstraints = defconstraints(\"min.yaml\")  \n",
    "    if (hwdesc!=None):\n",
    "        print(\"Mapping the Model on the Given Hardware\")\n",
    "    else:\n",
    "        print(\"Generating the Hardware Description and Logging Statistics\")\n",
    "        \n",
    "    scheduler = Scheduling(opts, hwdesc, constraints)\n",
    "    scheduler.run()\n",
    "    plugins = ['accelergy_ART','accelergy_ERT','cacti_memory','orion_noc','aladdin_compute']\n",
    "    instatiate_plugins()\n",
    "    \n",
    "    logger.save_statistics(area)\n",
    "    logger.save_statistics(energy)\n",
    "    \n",
    "def instantiate_plugins():\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-6-7dbab20dca1a>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7dbab20dca1a>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Logger():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,filename):\n",
    "        logging.save()\n",
    "\n",
    "    def __save__():\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatehw():\n",
    "    \"\"\"\n",
    "    Generate Hardware Description Yaml File \n",
    "    ### Iterate over Generated Hardware Description\n",
    "\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-8-bba73270b77b>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-bba73270b77b>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class defconstraints():\n",
    "        def __init__(self, yamlfile):\n",
    "            self.yamlfile = yamlfile      \n",
    "            self.parse_and_set_props()\n",
    "        def parse_and_set_props():\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-9-6c1848a3f1e1>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-6c1848a3f1e1>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class plugins():\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-13-b5fa24ac47b5>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b5fa24ac47b5>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def generate_execution_movie():\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def roofline_model():\n",
    "\n",
    "def generate_execution_movie():\n",
    "    \n",
    "def genvideologger(logger):\n",
    "    # Generate a Time Lapse Video \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def init_logger(log_file=None, log_file_level=logging.NOTSET):\n",
    "    log_format = logging.Formatter(\"[%(asctime)s %(levelname)s] %(message)s\")\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(log_format)\n",
    "    logger.handlers = [console_handler]\n",
    "    logging.setFormatter\n",
    "\n",
    "    if log_file and log_file != '':\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(log_file_level)\n",
    "        file_handler.setFormatter(log_format)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "\n",
    "logging.info(\"Message to log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit ('dl': conda)",
   "language": "python",
   "name": "python36464bitdlconda74ce50dd77ab450dad146f0ee6a8041f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
