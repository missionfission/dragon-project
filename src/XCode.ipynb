{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "from torchvision import models\n",
    "from yaml import dump\n",
    "from dlrm.dlrm_s_pytorch import DLRM_Net, dash_separated_ints, dash_separated_floats\n",
    "from ir.handlers import handlers\n",
    "from ir.trace import trace\n",
    "from ir.trace import get_backprop_memory\n",
    "from utils.logger import create_logger\n",
    "from utils.visualizer import plot_descent\n",
    "from utils.visualizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn import Linear\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "#         self.conv2 = GCNConv(4, 4)\n",
    "#         self.conv3 = GCNConv(4, 2)\n",
    "#         self.classifier = Linear(2, dataset.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "#         h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "#         # Apply a final (linear) classifier.\n",
    "#         out = self.classifier(h)\n",
    "\n",
    "#         return out, h\n",
    "\n",
    "# from torch_geometric.datasets import KarateClub\n",
    "\n",
    "# dataset = KarateClub()\n",
    "# data = dataset[0] \n",
    "# model = GCN()\n",
    "\n",
    "# _, h = model(data.x, data.edge_index)\n",
    "# gnn_graph = trace(model, (data.x, data.edge_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "# import argparse\n",
    "# import os\n",
    "# import random\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.parallel\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# import torch.optim as optim\n",
    "# import torch.utils.data\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')\n",
    "# # parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "# parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "# parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "# parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')\n",
    "# parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "# parser.add_argument('--ngf', type=int, default=64)\n",
    "# parser.add_argument('--ndf', type=int, default=64)\n",
    "# parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "# parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
    "# parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "# parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "# parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "# parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "# parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "# parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "# parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "# parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')\n",
    "\n",
    "# opt = parser.parse_args([])\n",
    "# print(opt)\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(opt.outf)\n",
    "# except OSError:\n",
    "#     pass\n",
    "\n",
    "# if opt.manualSeed is None:\n",
    "#     opt.manualSeed = random.randint(1, 10000)\n",
    "# print(\"Random Seed: \", opt.manualSeed)\n",
    "# random.seed(opt.manualSeed)\n",
    "# torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
    "# ngpu = int(opt.ngpu)\n",
    "# nz = int(opt.nz)\n",
    "# ngf = int(opt.ngf)\n",
    "# ndf = int(opt.ndf)\n",
    "# nc = 3\n",
    "\n",
    "# # custom weights initialization called on netG and netD\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         m.weight.data.normal_(0.0, 0.02)\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         m.weight.data.normal_(1.0, 0.02)\n",
    "#         m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, ngpu):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is Z, going into a convolution\n",
    "#             nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 8),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf*8) x 4 x 4\n",
    "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 4),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf*4) x 8 x 8\n",
    "#             nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 2),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf*2) x 16 x 16\n",
    "#             nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf) x 32 x 32\n",
    "#             nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "#             nn.Tanh()\n",
    "#             # state size. (nc) x 64 x 64\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         if input.is_cuda and self.ngpu > 1:\n",
    "#             output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "#         else:\n",
    "#             output = self.main(input)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# netG = Generator(ngpu).to(device)\n",
    "# netG.apply(weights_init)\n",
    "# if opt.netG != '':\n",
    "#     netG.load_state_dict(torch.load(opt.netG))\n",
    "\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, ngpu):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf) x 32 x 32\n",
    "#             nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 2),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*2) x 16 x 16\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 4),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*4) x 8 x 8\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 8),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*8) x 4 x 4\n",
    "#             nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         if input.is_cuda and self.ngpu > 1:\n",
    "#             output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "#         else:\n",
    "#             output = self.main(input)\n",
    "\n",
    "#         return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "# netD = Discriminator(ngpu).to(device)\n",
    "# netD.apply(weights_init)\n",
    "# if opt.netD != '':\n",
    "#     netD.load_state_dict(torch.load(opt.netD))\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "# real_label = 1\n",
    "# fake_label = 0\n",
    "\n",
    "# # setup optimizer\n",
    "# optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "# optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "# ############################\n",
    "# # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "# ###########################\n",
    "# # train with real\n",
    "# netD.zero_grad()\n",
    "# real_cpu = data[0].to(device)\n",
    "# batch_size = real_cpu.size(0)\n",
    "# label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "\n",
    "# errD_real = criterion(output, label)\n",
    "\n",
    "\n",
    "# D_x = output.mean().item()\n",
    "\n",
    "# # train with fake\n",
    "# noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "# label.fill_(fake_label)\n",
    "\n",
    "# errD_fake = criterion(output, label)\n",
    "\n",
    "# D_G_z1 = output.mean().item()\n",
    "# errD = errD_real + errD_fake\n",
    "# optimizerD.step()\n",
    "\n",
    "# ############################\n",
    "# # (2) Update G network: maximize log(D(G(z)))\n",
    "# ###########################\n",
    "# netG.zero_grad()\n",
    "# label.fill_(real_label)  # fake labels are real for generator cost\n",
    "\n",
    "# errG = criterion(output, label)\n",
    "\n",
    "# D_G_z2 = output.mean().item()\n",
    "# optimizerG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph1 = trace(netD, real_cpu)\n",
    "# output = netD(real_cpu)\n",
    "# errD_real.backward()\n",
    "# graph2 = trace(netG, noise)\n",
    "# fake = netG(noise)\n",
    "# graph3 = trace(netD, fake.detach())\n",
    "# output = netD(fake.detach())\n",
    "# errD_fake.backward()\n",
    "# output = netD(fake)\n",
    "# graph4 = trace(netD, fake)\n",
    "# errG.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from generator import Generator, get_mem_props, get_compute_props\n",
    "from generator import *\n",
    "from utils.visualizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "from utils.logger import create_logger\n",
    "\n",
    "\n",
    "class Scheduling:\n",
    "    def __init__(self, hwfile=\"default.yaml\"):\n",
    "        base_dir = \"configs/\"\n",
    "        self.total_cycles = 0\n",
    "        self.technology = [1,1,40] \n",
    "        # maybe change this later to peripheral logic node or speed\n",
    "#     [wire_cap , sense_amp_time, plogic_node],\n",
    "        self.logger = create_logger(\"logs/stats.txt\")\n",
    "        self.config = self.create_config(\n",
    "            yaml.load(open(base_dir + hwfile), Loader=yamlordereddictloader.Loader)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(self, config):\n",
    "\n",
    "    self.logger.info(\"Config Statistics : \")\n",
    "\n",
    "    self.mle = config[\"memory_levels\"]\n",
    "    self.mem_energy = np.zeros((self.mle))\n",
    "    self.compute_energy = 0\n",
    "    self.mem_read_access = np.zeros((self.mle))\n",
    "    self.mem_write_access = np.zeros((self.mle))\n",
    "    self.mem_size = np.zeros((self.mle))\n",
    "    self.mem_util = np.zeros((self.mle))\n",
    "    self.mem_free = np.zeros((self.mle))\n",
    "    self.mem_read_bw = np.zeros((self.mle))\n",
    "    self.mem_write_bw = np.zeros((self.mle))\n",
    "    self.internal_bandwidth_time = 0\n",
    "    self.total_cycles = 0 \n",
    "    self.bandwidth_idle_time = 0\n",
    "    self.compute_idle_time = 0\n",
    "    self.mem_size_idle_time = 0\n",
    "\n",
    "    self.force_connectivity = False\n",
    "    mm_compute = config[\"mm_compute\"]\n",
    "    vector_compute = config[\"vector_compute\"]\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"systolic_array\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"]) ** 2) * mm_compute[\"N_PE\"] / (2*4)\n",
    "        )\n",
    "        config[\"comp_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "        self.logger.info(\n",
    "            \"MM Compute per cycle : %d\", config[\"mm_compute_per_cycle\"]\n",
    "        )\n",
    "        self.logger.info(\"Compute Bandwidth Required : %d\", config[\"comp_bw\"])\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"mac\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"])) * mm_compute[\"N_PE\"] / 2\n",
    "        )\n",
    "        config[\"comp_read_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "    for i in range(self.mle):\n",
    "        memory = config[\"memory\"][\"level\" + str(i)]\n",
    "        self.mem_read_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"read_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_write_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"write_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_size[i] = memory[\"size\"]\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Memory at Level %d, Read Bandwidth %d Write Bandwidth %d\",\n",
    "            i,\n",
    "            self.mem_read_bw[i],\n",
    "            self.mem_write_bw[i],\n",
    "        )\n",
    "    for i in range(self.mle - 1):\n",
    "        memory = config[\"memory\"][\"level\" + str(i)]\n",
    "        read_energy, write_energy, leakage_power = get_mem_props(\n",
    "            memory[\"size\"], memory[\"width\"], memory[\"banks\"]\n",
    "        )\n",
    "        config[\"memory\"][\"level\" + str(i)][\"read_energy\"] = str(read_energy)\n",
    "        config[\"memory\"][\"level\" + str(i)][\"write_energy\"] = str(write_energy)\n",
    "        config[\"memory\"][\"level\" + str(i)][\"leakage_power\"] = str(leakage_power)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, graph):\n",
    "\n",
    "    \"\"\"\n",
    "     Check both size, utilization and bandwidths at every node\n",
    "     What about memory size that can also get exhausted ?\n",
    "     So if memory size is exhausted, then have to go to a previous level and write there ?\n",
    "     if any level utilization is exhausted then only the immediate memory required will be kept.\n",
    "     if the memory is empty in size, but is not bandwidth, it is useless?\n",
    "     Cannot do prefetching\n",
    "     Read access of the next node will decrease\n",
    "     Bandwidth is available but size is not?, can do prefetching, but now the memory fetches have to check, \n",
    "     whether to do fetches of the same node or a different node\n",
    "     Say bandwidth at level0 is sufficient, at level1 is insufficient, then at level1 we have a bottlenecks\n",
    "     slower so it will take its own time\n",
    "     Do vector operations in the meantime perhaps ? \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config = self.config\n",
    "\n",
    "    read_bw_req = []\n",
    "    write_bw_req = []\n",
    "    read_bw_actual = []\n",
    "    write_bw_actual = []\n",
    "    cycles = []\n",
    "    free_cycles = []\n",
    "    transferable_checkpointed_edge = []\n",
    "    all_checkpointed_edge = []\n",
    "    self.mem_util_log=[]\n",
    "    self.mem_util_full=[]\n",
    "    # Mem Fetch time of the last Nodes\n",
    "#     print(self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "\n",
    "    mem_free = True\n",
    "    for n, node in enumerate(graph.nodes):\n",
    "\n",
    "        # These are last level read/write accesses\n",
    "        compute_expense, weights = node.get_stats()\n",
    "        read_access = node.mem_fetch\n",
    "        write_access = 0\n",
    "        self.mem_read_access[1]+=(weights)\n",
    "        \n",
    "        self.mem_util[0] += node.in_edge_mem\n",
    "        node.mem_util = node.out_edge_mem + node.mem_fetch\n",
    "        # Total Free memory\n",
    "        for i in range(self.mle - 1):\n",
    "            self.mem_free[i] = self.mem_size[i] - self.mem_util[i]\n",
    "            \n",
    "#         print(\"2\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        time_compute = compute_expense / config[\"mm_compute_per_cycle\"]\n",
    "        read_bw_ll = read_access / (time_compute)\n",
    "        write_bw_ll = write_access / (time_compute)\n",
    "        step_cycles = time_compute\n",
    "        read_bw_req.append(read_bw_ll)\n",
    "        write_bw_req.append(write_bw_ll)\n",
    "        free_cycles.append(step_cycles)\n",
    "#         print(\"bandwidth\",read_bw_ll, write_bw_ll, step_cycles) \n",
    "        \n",
    "        if self.mem_free[0] < node.mem_util:\n",
    "            mem_free = False\n",
    "            # node mem_util = output edge\n",
    "            self.logger.info(\"Memory size is too low/ Memory is Full\")\n",
    "            self.logger.info(\"Node or Node memory Requirements too high\")\n",
    "            # Rearrange the checkpointed_nodes\n",
    "            # rearrange = True\n",
    "\n",
    "            # Is it possible now : Otherwise update the last level memory bandwidth requirements\n",
    "            assert (self.mem_free[0]+node.in_edge_mem)>0\n",
    "            # Change this later with the number of solid total cycles\n",
    "            used_pe_ratio = (self.mem_free[0]+node.in_edge_mem)/(node.mem_util+node.in_edge_mem)\n",
    "            n_swaps = int(1/used_pe_ratio+1)\n",
    "            swap_time = max(config[\"mm_compute\"][\"size\"]*4,time_compute//n_swaps)\n",
    "            self.mem_size_idle_time += swap_time*n_swaps + (node.mem_util+node.in_edge_mem)//self.mem_read_bw[self.mle-1] \n",
    "            step_cycles+=self.mem_size_idle_time\n",
    "            self.mem_read_access[0]+=(node.mem_util+node.in_edge_mem)\n",
    "            self.mem_write_access[0]+=(node.mem_util+node.in_edge_mem)\n",
    "        else:\n",
    "            self.mem_util[0] += node.mem_util\n",
    "            self.mem_free[0] -= node.mem_util\n",
    "#         print(\"2.5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_log.append(self.mem_util[0])\n",
    "        self.mem_read_access[0]+=node.weights+node.out_edge_mem\n",
    "        self.mem_write_access[0]+=node.weights+node.out_edge_mem\n",
    "        assert(self.mem_free[0] < self.mem_size[0])\n",
    "        # Last level memory fetch takes more time, so that may be a bottleneck\n",
    "        bandwidth_available = read_bw_ll < self.mem_read_bw[self.mle - 1]\n",
    "        \n",
    "        # If Bandwidth is not available : Cannot Prefetch\n",
    "        if (bandwidth_available) == False:\n",
    "            step_cycles += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "            self.bandwidth_idle_time += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "\n",
    "        # If memory is not free for the next node and Bandwidth is available : Move nodes back and forth\n",
    "        # if(total_mem_free[0] == 0 and (bandwidth_available)):\n",
    "        # for(nodes in checkpointed_nodes):\n",
    "        # checkpointed but not immediate node\n",
    "\n",
    "        # Check if memory is free and Bandwidth available : From the Data Dependence Graph, Prefetch new node\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        if self.mem_free[0] > 0 and (bandwidth_available):\n",
    "            if n < len(graph.nodes) - 1:\n",
    "                if self.mem_free[0] > node.next.mem_fetch:\n",
    "                    read_access += node.next.mem_fetch\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        self.mem_util[0] += node.next.mem_fetch\n",
    "                        self.mem_free[0] -= node.next.mem_fetch\n",
    "                        node.next.mem_fetch = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_fetch = read_access - read_bw_ll * step_cycles\n",
    "\n",
    "                else:\n",
    "                    read_access += self.mem_free[0]\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        node.next.mem_fetch = node.next.mem_fetch - self.mem_free[0]\n",
    "                        self.mem_util[0] = self.mem_size[0]\n",
    "                        self.mem_free[0] = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_fetch = read_access - read_bw_ll * step_cycles\n",
    "#         print(\"3\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_full.append(self.mem_util[0])   \n",
    "        \n",
    "            # TODO Consider Write bandwidth for a block read memory or Write Bandwidth  for a endurance purposes\n",
    "        self.mem_util[0] -= node.in_edge_mem\n",
    "#         print(\"4\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "    \n",
    "        if mem_free:\n",
    "            self.mem_util[0] -= node.mem_util\n",
    "#         print(\"5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Node operator %r, Step Cycles %d, Read Accesses %d, Write Accesses %d \",\n",
    "            node.operator,\n",
    "            step_cycles,\n",
    "            read_access,\n",
    "            write_access,\n",
    "        )\n",
    "        self.total_cycles += step_cycles\n",
    "        cycles.append(step_cycles)\n",
    "        read_bw_actual.append(read_access / step_cycles)\n",
    "        write_bw_actual.append(write_access / step_cycles)\n",
    "#         print(\"actual\",read_access / step_cycles, write_access / step_cycles, step_cycles)\n",
    "#     print(\"The total cycles are \", self.total_cycles)\n",
    "    return read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduling.create_config = create_config\n",
    "Scheduling.run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Forward and Runner Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner_forward(graph_set):\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\" \n",
    "    generator = Generator()\n",
    "    bandwidth = [2,10,50,75,100]\n",
    "    num_iterations = 50\n",
    "    for graph in graph_set:\n",
    "#     for i in range(len(bandwidth)):\n",
    "        scheduler = Scheduling()\n",
    "#         scheduler.mem_read_bw[1] = 10*bandwidth[i]\n",
    "        read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "        read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "#         bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n",
    "#         cycles_bar_graph(\"cycles.png\", cycles, free_cycles, graph.nodes)\n",
    "#         mem_util_bar_graph(\"mem_util.png\",scheduler.mem_util_full/scheduler.mem_size[0],scheduler.mem_util_log/scheduler.mem_size[0], graph.nodes)\n",
    "        in_time, in_energy, design, tech = generator.save_stats(scheduler)\n",
    "#         in_time, in_energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        print(\"======Optimizing Design and Connectivity=========\")\n",
    "        for i in range(num_iterations):\n",
    "            config = generator.backward_pass(scheduler)\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            _,_,_,_, cycles, free_cycles = scheduler.run(graph)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "#             time, energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        print(in_time[0]//time[0], in_energy[0]//energy[0])\n",
    "        print(\"===============Optimizing Technology=============\")\n",
    "        for i in range(10):\n",
    "            config = generator.backward_pass_tech(scheduler,\"time\")\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            _,_,_,_, cycles, free_cycles = scheduler.run(graph)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "#             time, energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        print(in_time[0]//time[0], in_energy[0]//energy[0])\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from models import vggnet_graph\n",
    "# print(\"=====================================VGG-Net=============================================\")\n",
    "# vgg11_graph = vggnet_graph()\n",
    "# runner_forward([vgg11_graph])\n",
    "# print(\"=====================================ResNet===============================================\")\n",
    "# runner_forward([resnet_graph])\n",
    "# runner_forward([dlrm_graph])\n",
    "# runner_forward([gnn_graph])\n",
    "# runner_forward([dart_graph])\n",
    "# runner_forward([bert_graph])\n",
    "# runner_forward([gpt2_graph])\n",
    "# runner_forward([a3c_graph]) # Actor Critic\n",
    "# runner_forward([gan_graph]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(graph_set, scheduler):\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\" \n",
    "    time_list = []\n",
    "    energy_list=[]\n",
    "    bandwidth_time_list = []\n",
    "    mem_size_idle_time_list=[]\n",
    "    bank_list=[]\n",
    "    mem_size_list=[]\n",
    "    compute_list=[]\n",
    "    tech_params_list = []\n",
    "    num_iterations = 6\n",
    "    generator = Generator()\n",
    "    bandwidth = [2,10,50,75,100]\n",
    "    for graph in graph_set:\n",
    "#     for i in range(len(bandwidth)):\n",
    "#         scheduler.mem_read_bw[1] = 10*bandwidth[i]\n",
    "        read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "        read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "#         bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n",
    "#         cycles_bar_graph(\"cycles.png\", cycles, free_cycles, graph.nodes)\n",
    "#         mem_util_bar_graph(\"mem_util.png\",scheduler.mem_util_full/scheduler.mem_size[0],scheduler.mem_util_log/scheduler.mem_size[0], graph.nodes)\n",
    "#         generator.save_statistics(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        time, energy, design, tech = generator.save_stats(scheduler)\n",
    "        time_list.append(time[0])\n",
    "        energy_list.append(energy[0])\n",
    "        bandwidth_time_list.append(time[1])\n",
    "        mem_size_idle_time_list.append(time[2])\n",
    "        bank_list.append(design[0])\n",
    "        mem_size_list.append(design[1])\n",
    "        tech_params_list.append(tech)\n",
    "        #         print(scheduler.config)\n",
    "        for i in range(num_iterations):\n",
    "            config = generator.backward_pass(scheduler, \"time\")\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "#             read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "# #             bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes, cycles)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "            time_list.append(time[0])\n",
    "            energy_list.append(energy[0])\n",
    "            bandwidth_time_list.append(time[1])\n",
    "            mem_size_idle_time_list.append(time[2])\n",
    "            bank_list.append(design[0])\n",
    "            mem_size_list.append(design[1])\n",
    "            tech_params_list.append(tech)\n",
    "        return [time_list, bandwidth_time_list, mem_size_idle_time_list, bank_list, mem_size_list, compute_list, tech_params_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Scheduling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a97c0d3b640a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScheduling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandwidth_time_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_size_idle_time_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbank_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_size_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtech_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvgg11_graph\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandwidth_time_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_size_idle_time_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtech_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot Design Parameters change over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Scheduling' is not defined"
     ]
    }
   ],
   "source": [
    "scheduler = Scheduling()\n",
    "time_list, bandwidth_time_list, mem_size_idle_time_list, bank_list, mem_size_list, compute_list, tech_params = runner([vgg11_graph], scheduler)\n",
    "plot_descent(time_list, bandwidth_time_list, mem_size_idle_time_list)\n",
    "print(tech_params)\n",
    "# Plot Design Parameters change over time\n",
    "plot_design_param_change(bank_list, mem_size_list, compute_list)\n",
    "# Plot Technology Parameters change over time\n",
    "plot_tech_param_change(np.transpose(np.array(tech_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Sweep Memory Banks/Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax2 = ax.twinx()\n",
    "# base_dir = \"figures/\"\n",
    "# error_config = {\"ecolor\": \"0.3\"}\n",
    "# index = np.arange(6)\n",
    "# for j in range(5):\n",
    "#     scheduler = Scheduling()\n",
    "#     scheduler.config[\"memory\"][\"level1\"][\"frequency\"]=5\n",
    "#     scheduler.config[\"memory\"][\"level1\"][\"frequency\"]*=2**j\n",
    "#     time_list, bandwidth_time_list,_,_,_= runner([vgg11_graph],scheduler)\n",
    "#     plot_descent_multiple(ax, time_list, bandwidth_time_list)\n",
    "#     plot_parameter_change_multiple(ax2, bank_list)\n",
    "\n",
    "# ax.legend(fontsize=20)\n",
    "# ax.set_xticks(index)\n",
    "# plt.xticks(rotation=80)\n",
    "# plt.rc(\"xtick\", labelsize=20)  # fontsize of the tick labels\n",
    "# plt.rc(\"ytick\", labelsize=20)\n",
    "# ax.set_ylabel(\"Grad descent time\", fontsize=20, fontweight=\"bold\")\n",
    "# ax2.set_ylabel(\"Grad descent parameters\", fontsize=20, fontweight=\"bold\")\n",
    "# fig.tight_layout()\n",
    "# plt.yscale(\"log\")\n",
    "# plt.savefig(base_dir + \"multiple_bandwidth.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Sweep Memory Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax2 = ax.twinx()\n",
    "# base_dir = \"figures/\"\n",
    "# error_config = {\"ecolor\": \"0.3\"}\n",
    "# index = np.arange(6)\n",
    "# for i in range(5,10):\n",
    "#     scheduler = Scheduling()\n",
    "#     scheduler.config[\"memory\"][\"level0\"][\"size\"]=10**(i/2)\n",
    "#     time_list, bandwidth_time_list,mem_size_idle_time_list, bank_list, mem_size_list, compute_list = runner([vgg11_graph],scheduler)\n",
    "#     plot_descent_multiple(ax, time_list, mem_size_idle_time_list)\n",
    "#     plot_parameter_change_multiple(ax2, mem_size_list)\n",
    "\n",
    "# ax.legend(fontsize=20)\n",
    "# ax.set_xticks(index)\n",
    "# plt.xticks(rotation=80)\n",
    "# plt.rc(\"xtick\", labelsize=20)  # fontsize of the tick labels\n",
    "# plt.rc(\"ytick\", labelsize=20)\n",
    "# ax.set_ylabel(\"Grad descent time\", fontsize=20, fontweight=\"bold\")\n",
    "# ax2.set_ylabel(\"Grad descent parameters\", fontsize=20, fontweight=\"bold\")\n",
    "# fig.tight_layout()\n",
    "# plt.yscale(\"log\")\n",
    "# plt.savefig(base_dir + \"memory_size_multiple.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technology Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "d = pd.read_csv('tables/sram.csv')\n",
    "a = np.array(d)\n",
    "\n",
    "input = a[:,:3]\n",
    "# output = a[:,3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('plugins/cacti/bus_width.out')\n",
    "\n",
    "\n",
    "d = d.drop(d.columns[[0,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]], axis=1)\n",
    "\n",
    "d = d.drop(' Associativity',1)\n",
    "d = d.drop(' Dynamic search energy (nJ)',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array(d)\n",
    "np.savetxt('bus.csv',a, fmt='%.18e', delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph(\"read_dummy.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_table = np.array(pd.read_csv(\"tables/sram.csv\", header=None))\n",
    "a = mem_table[np.where(mem_table[:, 1] == 4)]\n",
    "a = a[np.where(a[:, 2] == 32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute:\n",
    "    name       : MACs\n",
    "        class      : mac\n",
    "        attributes :            \n",
    "        instances       : 256\n",
    "        meshX           : 16\n",
    "        word-bits       : 16\n",
    "\n",
    "    memory: \n",
    "        name       : DRAM\n",
    "        class      : DRAM\n",
    "        attributes :\n",
    "        instances       : 1\n",
    "        word-bits       : 16\n",
    "  \n",
    "    name       : OutputBuffer\n",
    "        class      : SRAM\n",
    "        attributes :\n",
    "        entries         : 1024  # 64 * 16 = 1024\n",
    "        instances       : 1\n",
    "        meshX           : 1\n",
    "        word-bits       : 16\n",
    "        block-size      : 16\n",
    "        read_bandwidth  : 16 # words/cycle\n",
    "        write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : InputBuffer\n",
    "            class      : SRAM\n",
    "            attributes :\n",
    "            entries         : 1024 # 64 * 16 = 1024\n",
    "            instances       : 1\n",
    "            meshX           : 1\n",
    "            word-bits       : 16\n",
    "            block-size      : 16\n",
    "            read_bandwidth  : 16 # words/cycle\n",
    "            write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : PsumRegFile\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 1\n",
    "            instances       : 16\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 16\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "            \n",
    "        name       : WeightBuffer\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 64\n",
    "            instances       : 256\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 256\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "    noc:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_rep = make_graph_from_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('detectron': conda)",
   "language": "python",
   "name": "python38264bitdetectroncondad1a58fa40caf4642914849668b1a87c9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
