{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "# from torchvision import models\n",
    "from yaml import dump\n",
    "from dlrm.dlrm_s_pytorch import DLRM_Net, dash_separated_ints, dash_separated_floats\n",
    "from ir.handlers import handlers\n",
    "from ir.trace import trace\n",
    "from ir.trace import get_backprop_memory\n",
    "from utils.logger import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import argparse\n",
    "# import dlrm.dlrm_data_pytorch as dp\n",
    "# ### parse arguments ###\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description=\"Train Deep Learning Recommendation Model (DLRM)\"\n",
    "# )\n",
    "# # model related parameters\n",
    "# parser.add_argument(\"--arch-sparse-feature-size\", type=int, default=2)\n",
    "# parser.add_argument(\n",
    "#     \"--arch-embedding-size\", type=dash_separated_ints, default=\"4-3-2\"\n",
    "# )\n",
    "# # j will be replaced with the table number\n",
    "# parser.add_argument(\"--arch-mlp-bot\", type=dash_separated_ints, default=\"4-3-2\")\n",
    "# parser.add_argument(\"--arch-mlp-top\", type=dash_separated_ints, default=\"4-2-1\")\n",
    "# parser.add_argument(\n",
    "#     \"--arch-interaction-op\", type=str, choices=[\"dot\", \"cat\"], default=\"dot\"\n",
    "# )\n",
    "# parser.add_argument(\"--arch-interaction-itself\", action=\"store_true\", default=False)\n",
    "# # embedding table options\n",
    "# parser.add_argument(\"--md-flag\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--md-threshold\", type=int, default=200)\n",
    "# parser.add_argument(\"--md-temperature\", type=float, default=0.3)\n",
    "# parser.add_argument(\"--md-round-dims\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--qr-flag\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--qr-threshold\", type=int, default=200)\n",
    "# parser.add_argument(\"--qr-operation\", type=str, default=\"mult\")\n",
    "# parser.add_argument(\"--qr-collisions\", type=int, default=4)\n",
    "# # activations and loss\n",
    "# parser.add_argument(\"--activation-function\", type=str, default=\"relu\")\n",
    "# parser.add_argument(\"--loss-function\", type=str, default=\"mse\")  # or bce or wbce\n",
    "# parser.add_argument(\n",
    "#     \"--loss-weights\", type=dash_separated_floats, default=\"1.0-1.0\"\n",
    "# )  # for wbce\n",
    "# parser.add_argument(\"--loss-threshold\", type=float, default=0.0)  # 1.0e-7\n",
    "# parser.add_argument(\"--round-targets\", type=bool, default=False)\n",
    "# # data\n",
    "# parser.add_argument(\"--data-size\", type=int, default=1)\n",
    "# parser.add_argument(\"--num-batches\", type=int, default=0)\n",
    "# parser.add_argument(\n",
    "#     \"--data-generation\", type=str, default=\"random\"\n",
    "# )  # synthetic or dataset\n",
    "# parser.add_argument(\"--data-trace-file\", type=str, default=\"./input/dist_emb_j.log\")\n",
    "# parser.add_argument(\"--data-set\", type=str, default=\"kaggle\")  # or terabyte\n",
    "# parser.add_argument(\"--raw-data-file\", type=str, default=\"\")\n",
    "# parser.add_argument(\"--processed-data-file\", type=str, default=\"\")\n",
    "# parser.add_argument(\"--data-randomize\", type=str, default=\"total\")  # or day or none\n",
    "# parser.add_argument(\"--data-trace-enable-padding\", type=bool, default=False)\n",
    "# parser.add_argument(\"--max-ind-range\", type=int, default=-1)\n",
    "# parser.add_argument(\"--data-sub-sample-rate\", type=float, default=0.0)  # in [0, 1]\n",
    "# parser.add_argument(\"--num-indices-per-lookup\", type=int, default=10)\n",
    "# parser.add_argument(\"--num-indices-per-lookup-fixed\", type=bool, default=False)\n",
    "# parser.add_argument(\"--num-workers\", type=int, default=0)\n",
    "# parser.add_argument(\"--memory-map\", action=\"store_true\", default=False)\n",
    "# # training\n",
    "# parser.add_argument(\"--mini-batch-size\", type=int, default=1)\n",
    "# parser.add_argument(\"--nepochs\", type=int, default=1)\n",
    "# parser.add_argument(\"--learning-rate\", type=float, default=0.01)\n",
    "# parser.add_argument(\"--print-precision\", type=int, default=5)\n",
    "# parser.add_argument(\"--numpy-rand-seed\", type=int, default=123)\n",
    "# parser.add_argument(\"--sync-dense-params\", type=bool, default=True)\n",
    "# # inference\n",
    "# parser.add_argument(\"--inference-only\", action=\"store_true\", default=False)\n",
    "# # onnx\n",
    "# parser.add_argument(\"--save-onnx\", action=\"store_true\", default=False)\n",
    "# # gpu\n",
    "# parser.add_argument(\"--use-gpu\", action=\"store_true\", default=False)\n",
    "# # debugging and profiling\n",
    "# parser.add_argument(\"--print-freq\", type=int, default=1)\n",
    "# parser.add_argument(\"--test-freq\", type=int, default=-1)\n",
    "# parser.add_argument(\"--test-mini-batch-size\", type=int, default=-1)\n",
    "# parser.add_argument(\"--test-num-workers\", type=int, default=-1)\n",
    "# parser.add_argument(\"--print-time\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--debug-mode\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--enable-profiling\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--plot-compute-graph\", action=\"store_true\", default=False)\n",
    "# # store/load model\n",
    "# parser.add_argument(\"--save-model\", type=str, default=\"\")\n",
    "# parser.add_argument(\"--load-model\", type=str, default=\"\")\n",
    "# # mlperf logging (disables other output and stops early)\n",
    "# parser.add_argument(\"--mlperf-logging\", action=\"store_true\", default=False)\n",
    "# # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107\n",
    "# parser.add_argument(\"--mlperf-acc-threshold\", type=float, default=0.0)\n",
    "# # stop at target AUC Terabyte (no subsampling) 0.8025\n",
    "# parser.add_argument(\"--mlperf-auc-threshold\", type=float, default=0.0)\n",
    "# parser.add_argument(\"--mlperf-bin-loader\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--mlperf-bin-shuffle\", action=\"store_true\", default=False)\n",
    "# # LR policy\n",
    "# parser.add_argument(\"--lr-num-warmup-steps\", type=int, default=0)\n",
    "# parser.add_argument(\"--lr-decay-start-step\", type=int, default=0)\n",
    "# parser.add_argument(\"--lr-num-decay-steps\", type=int, default=0)\n",
    "# args = parser.parse_args([])\n",
    "\n",
    "# if args.mlperf_logging:\n",
    "#     print(\"command line args: \", json.dumps(vars(args)))\n",
    "\n",
    "# ### some basic setup ###\n",
    "# np.random.seed(args.numpy_rand_seed)\n",
    "# np.set_printoptions(precision=args.print_precision)\n",
    "# torch.set_printoptions(precision=args.print_precision)\n",
    "# torch.manual_seed(args.numpy_rand_seed)\n",
    "\n",
    "# if args.test_mini_batch_size < 0:\n",
    "#     # if the parameter is not set, use the training batch size\n",
    "#     args.test_mini_batch_size = args.mini_batch_size\n",
    "# if args.test_num_workers < 0:\n",
    "#     # if the parameter is not set, use the same parameter for training\n",
    "#     args.test_num_workers = args.num_workers\n",
    "\n",
    "# use_gpu = args.use_gpu and torch.cuda.is_available()\n",
    "# if use_gpu:\n",
    "#     torch.cuda.manual_seed_all(args.numpy_rand_seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     device = torch.device(\"cuda\", 0)\n",
    "#     ngpus = torch.cuda.device_count()  # 1\n",
    "#     print(\"Using {} GPU(s)...\".format(ngpus))\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"Using CPU...\")\n",
    "\n",
    "# ### prepare training data ###\n",
    "# ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=\"-\")\n",
    "# # input data\n",
    "# if args.data_generation == \"dataset\":\n",
    "\n",
    "#     train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)\n",
    "#     nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n",
    "#     nbatches_test = len(test_ld)\n",
    "\n",
    "#     ln_emb = train_data.counts\n",
    "#     # enforce maximum limit on number of vectors per embedding\n",
    "#     if args.max_ind_range > 0:\n",
    "#         ln_emb = np.array(\n",
    "#             list(\n",
    "#                 map(\n",
    "#                     lambda x: x if x < args.max_ind_range else args.max_ind_range,\n",
    "#                     ln_emb,\n",
    "#                 )\n",
    "#             )\n",
    "#         )\n",
    "#     m_den = train_data.m_den\n",
    "#     ln_bot[0] = m_den\n",
    "# else:\n",
    "#     # input and target at random\n",
    "#     ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=\"-\")\n",
    "#     m_den = ln_bot[0]\n",
    "#     train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)\n",
    "#     nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n",
    "\n",
    "# ### parse command line arguments ###\n",
    "# m_spa = args.arch_sparse_feature_size\n",
    "# num_fea = ln_emb.size + 1  # num sparse + num dense features\n",
    "# m_den_out = ln_bot[ln_bot.size - 1]\n",
    "# if args.arch_interaction_op == \"dot\":\n",
    "#     # approach 1: all\n",
    "#     # num_int = num_fea * num_fea + m_den_out\n",
    "#     # approach 2: unique\n",
    "#     if args.arch_interaction_itself:\n",
    "#         num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out\n",
    "#     else:\n",
    "#         num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out\n",
    "# elif args.arch_interaction_op == \"cat\":\n",
    "#     num_int = num_fea * m_den_out\n",
    "# else:\n",
    "#     sys.exit(\n",
    "#         \"ERROR: --arch-interaction-op=\"\n",
    "#         + args.arch_interaction_op\n",
    "#         + \" is not supported\"\n",
    "#     )\n",
    "# arch_mlp_top_adjusted = str(num_int) + \"-\" + args.arch_mlp_top\n",
    "# ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=\"-\")\n",
    "\n",
    "# # sanity check: feature sizes and mlp dimensions must match\n",
    "# if m_den != ln_bot[0]:\n",
    "#     sys.exit(\n",
    "#         \"ERROR: arch-dense-feature-size \"\n",
    "#         + str(m_den)\n",
    "#         + \" does not match first dim of bottom mlp \"\n",
    "#         + str(ln_bot[0])\n",
    "#     )\n",
    "# if args.qr_flag:\n",
    "#     if args.qr_operation == \"concat\" and 2 * m_spa != m_den_out:\n",
    "#         sys.exit(\n",
    "#             \"ERROR: 2 arch-sparse-feature-size \"\n",
    "#             + str(2 * m_spa)\n",
    "#             + \" does not match last dim of bottom mlp \"\n",
    "#             + str(m_den_out)\n",
    "#             + \" (note that the last dim of bottom mlp must be 2x the embedding dim)\"\n",
    "#         )\n",
    "#     if args.qr_operation != \"concat\" and m_spa != m_den_out:\n",
    "#         sys.exit(\n",
    "#             \"ERROR: arch-sparse-feature-size \"\n",
    "#             + str(m_spa)\n",
    "#             + \" does not match last dim of bottom mlp \"\n",
    "#             + str(m_den_out)\n",
    "#         )\n",
    "# else:\n",
    "#     if m_spa != m_den_out:\n",
    "#         sys.exit(\n",
    "#             \"ERROR: arch-sparse-feature-size \"\n",
    "#             + str(m_spa)\n",
    "#             + \" does not match last dim of bottom mlp \"\n",
    "#             + str(m_den_out)\n",
    "#         )\n",
    "# if num_int != ln_top[0]:\n",
    "#     sys.exit(\n",
    "#         \"ERROR: # of feature interactions \"\n",
    "#         + str(num_int)\n",
    "#         + \" does not match first dimension of top mlp \"\n",
    "#         + str(ln_top[0])\n",
    "#     )\n",
    "\n",
    "# # assign mixed dimensions if applicable\n",
    "# if args.md_flag:\n",
    "#     m_spa = md_solver(\n",
    "#         torch.tensor(ln_emb),\n",
    "#         args.md_temperature,  # alpha\n",
    "#         d0=m_spa,\n",
    "#         round_dim=args.md_round_dims,\n",
    "#     ).tolist()\n",
    "\n",
    "# # test prints (model arch)\n",
    "# if args.debug_mode:\n",
    "#     print(\"model arch:\")\n",
    "#     print(\n",
    "#         \"mlp top arch \"\n",
    "#         + str(ln_top.size - 1)\n",
    "#         + \" layers, with input to output dimensions:\"\n",
    "#     )\n",
    "#     print(ln_top)\n",
    "#     print(\"# of interactions\")\n",
    "#     print(num_int)\n",
    "#     print(\n",
    "#         \"mlp bot arch \"\n",
    "#         + str(ln_bot.size - 1)\n",
    "#         + \" layers, with input to output dimensions:\"\n",
    "#     )\n",
    "#     print(ln_bot)\n",
    "#     print(\"# of features (sparse and dense)\")\n",
    "#     print(num_fea)\n",
    "#     print(\"dense feature size\")\n",
    "#     print(m_den)\n",
    "#     print(\"sparse feature size\")\n",
    "#     print(m_spa)\n",
    "#     print(\n",
    "#         \"# of embeddings (= # of sparse features) \"\n",
    "#         + str(ln_emb.size)\n",
    "#         + \", with dimensions \"\n",
    "#         + str(m_spa)\n",
    "#         + \"x:\"\n",
    "#     )\n",
    "#     print(ln_emb)\n",
    "\n",
    "#     print(\"data (inputs and targets):\")\n",
    "#     for j, (X, lS_o, lS_i, T) in enumerate(train_ld):\n",
    "#         # early exit if nbatches was set by the user and has been exceeded\n",
    "#         if nbatches > 0 and j >= nbatches:\n",
    "#             break\n",
    "\n",
    "#         print(\"mini-batch: %d\" % j)\n",
    "#         print(X.detach().cpu().numpy())\n",
    "#         # transform offsets to lengths when printing\n",
    "#         print(\n",
    "#             [\n",
    "#                 np.diff(S_o.detach().cpu().tolist() + list(lS_i[i].shape)).tolist()\n",
    "#                 for i, S_o in enumerate(lS_o)\n",
    "#             ]\n",
    "#         )\n",
    "#         print([S_i.detach().cpu().tolist() for S_i in lS_i])\n",
    "#         print(T.detach().cpu().numpy())\n",
    "\n",
    "# ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1\n",
    "\n",
    "# ### construct the neural network specified above ###\n",
    "# # WARNING: to obtain exactly the same initialization for\n",
    "# # the weights we need to start from the same random seed.\n",
    "# # np.random.seed(args.numpy_rand_seed)\n",
    "# dlrm = DLRM_Net(\n",
    "#     m_spa,\n",
    "#     ln_emb,\n",
    "#     ln_bot,\n",
    "#     ln_top,\n",
    "#     arch_interaction_op=args.arch_interaction_op,\n",
    "#     arch_interaction_itself=args.arch_interaction_itself,\n",
    "#     sigmoid_bot=-1,\n",
    "#     sigmoid_top=ln_top.size - 2,\n",
    "#     sync_dense_params=args.sync_dense_params,\n",
    "#     loss_threshold=args.loss_threshold,\n",
    "#     ndevices=ndevices,\n",
    "#     qr_flag=args.qr_flag,\n",
    "#     qr_operation=args.qr_operation,\n",
    "#     qr_collisions=args.qr_collisions,\n",
    "#     qr_threshold=args.qr_threshold,\n",
    "#     md_flag=args.md_flag,\n",
    "#     md_threshold=args.md_threshold,\n",
    "# )\n",
    "# for j, (X, lS_o, lS_i, T) in enumerate(train_ld):\n",
    "#     Z = dlrm(X, lS_o, lS_i)\n",
    "# graph = trace(dlrm, (X, lS_o, lS_i))\n",
    "# print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just preload the graph for fast experiments \n",
    "# Create Graph\n",
    "\n",
    "# for name, model in models.__dict__.items():\n",
    "# #     print(name)\n",
    "#     if not name.islower() or name.startswith(\"__\") or not callable(model):\n",
    "#         continue\n",
    "#     if \"vgg11\" in name and \"vgg11_bn\" not in name :\n",
    "#         inputs = torch.randn(1, 3, 299, 299)\n",
    "#         vgg11_graph = trace(model().eval(), inputs)\n",
    "#         break\n",
    "#         print(vgg11_graph)\n",
    "#     if \"resnet50\" in name:\n",
    "# #         model = model().eval()\n",
    "#         inputs = torch.randn(1, 3, 100, 100)\n",
    "#         graph = trace(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import BertModel, BertConfig\n",
    "# from transformers import GPT2Model, GPT2Config\n",
    "# # configuration = BertConfig()\n",
    "# # model = BertModel(configuration)\n",
    "\n",
    "# configuration = GPT2Config()\n",
    "# model = GPT2Model(configuration)\n",
    "# # model.configMM\n",
    "\n",
    "# # tokenizer = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'tokenizer', 'bert-base-cased', do_basic_tokenize=False)\n",
    "\n",
    "# # Tokenized input\n",
    "# # text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "# # tokenized_text = tokenizer.tokenize(text)\n",
    "# # indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# # print(indexed_tokens)\n",
    "# ### Get the hidden states computed by `bertModel`\n",
    "# # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "# segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# indexed_tokens = [101, 2627, 1108, 3104, 1124, 15703, 136, 102, 3104, 1124, 15703, 1108, 170, 16797, 8284, 102]\n",
    "\n",
    "# # Convert inputs to PyTorch tensors\n",
    "# segments_tensors = torch.tensor([segments_ids])\n",
    "# tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# # model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'model', 'bert-base-cased')\n",
    "# model.eval()\n",
    "\n",
    "# model(tokens_tensor)\n",
    "# graph = trace(model, tokens_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dgl.nn.pytorch import GraphConv\n",
    "# import dgl\n",
    "# import numpy as np\n",
    "\n",
    "# def build_karate_club_graph():\n",
    "#     # All 78 edges are stored in two numpy arrays. One for source endpoints\n",
    "#     # while the other for destination endpoints.\n",
    "#     src = np.array([1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 6, 7, 7, 7, 7, 8, 8, 9, 10, 10,\n",
    "#         10, 11, 12, 12, 13, 13, 13, 13, 16, 16, 17, 17, 19, 19, 21, 21,\n",
    "#         25, 25, 27, 27, 27, 28, 29, 29, 30, 30, 31, 31, 31, 31, 32, 32,\n",
    "#         32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33,\n",
    "#         33, 33, 33, 33, 33, 33, 33, 33, 33, 33])\n",
    "#     dst = np.array([0, 0, 1, 0, 1, 2, 0, 0, 0, 4, 5, 0, 1, 2, 3, 0, 2, 2, 0, 4,\n",
    "#         5, 0, 0, 3, 0, 1, 2, 3, 5, 6, 0, 1, 0, 1, 0, 1, 23, 24, 2, 23,\n",
    "#         24, 2, 23, 26, 1, 8, 0, 24, 25, 28, 2, 8, 14, 15, 18, 20, 22, 23,\n",
    "#         29, 30, 31, 8, 9, 13, 14, 15, 18, 19, 20, 22, 23, 26, 27, 28, 29, 30,\n",
    "#         31, 32])\n",
    "#     # Edges are directional in DGL; Make them bi-directional.\n",
    "#     u = np.concatenate([src, dst])\n",
    "#     v = np.concatenate([dst, src])\n",
    "#     # Construct a DGLGraph\n",
    "#     return dgl.DGLGraph((u, v))\n",
    "\n",
    "# class GCN(nn.Module):\n",
    "#     def __init__(self, in_feats, hidden_size, num_classes):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "#         self.conv2 = GraphConv(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, g, inputs):\n",
    "#         h = self.conv1(g, inputs)\n",
    "#         h = torch.relu(h)\n",
    "#         h = self.conv2(g, h)\n",
    "#         return h\n",
    "\n",
    "# # The first layer transforms input features of size of 5 to a hidden size of 5.\n",
    "# # The second layer transforms the hidden layer and produces output features of\n",
    "# # size 2, corresponding to the two groups of the karate club.\n",
    "# net = GCN(5, 5, 2)\n",
    "# # model = net(G,inputs)\n",
    "\n",
    "# G = build_karate_club_graph()\n",
    "# embed = nn.Embedding(34, 5)  # 34 nodes with embedding dim equal to 5\n",
    "# G.ndata['feat'] = embed.weight\n",
    "# inputs = embed.weight\n",
    "\n",
    "# graph = trace(net, (G,inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch-scatter==latest+cu102 -f https://pytorch-geometric.com/whl/torch-1.6.0.html\n",
    "# !pip install -q torch-sparse==latest+cu102 -f https://pytorch-geometric.com/whl/torch-1.6.0.html\n",
    "# !pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch-scatter==latest+cu92 -f https://pytorch-geometric.com/whl/torch-1.6.0.html\n",
    "# !pip install -q torch-sparse==latest+cu92 -f https://pytorch-geometric.com/whl/torch-1.6.0.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn import Linear\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "#         self.conv2 = GCNConv(4, 4)\n",
    "#         self.conv3 = GCNConv(4, 2)\n",
    "#         self.classifier = Linear(2, dataset.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "#         h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "#         # Apply a final (linear) classifier.\n",
    "#         out = self.classifier(h)\n",
    "\n",
    "#         return out, h\n",
    "\n",
    "# model = GCN()\n",
    "# from torch_geometric.datasets import KarateClub\n",
    "\n",
    "# dataset = KarateClub()\n",
    "# data = dataset[0] \n",
    "# _, h = model(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from generator import Generator\n",
    "from utils.visualizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "from utils.logger import create_logger\n",
    "\n",
    "\n",
    "class Scheduling:\n",
    "    def __init__(self, hwfile=\"default.yaml\"):\n",
    "        base_dir = \"configs/\"\n",
    "        self.total_cycles = 0\n",
    "        self.logger = create_logger(\"logs/stats.txt\")\n",
    "        self.config = self.create_config(\n",
    "            yaml.load(open(base_dir + hwfile), Loader=yamlordereddictloader.Loader)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(self, config):\n",
    "\n",
    "    self.logger.info(\"Config Statistics : \")\n",
    "\n",
    "    self.mle = config[\"memory_levels\"]\n",
    "    self.mem_energy = np.zeros((self.mle))\n",
    "    self.compute_energy = 0\n",
    "    self.mem_read_access = np.zeros((self.mle))\n",
    "    self.mem_write_access = np.zeros((self.mle))\n",
    "    self.mem_size = np.zeros((self.mle))\n",
    "    self.mem_util = np.zeros((self.mle))\n",
    "    self.mem_free = np.zeros((self.mle))\n",
    "    self.mem_read_bw = np.zeros((self.mle))\n",
    "    self.mem_write_bw = np.zeros((self.mle))\n",
    "    self.internal_bandwidth_time = 0\n",
    "    self.total_cycles = 0 \n",
    "    self.bandwidth_idle_time = 0\n",
    "    self.compute_idle_time = 0\n",
    "    self.mem_size_idle_time = 0\n",
    "    self.force_connectivity = False\n",
    "    mm_compute = config[\"mm_compute\"]\n",
    "    vector_compute = config[\"vector_compute\"]\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"systolic_array\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"]) ** 2) * mm_compute[\"N_PE\"] / 2\n",
    "        )\n",
    "        config[\"comp_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "        self.logger.info(\n",
    "            \"MM Compute per cycle : %d\", config[\"mm_compute_per_cycle\"]\n",
    "        )\n",
    "        self.logger.info(\"Compute Bandwidth Required : %d\", config[\"comp_bw\"])\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"mac\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"])) * mm_compute[\"N_PE\"] / 2\n",
    "        )\n",
    "        config[\"comp_read_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "    for i in range(self.mle):\n",
    "        memory = config[\"memory\"][\"level\" + str(i)]\n",
    "        self.mem_read_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"read_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_write_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"write_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_size[i] = memory[\"size\"]\n",
    "\n",
    "        self.logger.info(\n",
    "            \"Memory at Level %d, Read Bandwidth %d Write Bandwidth %d\",\n",
    "            i,\n",
    "            self.mem_read_bw[i],\n",
    "            self.mem_write_bw[i],\n",
    "        )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, graph):\n",
    "\n",
    "    \"\"\"\n",
    "     Check both size, utilization and bandwidths at every node\n",
    "     What about memory size that can also get exhausted ?\n",
    "     So if memory size is exhausted, then have to go to a previous level and write there ?\n",
    "     if any level utilization is exhausted then only the immediate memory required will be kept.\n",
    "     if the memory is empty in size, but is not bandwidth, it is useless?\n",
    "     Cannot do prefetching\n",
    "     Read access of the next node will decrease\n",
    "     Bandwidth is available but size is not?, can do prefetching, but now the memory fetches have to check, \n",
    "     whether to do fetches of the same node or a different node\n",
    "     Say bandwidth at level0 is sufficient, at level1 is insufficient, then at level1 we have a bottlenecks\n",
    "     slower so it will take its own time\n",
    "     Do vector operations in the meantime perhaps ? \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config = self.config\n",
    "\n",
    "    read_bw_req = []\n",
    "    write_bw_req = []\n",
    "    read_bw_actual = []\n",
    "    write_bw_actual = []\n",
    "    cycles = []\n",
    "    free_cycles = []\n",
    "    transferable_checkpointed_edge = []\n",
    "    all_checkpointed_edge = []\n",
    "    self.mem_util_log=[]\n",
    "    self.mem_util_full=[]\n",
    "    # Mem Fetch time of the last Nodes\n",
    "#     print(self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "\n",
    "    mem_free = True\n",
    "    for n, node in enumerate(graph.nodes):\n",
    "\n",
    "        # These are last level read/write accesses\n",
    "        compute_expense, read_access, write_access = node.get_stats()\n",
    "#         print(read_access, node.in_edge_mem)\n",
    "        \n",
    "        self.mem_read_access[1]+=(read_access)\n",
    "      \n",
    "        self.logger.info(node.get_stats())\n",
    "        self.mem_util[0] += node.in_edge_mem\n",
    "        \n",
    "        # Total Free memory\n",
    "        for i in range(self.mle - 1):\n",
    "            self.mem_free[i] = self.mem_size[i] - self.mem_util[i]\n",
    "            \n",
    "#         print(\"2\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        time_compute = compute_expense / config[\"mm_compute_per_cycle\"]\n",
    "        read_bw_ll = read_access / (time_compute)\n",
    "        write_bw_ll = write_access / (time_compute)\n",
    "        step_cycles = time_compute\n",
    "        read_bw_req.append(read_bw_ll)\n",
    "        write_bw_req.append(write_bw_ll)\n",
    "        free_cycles.append(step_cycles)\n",
    "#         print(\"bandwidth\",read_bw_ll, write_bw_ll, step_cycles) \n",
    "        \n",
    "        if self.mem_free[0] < node.mem_util:\n",
    "            mem_free = False\n",
    "            # node mem_util = output edge\n",
    "            self.logger.info(\"Memory size is too low/ Memory is Full\")\n",
    "            self.logger.info(\"Node or Node memory Requirements too high\")\n",
    "            # Rearrange the checkpointed_nodes\n",
    "            # rearrange = True\n",
    "\n",
    "            # Is it possible now : Otherwise update the last level memory bandwidth requirements\n",
    "            if(self.mem_free[0]<0):      \n",
    "                step_cycles += 2*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem) + 1)*(\n",
    "                   (self.mem_free[0]+node.in_edge_mem)  / self.mem_read_bw[self.mle - 1]\n",
    "                ) + time_compute*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem) + 1)/256\n",
    "                # Change this later with the number of solid total cycles\n",
    "                self.mem_size_idle_time += 2*(node.in_edge_mem//self.mem_read_bw[self.mle - 1]) + time_compute/256*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem) + 1)\n",
    "                self.mem_read_access[0]+=node.in_edge_mem*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem)) \n",
    "                self.mem_write_access[0]+=node.in_edge_mem*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem)) \n",
    "        else:\n",
    "            self.mem_util[0] += node.mem_util\n",
    "            self.mem_free[0] -= node.mem_util\n",
    "#         print(\"2.5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_log.append(self.mem_util[0])\n",
    "        self.mem_read_access[0]+=node.mem_util\n",
    "        self.mem_write_access[0]+=node.mem_util\n",
    "            \n",
    "        assert(self.mem_free[0] < self.mem_size[0])\n",
    "        # Last level memory fetch takes more time, so that may be a bottleneck\n",
    "        bandwidth_available = read_bw_ll < self.mem_read_bw[self.mle - 1]\n",
    "        \n",
    "        # If Bandwidth is not available : Cannot Prefetch\n",
    "        if (bandwidth_available) == False:\n",
    "            step_cycles += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "            self.bandwidth_idle_time += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "\n",
    "        # If memory is not free for the next node and Bandwidth is available : Move nodes back and forth\n",
    "        # if(total_mem_free[0] == 0 and (bandwidth_available)):\n",
    "        # for(nodes in checkpointed_nodes):\n",
    "        # checkpointed but not immediate node\n",
    "\n",
    "        # Check if memory is free and Bandwidth available : From the Data Dependence Graph, Prefetch new node\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        if self.mem_free[0] > 0 and (bandwidth_available):\n",
    "            if n < len(graph.nodes) - 1:\n",
    "                if self.mem_free[0] > node.next.mem_util:\n",
    "                    read_access += node.next.mem_util\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        self.mem_util[0] += node.next.mem_util\n",
    "                        self.mem_write_access[0] += node.next.mem_util\n",
    "                        self.mem_free[0] -= node.next.mem_util\n",
    "                        node.next.mem_util = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_write_access[0]+=read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_util = read_access - read_bw_ll * step_cycles\n",
    "\n",
    "                else:\n",
    "                    read_access += self.mem_free[0]\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        node.next.mem_util = node.next.mem_util - self.mem_free[0]\n",
    "                        self.mem_util[0] = self.mem_size[0]\n",
    "                        self.mem_free[0] = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_util = read_access - read_bw_ll * step_cycles\n",
    "#         print(\"3\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_full.append(self.mem_util[0])   \n",
    "        \n",
    "            # TODO Consider Write bandwidth for a block read memory or Write Bandwidth  for a endurance purposes\n",
    "        self.mem_util[0] -= node.in_edge_mem\n",
    "#         print(\"4\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "    \n",
    "        if mem_free:\n",
    "            self.mem_util[0] -= node.mem_util\n",
    "#         print(\"5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Node operator %r, Step Cycles %d, Read Accesses %d, Write Accesses %d \",\n",
    "            node.operator,\n",
    "            step_cycles,\n",
    "            read_access,\n",
    "            write_access,\n",
    "        )\n",
    "        self.total_cycles += step_cycles\n",
    "        cycles.append(step_cycles)\n",
    "        read_bw_actual.append(read_access / step_cycles)\n",
    "        write_bw_actual.append(write_access / step_cycles)\n",
    "#         print(\"actual\",read_access / step_cycles, write_access / step_cycles, step_cycles)\n",
    "#     print(\"The total cycles are \", self.total_cycles)\n",
    "    return read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduling.create_config = create_config\n",
    "Scheduling.run = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(graph_set):\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Scheduling works in the following way :\n",
    "    1. Start with a given/random Hardware point -> Nodes of the graph are scheduled (prefetching)\n",
    "    \n",
    "    2. Do the Scheduling with that Point -> Mapping stops here -> Further evaluation is done using accelergy \n",
    "    (with values taken from ERT/ART) -> If values not available -> Use plugins for generating these values \n",
    "    \n",
    "    3. Log bottlenecks and work on a different Hardware point -> do this till some realistically\n",
    "    max, min values are not violated -> Values/Analyses for a different/unavailable point will require full \n",
    "    From the technology node set, generate the reference tables using plugins integration of plugins -> \n",
    "    Currently using a table at 40nm.   \n",
    "    \n",
    "    4. Optimization metric (time/area/energy) of execution in various components, \n",
    "    then optimize the metric of execution and take decisions accordingly \n",
    "    \n",
    "    5. Optimize over different workloads ?\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    num_iterations = 10\n",
    "    generator = Generator()\n",
    "    bandwidth = [2,10,50,75,100]\n",
    "    for graph in graph_set:\n",
    "#     for i in range(len(bandwidth)):\n",
    "        scheduler = Scheduling()\n",
    "#         scheduler.mem_read_bw[1] = 10*bandwidth[i]\n",
    "        read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "        read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "#         bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n",
    "#         cycles_bar_graph(\"cycles.png\", cycles, free_cycles, graph.nodes)\n",
    "#         mem_util_bar_graph(\"mem_util.png\",scheduler.mem_util_full/scheduler.mem_size[0],scheduler.mem_util_log/scheduler.mem_size[0], graph.nodes)\n",
    "#         generator.save_statistics(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        generator.save_statistics(scheduler)\n",
    "        \n",
    "#         for i in range(num_iterations):\n",
    "#             nexthw = generator.findnext(scheduler)\n",
    "#             generator.writehwfile(nexthw, str(i) + \"hw.yaml\")\n",
    "#             scheduler.create_config(nexthw)\n",
    "#             read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "#             read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "# #             bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes, cycles)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to plot memory bandwidth utilization over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 2.18 2.16876953125 0 0 5 1000000\n",
      "Energy 5.621051381000001 0.03665625 0.052297421000000004 0.04181651 0.5435481200000001 0.0218 0.0 2.18\n"
     ]
    }
   ],
   "source": [
    "runner([graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig = make_dot(model(tokens_tensor, segments_tensors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technology Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "d = pd.read_csv('tables/sram.csv')\n",
    "a = np.array(d)\n",
    "\n",
    "input = a[:,:3]\n",
    "# output = a[:,3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('plugins/cacti/bus_width.out')\n",
    "\n",
    "\n",
    "d = d.drop(d.columns[[0,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]], axis=1)\n",
    "\n",
    "d = d.drop(' Associativity',1)\n",
    "d = d.drop(' Dynamic search energy (nJ)',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array(d)\n",
    "np.savetxt('bus.csv',a, fmt='%.18e', delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph(\"read_dummy.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_table = np.array(pd.read_csv(\"tables/sram.csv\", header=None))\n",
    "a = mem_table[np.where(mem_table[:, 1] == 4)]\n",
    "a = a[np.where(a[:, 2] == 32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute:\n",
    "    name       : MACs\n",
    "        class      : mac\n",
    "        attributes :            \n",
    "        instances       : 256\n",
    "        meshX           : 16\n",
    "        word-bits       : 16\n",
    "\n",
    "    memory: \n",
    "        name       : DRAM\n",
    "        class      : DRAM\n",
    "        attributes :\n",
    "        instances       : 1\n",
    "        word-bits       : 16\n",
    "  \n",
    "    name       : OutputBuffer\n",
    "        class      : SRAM\n",
    "        attributes :\n",
    "        entries         : 1024  # 64 * 16 = 1024\n",
    "        instances       : 1\n",
    "        meshX           : 1\n",
    "        word-bits       : 16\n",
    "        block-size      : 16\n",
    "        read_bandwidth  : 16 # words/cycle\n",
    "        write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : InputBuffer\n",
    "            class      : SRAM\n",
    "            attributes :\n",
    "            entries         : 1024 # 64 * 16 = 1024\n",
    "            instances       : 1\n",
    "            meshX           : 1\n",
    "            word-bits       : 16\n",
    "            block-size      : 16\n",
    "            read_bandwidth  : 16 # words/cycle\n",
    "            write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : PsumRegFile\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 1\n",
    "            instances       : 16\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 16\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "            \n",
    "        name       : WeightBuffer\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 64\n",
    "            instances       : 256\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 256\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "    noc:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit ('dl': conda)",
   "language": "python",
   "name": "python36464bitdlconda74ce50dd77ab450dad146f0ee6a8041f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
