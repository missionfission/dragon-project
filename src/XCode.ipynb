{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "from torchvision import models\n",
    "from yaml import dump\n",
    "from dlrm.dlrm_s_pytorch import DLRM_Net, dash_separated_ints, dash_separated_floats\n",
    "from ir.handlers import handlers\n",
    "from ir.trace import trace\n",
    "from ir.trace import get_backprop_memory\n",
    "from utils.logger import create_logger\n",
    "from utils.visualizer import plot_descent\n",
    "from utils.visualizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import dlrm.dlrm_data_pytorch as dp\n",
    "# ### parse arguments ###\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Train Deep Learning Recommendation Model (DLRM)\"\n",
    ")\n",
    "# model related parameters\n",
    "parser.add_argument(\"--arch-sparse-feature-size\", type=int, default=2)\n",
    "parser.add_argument(\n",
    "    \"--arch-embedding-size\", type=dash_separated_ints, default=\"4-3-2\"\n",
    ")\n",
    "# j will be replaced with the table number\n",
    "parser.add_argument(\"--arch-mlp-bot\", type=dash_separated_ints, default=\"4-3-2\")\n",
    "parser.add_argument(\"--arch-mlp-top\", type=dash_separated_ints, default=\"4-2-1\")\n",
    "parser.add_argument(\n",
    "    \"--arch-interaction-op\", type=str, choices=[\"dot\", \"cat\"], default=\"dot\"\n",
    ")\n",
    "parser.add_argument(\"--arch-interaction-itself\", action=\"store_true\", default=False)\n",
    "# embedding table options\n",
    "parser.add_argument(\"--md-flag\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--md-threshold\", type=int, default=200)\n",
    "parser.add_argument(\"--md-temperature\", type=float, default=0.3)\n",
    "parser.add_argument(\"--md-round-dims\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--qr-flag\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--qr-threshold\", type=int, default=200)\n",
    "parser.add_argument(\"--qr-operation\", type=str, default=\"mult\")\n",
    "parser.add_argument(\"--qr-collisions\", type=int, default=4)\n",
    "# activations and loss\n",
    "parser.add_argument(\"--activation-function\", type=str, default=\"relu\")\n",
    "parser.add_argument(\"--loss-function\", type=str, default=\"mse\")  # or bce or wbce\n",
    "parser.add_argument(\n",
    "    \"--loss-weights\", type=dash_separated_floats, default=\"1.0-1.0\"\n",
    ")  # for wbce\n",
    "parser.add_argument(\"--loss-threshold\", type=float, default=0.0)  # 1.0e-7\n",
    "parser.add_argument(\"--round-targets\", type=bool, default=False)\n",
    "# data\n",
    "parser.add_argument(\"--data-size\", type=int, default=1)\n",
    "parser.add_argument(\"--num-batches\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"--data-generation\", type=str, default=\"random\"\n",
    ")  # synthetic or dataset\n",
    "parser.add_argument(\"--data-trace-file\", type=str, default=\"./input/dist_emb_j.log\")\n",
    "parser.add_argument(\"--data-set\", type=str, default=\"kaggle\")  # or terabyte\n",
    "parser.add_argument(\"--raw-data-file\", type=str, default=\"\")\n",
    "parser.add_argument(\"--processed-data-file\", type=str, default=\"\")\n",
    "parser.add_argument(\"--data-randomize\", type=str, default=\"total\")  # or day or none\n",
    "parser.add_argument(\"--data-trace-enable-padding\", type=bool, default=False)\n",
    "parser.add_argument(\"--max-ind-range\", type=int, default=-1)\n",
    "parser.add_argument(\"--data-sub-sample-rate\", type=float, default=0.0)  # in [0, 1]\n",
    "parser.add_argument(\"--num-indices-per-lookup\", type=int, default=10)\n",
    "parser.add_argument(\"--num-indices-per-lookup-fixed\", type=bool, default=False)\n",
    "parser.add_argument(\"--num-workers\", type=int, default=0)\n",
    "parser.add_argument(\"--memory-map\", action=\"store_true\", default=False)\n",
    "# training\n",
    "parser.add_argument(\"--mini-batch-size\", type=int, default=1)\n",
    "parser.add_argument(\"--nepochs\", type=int, default=1)\n",
    "parser.add_argument(\"--learning-rate\", type=float, default=0.01)\n",
    "parser.add_argument(\"--print-precision\", type=int, default=5)\n",
    "parser.add_argument(\"--numpy-rand-seed\", type=int, default=123)\n",
    "parser.add_argument(\"--sync-dense-params\", type=bool, default=True)\n",
    "# inference\n",
    "parser.add_argument(\"--inference-only\", action=\"store_true\", default=False)\n",
    "# onnx\n",
    "parser.add_argument(\"--save-onnx\", action=\"store_true\", default=False)\n",
    "# gpu\n",
    "parser.add_argument(\"--use-gpu\", action=\"store_true\", default=False)\n",
    "# debugging and profiling\n",
    "parser.add_argument(\"--print-freq\", type=int, default=1)\n",
    "parser.add_argument(\"--test-freq\", type=int, default=-1)\n",
    "parser.add_argument(\"--test-mini-batch-size\", type=int, default=-1)\n",
    "parser.add_argument(\"--test-num-workers\", type=int, default=-1)\n",
    "parser.add_argument(\"--print-time\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--debug-mode\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--enable-profiling\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--plot-compute-graph\", action=\"store_true\", default=False)\n",
    "# store/load model\n",
    "parser.add_argument(\"--save-model\", type=str, default=\"\")\n",
    "parser.add_argument(\"--load-model\", type=str, default=\"\")\n",
    "# mlperf logging (disables other output and stops early)\n",
    "parser.add_argument(\"--mlperf-logging\", action=\"store_true\", default=False)\n",
    "# stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107\n",
    "parser.add_argument(\"--mlperf-acc-threshold\", type=float, default=0.0)\n",
    "# stop at target AUC Terabyte (no subsampling) 0.8025\n",
    "parser.add_argument(\"--mlperf-auc-threshold\", type=float, default=0.0)\n",
    "parser.add_argument(\"--mlperf-bin-loader\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--mlperf-bin-shuffle\", action=\"store_true\", default=False)\n",
    "# LR policy\n",
    "parser.add_argument(\"--lr-num-warmup-steps\", type=int, default=0)\n",
    "parser.add_argument(\"--lr-decay-start-step\", type=int, default=0)\n",
    "parser.add_argument(\"--lr-num-decay-steps\", type=int, default=0)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# if args.mlperf_logging:\n",
    "#     print(\"command line args: \", json.dumps(vars(args)))\n",
    "\n",
    "### some basic setup ###\n",
    "np.random.seed(args.numpy_rand_seed)\n",
    "np.set_printoptions(precision=args.print_precision)\n",
    "torch.set_printoptions(precision=args.print_precision)\n",
    "torch.manual_seed(args.numpy_rand_seed)\n",
    "\n",
    "if args.test_mini_batch_size < 0:\n",
    "    # if the parameter is not set, use the training batch size\n",
    "    args.test_mini_batch_size = args.mini_batch_size\n",
    "if args.test_num_workers < 0:\n",
    "    # if the parameter is not set, use the same parameter for training\n",
    "    args.test_num_workers = args.num_workers\n",
    "\n",
    "use_gpu = args.use_gpu and torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed_all(args.numpy_rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    device = torch.device(\"cuda\", 0)\n",
    "    ngpus = torch.cuda.device_count()  # 1\n",
    "    print(\"Using {} GPU(s)...\".format(ngpus))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU...\")\n",
    "\n",
    "# ### prepare training data ###\n",
    "ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=\"-\")\n",
    "# input data\n",
    "if args.data_generation == \"dataset\":\n",
    "\n",
    "    train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)\n",
    "    nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n",
    "    nbatches_test = len(test_ld)\n",
    "\n",
    "    ln_emb = train_data.counts\n",
    "    # enforce maximum limit on number of vectors per embedding\n",
    "    if args.max_ind_range > 0:\n",
    "        ln_emb = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: x if x < args.max_ind_range else args.max_ind_range,\n",
    "                    ln_emb,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    m_den = train_data.m_den\n",
    "    ln_bot[0] = m_den\n",
    "else:\n",
    "    # input and target at random\n",
    "    ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=\"-\")\n",
    "    m_den = ln_bot[0]\n",
    "    train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)\n",
    "    nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n",
    "\n",
    "### parse command line arguments ###\n",
    "m_spa = args.arch_sparse_feature_size\n",
    "num_fea = ln_emb.size + 1  # num sparse + num dense features\n",
    "m_den_out = ln_bot[ln_bot.size - 1]\n",
    "if args.arch_interaction_op == \"dot\":\n",
    "    # approach 1: all\n",
    "    # num_int = num_fea * num_fea + m_den_out\n",
    "    # approach 2: unique\n",
    "    if args.arch_interaction_itself:\n",
    "        num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out\n",
    "    else:\n",
    "        num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out\n",
    "elif args.arch_interaction_op == \"cat\":\n",
    "    num_int = num_fea * m_den_out\n",
    "else:\n",
    "    sys.exit(\n",
    "        \"ERROR: --arch-interaction-op=\"\n",
    "        + args.arch_interaction_op\n",
    "        + \" is not supported\"\n",
    "    )\n",
    "arch_mlp_top_adjusted = str(num_int) + \"-\" + args.arch_mlp_top\n",
    "ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=\"-\")\n",
    "\n",
    "# sanity check: feature sizes and mlp dimensions must match\n",
    "if m_den != ln_bot[0]:\n",
    "    sys.exit(\n",
    "        \"ERROR: arch-dense-feature-size \"\n",
    "        + str(m_den)\n",
    "        + \" does not match first dim of bottom mlp \"\n",
    "        + str(ln_bot[0])\n",
    "    )\n",
    "if args.qr_flag:\n",
    "    if args.qr_operation == \"concat\" and 2 * m_spa != m_den_out:\n",
    "        sys.exit(\n",
    "            \"ERROR: 2 arch-sparse-feature-size \"\n",
    "            + str(2 * m_spa)\n",
    "            + \" does not match last dim of bottom mlp \"\n",
    "            + str(m_den_out)\n",
    "            + \" (note that the last dim of bottom mlp must be 2x the embedding dim)\"\n",
    "        )\n",
    "    if args.qr_operation != \"concat\" and m_spa != m_den_out:\n",
    "        sys.exit(\n",
    "            \"ERROR: arch-sparse-feature-size \"\n",
    "            + str(m_spa)\n",
    "            + \" does not match last dim of bottom mlp \"\n",
    "            + str(m_den_out)\n",
    "        )\n",
    "else:\n",
    "    if m_spa != m_den_out:\n",
    "        sys.exit(\n",
    "            \"ERROR: arch-sparse-feature-size \"\n",
    "            + str(m_spa)\n",
    "            + \" does not match last dim of bottom mlp \"\n",
    "            + str(m_den_out)\n",
    "        )\n",
    "if num_int != ln_top[0]:\n",
    "    sys.exit(\n",
    "        \"ERROR: # of feature interactions \"\n",
    "        + str(num_int)\n",
    "        + \" does not match first dimension of top mlp \"\n",
    "        + str(ln_top[0])\n",
    "    )\n",
    "\n",
    "ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1\n",
    "\n",
    "# ### construct the neural network specified above ###\n",
    "# # WARNING: to obtain exactly the same initialization for\n",
    "# # the weights we need to start from the same random seed.\n",
    "# # np.random.seed(args.numpy_rand_seed)\n",
    "dlrm = DLRM_Net(\n",
    "    m_spa,\n",
    "    ln_emb,\n",
    "    ln_bot,\n",
    "    ln_top,\n",
    "    arch_interaction_op=args.arch_interaction_op,\n",
    "    arch_interaction_itself=args.arch_interaction_itself,\n",
    "    sigmoid_bot=-1,\n",
    "    sigmoid_top=ln_top.size - 2,\n",
    "    sync_dense_params=args.sync_dense_params,\n",
    "    loss_threshold=args.loss_threshold,\n",
    "    ndevices=ndevices,\n",
    "    qr_flag=args.qr_flag,\n",
    "    qr_operation=args.qr_operation,\n",
    "    qr_collisions=args.qr_collisions,\n",
    "    qr_threshold=args.qr_threshold,\n",
    "    md_flag=args.md_flag,\n",
    "    md_threshold=args.md_threshold,\n",
    ")\n",
    "for j, (X, lS_o, lS_i, T) in enumerate(train_ld):\n",
    "    Z = dlrm(X, lS_o, lS_i)\n",
    "dlrm_graph = trace(dlrm, (X, lS_o, lS_i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet and VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision.models.vgg.VGG (\n",
      "\t%input.1: float[1, 3, 299, 299],\n",
      "\t%1: float[64, 3, 3, 3],\n",
      "\t%2: float[64],\n",
      "\t%3: float[128, 64, 3, 3],\n",
      "\t%4: float[128],\n",
      "\t%5: float[256, 128, 3, 3],\n",
      "\t%6: float[256],\n",
      "\t%7: float[256, 256, 3, 3],\n",
      "\t%8: float[256],\n",
      "\t%9: float[512, 256, 3, 3],\n",
      "\t%10: float[512],\n",
      "\t%11: float[512, 512, 3, 3],\n",
      "\t%12: float[512],\n",
      "\t%13: float[512, 512, 3, 3],\n",
      "\t%14: float[512],\n",
      "\t%15: float[512, 512, 3, 3],\n",
      "\t%16: float[512],\n",
      "\t%17: float[4096, 25088],\n",
      "\t%18: float[4096],\n",
      "\t%19: float[4096, 4096],\n",
      "\t%20: float[4096],\n",
      "\t%21: float[1000, 4096],\n",
      "\t%22: float[1000]\n",
      "):\n",
      "\taten::_convolution\n",
      "\taten::_convolution\n",
      "\taten::_convolution\n",
      "\taten::_convolution\n",
      "\taten::_convolution\n",
      "\taten::_convolution\n",
      "\taten::_convolution\n",
      "\taten::_convolution\n",
      "\taten::adaptive_avg_pool2d\n",
      "\taten::addmm\n",
      "\taten::addmm\n",
      "\taten::addmm\n",
      "\treturn %329: float[1, 1000]\n"
     ]
    }
   ],
   "source": [
    "# Just preload the graph for fast experiments \n",
    "# Create Graph\n",
    "\n",
    "for name, model in models.__dict__.items():\n",
    "#     print(name)\n",
    "    if not name.islower() or name.startswith(\"__\") or not callable(model):\n",
    "        continue\n",
    "    if \"vgg11\" in name and \"vgg11_bn\" not in name :\n",
    "        inputs = torch.randn(1, 3, 299, 299)\n",
    "        vgg11_graph = trace(model().eval(), inputs) \n",
    "        print(vgg11_graph)\n",
    "        break\n",
    "       \n",
    "    if \"resnet50\" in name:\n",
    "        model = model().eval()\n",
    "        inputs = torch.randn(1, 3, 100, 100)\n",
    "        resnet_graph = trace(model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert and GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0923 21:40:52.065365 140102590277376 file_utils.py:39] PyTorch version 1.5.0 available.\n",
      "/home/khushal/anaconda2/envs/dl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "\n",
    "configuration = GPT2Config()\n",
    "model = GPT2Model(configuration)\n",
    "# model.configMM\n",
    "\n",
    "# tokenizer = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'tokenizer', 'bert-base-cased', do_basic_tokenize=False)\n",
    "\n",
    "# Tokenized input\n",
    "# text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "# tokenized_text = tokenizer.tokenize(text)\n",
    "# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# print(indexed_tokens)\n",
    "### Get the hidden states computed by `bertModel`\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "indexed_tokens = [101, 2627, 1108, 3104, 1124, 15703, 136, 102, 3104, 1124, 15703, 1108, 170, 16797, 8284, 102]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'model', 'bert-base-cased')\n",
    "model.eval()\n",
    "\n",
    "model(tokens_tensor)\n",
    "gpt2_graph = trace(model, tokens_tensor)\n",
    "\n",
    "configuration = BertConfig()\n",
    "model = BertModel(configuration)\n",
    "model.eval()\n",
    "\n",
    "model(tokens_tensor)\n",
    "bert_graph = trace(model, tokens_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn import Linear\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "#         self.conv2 = GCNConv(4, 4)\n",
    "#         self.conv3 = GCNConv(4, 2)\n",
    "#         self.classifier = Linear(2, dataset.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "#         h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "#         # Apply a final (linear) classifier.\n",
    "#         out = self.classifier(h)\n",
    "\n",
    "#         return out, h\n",
    "\n",
    "# model = GCN()\n",
    "# from torch_geometric.datasets import KarateClub\n",
    "\n",
    "# dataset = KarateClub()\n",
    "# data = dataset[0] \n",
    "# _, h = model(data.x, data.edge_index)\n",
    "# gnn_graph = trace(model, (data.x, data.edge_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')\n",
    "\n",
    "PRIMITIVES = [\n",
    "    'none',\n",
    "    'max_pool_3x3',\n",
    "    'avg_pool_3x3',\n",
    "    'skip_connect',\n",
    "    'sep_conv_3x3',\n",
    "    'sep_conv_5x5',\n",
    "    'dil_conv_3x3',\n",
    "    'dil_conv_5x5'\n",
    "]\n",
    "\n",
    "NASNet = Genotype(\n",
    "  normal = [\n",
    "    ('sep_conv_5x5', 1),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('sep_conv_5x5', 0),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ('skip_connect', 0),\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('sep_conv_3x3', 1),\n",
    "    ('skip_connect', 1),\n",
    "  ],\n",
    "  normal_concat = [2, 3, 4, 5, 6],\n",
    "  reduce = [\n",
    "    ('sep_conv_5x5', 1),\n",
    "    ('sep_conv_7x7', 0),\n",
    "    ('max_pool_3x3', 1),\n",
    "    ('sep_conv_7x7', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ('sep_conv_5x5', 0),\n",
    "    ('skip_connect', 3),\n",
    "    ('avg_pool_3x3', 2),\n",
    "    ('sep_conv_3x3', 2),\n",
    "    ('max_pool_3x3', 1),\n",
    "  ],\n",
    "  reduce_concat = [4, 5, 6],\n",
    ")\n",
    "    \n",
    "AmoebaNet = Genotype(\n",
    "  normal = [\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('max_pool_3x3', 1),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('sep_conv_5x5', 2),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('avg_pool_3x3', 3),\n",
    "    ('sep_conv_3x3', 1),\n",
    "    ('skip_connect', 1),\n",
    "    ('skip_connect', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ],\n",
    "  normal_concat = [4, 5, 6],\n",
    "  reduce = [\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('sep_conv_3x3', 1),\n",
    "    ('max_pool_3x3', 0),\n",
    "    ('sep_conv_7x7', 2),\n",
    "    ('sep_conv_7x7', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ('max_pool_3x3', 0),\n",
    "    ('max_pool_3x3', 1),\n",
    "    ('conv_7x1_1x7', 0),\n",
    "    ('sep_conv_3x3', 5),\n",
    "  ],\n",
    "  reduce_concat = [3, 4, 6]\n",
    ")\n",
    "\n",
    "DARTS_V1 = Genotype(normal=[('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('skip_connect', 0), ('sep_conv_3x3', 1), ('skip_connect', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('skip_connect', 2)], normal_concat=[2, 3, 4, 5], reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('skip_connect', 2), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('skip_connect', 2), ('skip_connect', 2), ('avg_pool_3x3', 0)], reduce_concat=[2, 3, 4, 5])\n",
    "DARTS_V2 = Genotype(normal=[('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 1), ('skip_connect', 0), ('skip_connect', 0), ('dil_conv_3x3', 2)], normal_concat=[2, 3, 4, 5], reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('skip_connect', 2), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 2), ('skip_connect', 2), ('max_pool_3x3', 1)], reduce_concat=[2, 3, 4, 5])\n",
    "\n",
    "DARTS = DARTS_V2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkImageNet(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes, layers, auxiliary, genotype):\n",
    "    super(NetworkImageNet, self).__init__()\n",
    "    self._layers = layers\n",
    "    self._auxiliary = auxiliary\n",
    "\n",
    "    self.stem0 = nn.Sequential(\n",
    "      nn.Conv2d(3, C // 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(C // 2),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(C // 2, C, 3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(C),\n",
    "    )\n",
    "\n",
    "    self.stem1 = nn.Sequential(\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(C, C, 3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(C),\n",
    "    )\n",
    "\n",
    "    C_prev_prev, C_prev, C_curr = C, C, C\n",
    "\n",
    "    self.cells = nn.ModuleList()\n",
    "    reduction_prev = True\n",
    "    for i in range(layers):\n",
    "      if i in [layers // 3, 2 * layers // 3]:\n",
    "        C_curr *= 2\n",
    "        reduction = True\n",
    "      else:\n",
    "        reduction = False\n",
    "      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
    "      reduction_prev = reduction\n",
    "      self.cells += [cell]\n",
    "      C_prev_prev, C_prev = C_prev, cell.multiplier * C_curr\n",
    "      if i == 2 * layers // 3:\n",
    "        C_to_auxiliary = C_prev\n",
    "\n",
    "    if auxiliary:\n",
    "      self.auxiliary_head = AuxiliaryHeadImageNet(C_to_auxiliary, num_classes)\n",
    "    self.global_pooling = nn.AvgPool2d(7)\n",
    "    self.classifier = nn.Linear(C_prev, num_classes)\n",
    "\n",
    "  def forward(self, input):\n",
    "    logits_aux = None\n",
    "    s0 = self.stem0(input)\n",
    "    s1 = self.stem1(s0)\n",
    "    for i, cell in enumerate(self.cells):\n",
    "      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\n",
    "      if i == 2 * self._layers // 3:\n",
    "        if self._auxiliary and self.training:\n",
    "          logits_aux = self.auxiliary_head(s1)\n",
    "    out = self.global_pooling(s1)\n",
    "    logits = self.classifier(out.view(out.size(0), -1))\n",
    "    return logits, logits_aux\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryHeadImageNet(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes):\n",
    "    \"\"\"assuming input size 14x14\"\"\"\n",
    "    super(AuxiliaryHeadImageNet, self).__init__()\n",
    "    self.features = nn.Sequential(\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.AvgPool2d(5, stride=2, padding=0, count_include_pad=False),\n",
    "      nn.Conv2d(C, 128, 1, bias=False),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(128, 768, 2, bias=False),\n",
    "      # NOTE: This batchnorm was omitted in my earlier implementation due to a typo.\n",
    "      # Commenting it out for consistency with the experiments in the paper.\n",
    "      # nn.BatchNorm2d(768),\n",
    "      nn.ReLU(inplace=True)\n",
    "    )\n",
    "    self.classifier = nn.Linear(768, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    x = self.classifier(x.view(x.size(0),-1))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(nn.Module):\n",
    "\n",
    "  def __init__(self, genotype, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
    "    super(Cell, self).__init__()\n",
    "    print(C_prev_prev, C_prev, C)\n",
    "\n",
    "    if reduction_prev:\n",
    "      self.preprocess0 = FactorizedReduce(C_prev_prev, C)\n",
    "    else:\n",
    "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0)\n",
    "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0)\n",
    "    \n",
    "    if reduction:\n",
    "      op_names, indices = zip(*genotype.reduce)\n",
    "      concat = genotype.reduce_concat\n",
    "    else:\n",
    "      op_names, indices = zip(*genotype.normal)\n",
    "      concat = genotype.normal_concat\n",
    "    self._compile(C, op_names, indices, concat, reduction)\n",
    "\n",
    "  def _compile(self, C, op_names, indices, concat, reduction):\n",
    "    assert len(op_names) == len(indices)\n",
    "    self._steps = len(op_names) // 2\n",
    "    self._concat = concat\n",
    "    self.multiplier = len(concat)\n",
    "\n",
    "    self._ops = nn.ModuleList()\n",
    "    for name, index in zip(op_names, indices):\n",
    "      stride = 2 if reduction and index < 2 else 1\n",
    "      op = OPS[name](C, stride, True)\n",
    "      self._ops += [op]\n",
    "    self._indices = indices\n",
    "\n",
    "  def forward(self, s0, s1, drop_prob):\n",
    "    s0 = self.preprocess0(s0)\n",
    "    s1 = self.preprocess1(s1)\n",
    "\n",
    "    states = [s0, s1]\n",
    "    for i in range(self._steps):\n",
    "      h1 = states[self._indices[2*i]]\n",
    "      h2 = states[self._indices[2*i+1]]\n",
    "      op1 = self._ops[2*i]\n",
    "      op2 = self._ops[2*i+1]\n",
    "      h1 = op1(h1)\n",
    "      h2 = op2(h2)\n",
    "      if self.training and drop_prob > 0.:\n",
    "        if not isinstance(op1, Identity):\n",
    "          h1 = drop_path(h1, drop_prob)\n",
    "        if not isinstance(op2, Identity):\n",
    "          h2 = drop_path(h2, drop_prob)\n",
    "      s = h1 + h2\n",
    "      states += [s]\n",
    "    return torch.cat([states[i] for i in self._concat], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data', type=str, default='../data/imagenet/', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1, help='init learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-5, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=100, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=250, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=48, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=14, help='total number of layers')\n",
    "parser.add_argument('--auxiliary', action='store_true', default=False, help='use auxiliary tower')\n",
    "parser.add_argument('--auxiliary_weight', type=float, default=0.4, help='weight for auxiliary loss')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "parser.add_argument('--arch', type=str, default='DARTS', help='which architecture to use')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='gradient clipping')\n",
    "parser.add_argument('--label_smooth', type=float, default=0.1, help='label smoothing')\n",
    "parser.add_argument('--gamma', type=float, default=0.97, help='learning rate decay')\n",
    "parser.add_argument('--decay_period', type=int, default=1, help='epochs between two learning rate decays')\n",
    "parser.add_argument('--parallel', action='store_true', default=False, help='data parallelism')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from plugins.darts.cnn.operations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48 48\n",
      "48 192 48\n",
      "192 192 48\n",
      "192 192 48\n",
      "192 192 96\n",
      "192 384 96\n",
      "384 384 96\n",
      "384 384 96\n",
      "384 384 96\n",
      "384 384 192\n",
      "384 768 192\n",
      "768 768 192\n",
      "768 768 192\n",
      "768 768 192\n"
     ]
    }
   ],
   "source": [
    "CLASSES = 1000\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = NetworkImageNet(args.init_channels, CLASSES, args.layers, args.auxiliary, DARTS)\n",
    "\n",
    "inputs = torch.randn(1, 3, 299, 299)\n",
    "inputs = Variable(inputs)\n",
    "\n",
    "# dart_graph = trace(model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batchSize=64, beta1=0.5, classes='bedroom', cuda=False, imageSize=64, lr=0.0002, manualSeed=None, ndf=64, netD='', netG='', ngf=64, ngpu=1, niter=25, nz=100, outf='.', workers=2)\n",
      "Random Seed:  8316\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')\n",
    "# parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')\n",
    "\n",
    "opt = parser.parse_args([])\n",
    "print(opt)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "nc = 3\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG))\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "# ############################\n",
    "# # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "# ###########################\n",
    "# # train with real\n",
    "# netD.zero_grad()\n",
    "# real_cpu = data[0].to(device)\n",
    "# batch_size = real_cpu.size(0)\n",
    "# label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "\n",
    "# errD_real = criterion(output, label)\n",
    "\n",
    "# graph1 = trace(netD, real_cpu)\n",
    "# output = netD(real_cpu)\n",
    "# errD_real.backward()\n",
    "# graph2 = trace(netG, noise)\n",
    "# fake = netG(noise)\n",
    "# graph3 = trace(netD, fake.detach())\n",
    "# output = netD(fake.detach())\n",
    "# errD_fake.backward()\n",
    "# output = netD(fake)\n",
    "# graph4 = trace(netD, fake)\n",
    "# errG.backward()\n",
    "\n",
    "# D_x = output.mean().item()\n",
    "\n",
    "# # train with fake\n",
    "# noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "# label.fill_(fake_label)\n",
    "\n",
    "# errD_fake = criterion(output, label)\n",
    "\n",
    "# D_G_z1 = output.mean().item()\n",
    "# errD = errD_real + errD_fake\n",
    "# optimizerD.step()\n",
    "\n",
    "# ############################\n",
    "# # (2) Update G network: maximize log(D(G(z)))\n",
    "# ###########################\n",
    "# netG.zero_grad()\n",
    "# label.fill_(real_label)  # fake labels are real for generator cost\n",
    "\n",
    "# errG = criterion(output, label)\n",
    "\n",
    "# D_G_z2 = output.mean().item()\n",
    "# optimizerG.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from generator import Generator, get_mem_props, get_compute_props\n",
    "from utils.visualizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "from utils.logger import create_logger\n",
    "\n",
    "\n",
    "class Scheduling:\n",
    "    def __init__(self, hwfile=\"default.yaml\"):\n",
    "        base_dir = \"configs/\"\n",
    "        self.total_cycles = 0\n",
    "        self.logger = create_logger(\"logs/stats.txt\")\n",
    "        self.config = self.create_config(\n",
    "            yaml.load(open(base_dir + hwfile), Loader=yamlordereddictloader.Loader)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(self, config):\n",
    "\n",
    "    self.logger.info(\"Config Statistics : \")\n",
    "\n",
    "    self.mle = config[\"memory_levels\"]\n",
    "    self.mem_energy = np.zeros((self.mle))\n",
    "    self.compute_energy = 0\n",
    "    self.mem_read_access = np.zeros((self.mle))\n",
    "    self.mem_write_access = np.zeros((self.mle))\n",
    "    self.mem_size = np.zeros((self.mle))\n",
    "    self.mem_util = np.zeros((self.mle))\n",
    "    self.mem_free = np.zeros((self.mle))\n",
    "    self.mem_read_bw = np.zeros((self.mle))\n",
    "    self.mem_write_bw = np.zeros((self.mle))\n",
    "    self.internal_bandwidth_time = 0\n",
    "    self.total_cycles = 0 \n",
    "    self.bandwidth_idle_time = 0\n",
    "    self.compute_idle_time = 0\n",
    "    self.mem_size_idle_time = 0\n",
    "    self.force_connectivity = False\n",
    "    mm_compute = config[\"mm_compute\"]\n",
    "    vector_compute = config[\"vector_compute\"]\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"systolic_array\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"]) ** 2) * mm_compute[\"N_PE\"] / 2\n",
    "        )\n",
    "        config[\"comp_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "        self.logger.info(\n",
    "            \"MM Compute per cycle : %d\", config[\"mm_compute_per_cycle\"]\n",
    "        )\n",
    "        self.logger.info(\"Compute Bandwidth Required : %d\", config[\"comp_bw\"])\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"mac\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"])) * mm_compute[\"N_PE\"] / 2\n",
    "        )\n",
    "        config[\"comp_read_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "    for i in range(self.mle):\n",
    "        memory = config[\"memory\"][\"level\" + str(i)]\n",
    "        self.mem_read_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"read_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_write_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"write_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_size[i] = memory[\"size\"]\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Memory at Level %d, Read Bandwidth %d Write Bandwidth %d\",\n",
    "            i,\n",
    "            self.mem_read_bw[i],\n",
    "            self.mem_write_bw[i],\n",
    "        )\n",
    "    for i in range(self.mle - 1):\n",
    "        memory = config[\"memory\"][\"level\" + str(i)]\n",
    "        read_energy, write_energy, leakage_power = get_mem_props(\n",
    "            memory[\"size\"], memory[\"width\"], memory[\"banks\"]\n",
    "        )\n",
    "        config[\"memory\"][\"level\" + str(i)][\"read_energy\"] = str(read_energy)\n",
    "        config[\"memory\"][\"level\" + str(i)][\"write_energy\"] = str(write_energy)\n",
    "        config[\"memory\"][\"level\" + str(i)][\"leakage_power\"] = str(leakage_power)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, graph):\n",
    "\n",
    "    \"\"\"\n",
    "     Check both size, utilization and bandwidths at every node\n",
    "     What about memory size that can also get exhausted ?\n",
    "     So if memory size is exhausted, then have to go to a previous level and write there ?\n",
    "     if any level utilization is exhausted then only the immediate memory required will be kept.\n",
    "     if the memory is empty in size, but is not bandwidth, it is useless?\n",
    "     Cannot do prefetching\n",
    "     Read access of the next node will decrease\n",
    "     Bandwidth is available but size is not?, can do prefetching, but now the memory fetches have to check, \n",
    "     whether to do fetches of the same node or a different node\n",
    "     Say bandwidth at level0 is sufficient, at level1 is insufficient, then at level1 we have a bottlenecks\n",
    "     slower so it will take its own time\n",
    "     Do vector operations in the meantime perhaps ? \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config = self.config\n",
    "\n",
    "    read_bw_req = []\n",
    "    write_bw_req = []\n",
    "    read_bw_actual = []\n",
    "    write_bw_actual = []\n",
    "    cycles = []\n",
    "    free_cycles = []\n",
    "    transferable_checkpointed_edge = []\n",
    "    all_checkpointed_edge = []\n",
    "    self.mem_util_log=[]\n",
    "    self.mem_util_full=[]\n",
    "    # Mem Fetch time of the last Nodes\n",
    "#     print(self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "\n",
    "    mem_free = True\n",
    "    for n, node in enumerate(graph.nodes):\n",
    "\n",
    "        # These are last level read/write accesses\n",
    "        compute_expense, read_access, write_access = node.get_stats()\n",
    "#         print(read_access, node.in_edge_mem)\n",
    "        \n",
    "        self.mem_read_access[1]+=(read_access)\n",
    "      \n",
    "        self.logger.info(node.get_stats())\n",
    "        self.mem_util[0] += node.in_edge_mem\n",
    "        \n",
    "        # Total Free memory\n",
    "        for i in range(self.mle - 1):\n",
    "            self.mem_free[i] = self.mem_size[i] - self.mem_util[i]\n",
    "            \n",
    "#         print(\"2\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        time_compute = compute_expense / config[\"mm_compute_per_cycle\"]\n",
    "        read_bw_ll = read_access / (time_compute)\n",
    "        write_bw_ll = write_access / (time_compute)\n",
    "        step_cycles = time_compute\n",
    "        read_bw_req.append(read_bw_ll)\n",
    "        write_bw_req.append(write_bw_ll)\n",
    "        free_cycles.append(step_cycles)\n",
    "#         print(\"bandwidth\",read_bw_ll, write_bw_ll, step_cycles) \n",
    "        \n",
    "        if self.mem_free[0] < node.mem_util:\n",
    "            mem_free = False\n",
    "            # node mem_util = output edge\n",
    "            self.logger.info(\"Memory size is too low/ Memory is Full\")\n",
    "            self.logger.info(\"Node or Node memory Requirements too high\")\n",
    "            # Rearrange the checkpointed_nodes\n",
    "            # rearrange = True\n",
    "\n",
    "            # Is it possible now : Otherwise update the last level memory bandwidth requirements\n",
    "            if(self.mem_free[0]<0):      \n",
    "                step_cycles += 2*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem) + 1)*(\n",
    "                   (self.mem_free[0]+node.in_edge_mem)  / self.mem_read_bw[self.mle - 1]\n",
    "                ) + time_compute*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem) + 1)/256\n",
    "                # Change this later with the number of solid total cycles\n",
    "                self.mem_size_idle_time += 2*(node.in_edge_mem//self.mem_read_bw[self.mle - 1]) + time_compute/256*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem) + 1)\n",
    "                self.mem_read_access[0]+=node.in_edge_mem*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem)) \n",
    "                self.mem_write_access[0]+=node.in_edge_mem*(node.in_edge_mem//(self.mem_free[0]+node.in_edge_mem)) \n",
    "        else:\n",
    "            self.mem_util[0] += node.mem_util\n",
    "            self.mem_free[0] -= node.mem_util\n",
    "#         print(\"2.5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_log.append(self.mem_util[0])\n",
    "        self.mem_read_access[0]+=node.mem_util\n",
    "        self.mem_write_access[0]+=node.mem_util\n",
    "            \n",
    "        assert(self.mem_free[0] < self.mem_size[0])\n",
    "        # Last level memory fetch takes more time, so that may be a bottleneck\n",
    "        bandwidth_available = read_bw_ll < self.mem_read_bw[self.mle - 1]\n",
    "        \n",
    "        # If Bandwidth is not available : Cannot Prefetch\n",
    "        if (bandwidth_available) == False:\n",
    "            step_cycles += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "            self.bandwidth_idle_time += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "\n",
    "        # If memory is not free for the next node and Bandwidth is available : Move nodes back and forth\n",
    "        # if(total_mem_free[0] == 0 and (bandwidth_available)):\n",
    "        # for(nodes in checkpointed_nodes):\n",
    "        # checkpointed but not immediate node\n",
    "\n",
    "        # Check if memory is free and Bandwidth available : From the Data Dependence Graph, Prefetch new node\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        if self.mem_free[0] > 0 and (bandwidth_available):\n",
    "            if n < len(graph.nodes) - 1:\n",
    "                if self.mem_free[0] > node.next.mem_util:\n",
    "                    read_access += node.next.mem_util\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        self.mem_util[0] += node.next.mem_util\n",
    "                        self.mem_write_access[0] += node.next.mem_util\n",
    "                        self.mem_free[0] -= node.next.mem_util\n",
    "                        node.next.mem_util = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_write_access[0]+=read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_util = read_access - read_bw_ll * step_cycles\n",
    "\n",
    "                else:\n",
    "                    read_access += self.mem_free[0]\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        node.next.mem_util = node.next.mem_util - self.mem_free[0]\n",
    "                        self.mem_util[0] = self.mem_size[0]\n",
    "                        self.mem_free[0] = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_util = read_access - read_bw_ll * step_cycles\n",
    "#         print(\"3\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_full.append(self.mem_util[0])   \n",
    "        \n",
    "            # TODO Consider Write bandwidth for a block read memory or Write Bandwidth  for a endurance purposes\n",
    "        self.mem_util[0] -= node.in_edge_mem\n",
    "#         print(\"4\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "    \n",
    "        if mem_free:\n",
    "            self.mem_util[0] -= node.mem_util\n",
    "#         print(\"5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Node operator %r, Step Cycles %d, Read Accesses %d, Write Accesses %d \",\n",
    "            node.operator,\n",
    "            step_cycles,\n",
    "            read_access,\n",
    "            write_access,\n",
    "        )\n",
    "        self.total_cycles += step_cycles\n",
    "        cycles.append(step_cycles)\n",
    "        read_bw_actual.append(read_access / step_cycles)\n",
    "        write_bw_actual.append(write_access / step_cycles)\n",
    "#         print(\"actual\",read_access / step_cycles, write_access / step_cycles, step_cycles)\n",
    "#     print(\"The total cycles are \", self.total_cycles)\n",
    "    return read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduling.create_config = create_config\n",
    "Scheduling.run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Forward and Runner Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner_forward(graph_set):\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\" \n",
    "    generator = Generator()\n",
    "    bandwidth = [2,10,50,75,100]\n",
    "    num_iterations = 200\n",
    "    for graph in graph_set:\n",
    "#     for i in range(len(bandwidth)):\n",
    "        scheduler = Scheduling()\n",
    "#         scheduler.mem_read_bw[1] = 10*bandwidth[i]\n",
    "        read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "        read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "#         bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n",
    "#         cycles_bar_graph(\"cycles.png\", cycles, free_cycles, graph.nodes)\n",
    "#         mem_util_bar_graph(\"mem_util.png\",scheduler.mem_util_full/scheduler.mem_size[0],scheduler.mem_util_log/scheduler.mem_size[0], graph.nodes)\n",
    "        time, energy, design, tech = generator.save_stats(scheduler)\n",
    "#         time, energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            config = generator.backward_pass(scheduler)\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            _,_,_,_, cycles, free_cycles = scheduler.run(graph)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "#             time, energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner_forward([vgg11_graph])\n",
    "# runner_forward([resnet_graph])\n",
    "# runner_forward([dlrm_graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(graph_set, scheduler):\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\" \n",
    "    time_list = []\n",
    "    energy_list=[]\n",
    "    bandwidth_time_list = []\n",
    "    mem_size_idle_time_list=[]\n",
    "    bank_list=[]\n",
    "    mem_size_list=[]\n",
    "    compute_list=[]\n",
    "    tech_params_list = []\n",
    "    num_iterations = 6\n",
    "    generator = Generator()\n",
    "    bandwidth = [2,10,50,75,100]\n",
    "    for graph in graph_set:\n",
    "#     for i in range(len(bandwidth)):\n",
    "#         scheduler.mem_read_bw[1] = 10*bandwidth[i]\n",
    "        read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "        read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "#         bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n",
    "#         cycles_bar_graph(\"cycles.png\", cycles, free_cycles, graph.nodes)\n",
    "#         mem_util_bar_graph(\"mem_util.png\",scheduler.mem_util_full/scheduler.mem_size[0],scheduler.mem_util_log/scheduler.mem_size[0], graph.nodes)\n",
    "#         generator.save_statistics(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        time, energy, design, tech = generator.save_stats(scheduler)\n",
    "        time_list.append(time[0])\n",
    "        energy_list.append(energy[0])\n",
    "        bandwidth_time_list.append(time[1])\n",
    "        mem_size_idle_time_list.append(time[2])\n",
    "        bank_list.append(design[0])\n",
    "        mem_size_list.append(design[1])\n",
    "        tech_params_list.append(tech)\n",
    "        #         print(scheduler.config)\n",
    "        for i in range(num_iterations):\n",
    "            config = generator.backward_pass(scheduler, \"time\")\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "#             read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "# #             bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes, cycles)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "            time_list.append(time[0])\n",
    "            energy_list.append(energy[0])\n",
    "            bandwidth_time_list.append(time[1])\n",
    "            mem_size_idle_time_list.append(time[2])\n",
    "            bank_list.append(design[0])\n",
    "            mem_size_list.append(design[1])\n",
    "        return [time_list, bandwidth_time_list, mem_size_idle_time_list, bank_list, mem_size_list, compute_list, tech_params_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scheduler = Scheduling()\n",
    "time_list, bandwidth_time_list, mem_size_idle_time_list, bank_list, mem_size_list, compute_list, tech_params = runner([vgg11_graph], scheduler)\n",
    "# plot_descent(time_list, bandwidth_time_list, mem_size_idle_time_list)\n",
    "## Plot Design Parameters change over time\n",
    "# plot_design_param_change(bank_list, mem_size_list, compute_list)\n",
    "## Plot Technology Parameters change over time\n",
    "# plot_tech_param_change(tech_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Sweep Memory Banks/Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax2 = ax.twinx()\n",
    "# base_dir = \"figures/\"\n",
    "# error_config = {\"ecolor\": \"0.3\"}\n",
    "# index = np.arange(6)\n",
    "# for j in range(5):\n",
    "#     scheduler = Scheduling()\n",
    "#     scheduler.config[\"memory\"][\"level1\"][\"frequency\"]=5\n",
    "#     scheduler.config[\"memory\"][\"level1\"][\"frequency\"]*=2**j\n",
    "#     time_list, bandwidth_time_list,_,_,_= runner([vgg11_graph],scheduler)\n",
    "#     plot_descent_multiple(ax, time_list, bandwidth_time_list)\n",
    "#     plot_parameter_change_multiple(ax2, bank_list)\n",
    "\n",
    "# ax.legend(fontsize=20)\n",
    "# ax.set_xticks(index)\n",
    "# plt.xticks(rotation=80)\n",
    "# plt.rc(\"xtick\", labelsize=20)  # fontsize of the tick labels\n",
    "# plt.rc(\"ytick\", labelsize=20)\n",
    "# ax.set_ylabel(\"Grad descent time\", fontsize=20, fontweight=\"bold\")\n",
    "# ax2.set_ylabel(\"Grad descent parameters\", fontsize=20, fontweight=\"bold\")\n",
    "# fig.tight_layout()\n",
    "# plt.yscale(\"log\")\n",
    "# plt.savefig(base_dir + \"multiple_bandwidth.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Sweep Memory Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax2 = ax.twinx()\n",
    "# base_dir = \"figures/\"\n",
    "# error_config = {\"ecolor\": \"0.3\"}\n",
    "# index = np.arange(6)\n",
    "# for i in range(5,10):\n",
    "#     scheduler = Scheduling()\n",
    "#     scheduler.config[\"memory\"][\"level0\"][\"size\"]=10**(i/2)\n",
    "#     time_list, bandwidth_time_list,mem_size_idle_time_list, bank_list, mem_size_list, compute_list = runner([vgg11_graph],scheduler)\n",
    "#     plot_descent_multiple(ax, time_list, mem_size_idle_time_list)\n",
    "#     plot_parameter_change_multiple(ax2, mem_size_list)\n",
    "\n",
    "# ax.legend(fontsize=20)\n",
    "# ax.set_xticks(index)\n",
    "# plt.xticks(rotation=80)\n",
    "# plt.rc(\"xtick\", labelsize=20)  # fontsize of the tick labels\n",
    "# plt.rc(\"ytick\", labelsize=20)\n",
    "# ax.set_ylabel(\"Grad descent time\", fontsize=20, fontweight=\"bold\")\n",
    "# ax2.set_ylabel(\"Grad descent parameters\", fontsize=20, fontweight=\"bold\")\n",
    "# fig.tight_layout()\n",
    "# plt.yscale(\"log\")\n",
    "# plt.savefig(base_dir + \"memory_size_multiple.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technology Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "d = pd.read_csv('tables/sram.csv')\n",
    "a = np.array(d)\n",
    "\n",
    "input = a[:,:3]\n",
    "# output = a[:,3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('plugins/cacti/bus_width.out')\n",
    "\n",
    "\n",
    "d = d.drop(d.columns[[0,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]], axis=1)\n",
    "\n",
    "d = d.drop(' Associativity',1)\n",
    "d = d.drop(' Dynamic search energy (nJ)',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array(d)\n",
    "np.savetxt('bus.csv',a, fmt='%.18e', delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph(\"read_dummy.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_table = np.array(pd.read_csv(\"tables/sram.csv\", header=None))\n",
    "a = mem_table[np.where(mem_table[:, 1] == 4)]\n",
    "a = a[np.where(a[:, 2] == 32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute:\n",
    "    name       : MACs\n",
    "        class      : mac\n",
    "        attributes :            \n",
    "        instances       : 256\n",
    "        meshX           : 16\n",
    "        word-bits       : 16\n",
    "\n",
    "    memory: \n",
    "        name       : DRAM\n",
    "        class      : DRAM\n",
    "        attributes :\n",
    "        instances       : 1\n",
    "        word-bits       : 16\n",
    "  \n",
    "    name       : OutputBuffer\n",
    "        class      : SRAM\n",
    "        attributes :\n",
    "        entries         : 1024  # 64 * 16 = 1024\n",
    "        instances       : 1\n",
    "        meshX           : 1\n",
    "        word-bits       : 16\n",
    "        block-size      : 16\n",
    "        read_bandwidth  : 16 # words/cycle\n",
    "        write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : InputBuffer\n",
    "            class      : SRAM\n",
    "            attributes :\n",
    "            entries         : 1024 # 64 * 16 = 1024\n",
    "            instances       : 1\n",
    "            meshX           : 1\n",
    "            word-bits       : 16\n",
    "            block-size      : 16\n",
    "            read_bandwidth  : 16 # words/cycle\n",
    "            write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : PsumRegFile\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 1\n",
    "            instances       : 16\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 16\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "            \n",
    "        name       : WeightBuffer\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 64\n",
    "            instances       : 256\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 256\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "    noc:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
