{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "from torchvision import models\n",
    "from yaml import dump\n",
    "from dlrm.dlrm_s_pytorch import DLRM_Net, dash_separated_ints, dash_separated_floats\n",
    "from ir.handlers import handlers\n",
    "from ir.trace import trace\n",
    "from ir.trace import get_backprop_memory\n",
    "from utils.logger import create_logger\n",
    "from utils.visualizer import plot_descent\n",
    "from utils.visualizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import gym\n",
    "# import numpy as np\n",
    "# from itertools import count\n",
    "# from collections import namedtuple\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Categorical\n",
    "\n",
    "# # Cart Pole\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "#                     help='discount factor (default: 0.99)')\n",
    "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "#                     help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true',\n",
    "#                     help='render the environment')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='interval between training status logs (default: 10)')\n",
    "# args = parser.parse_args([])\n",
    "\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "# SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n",
    "# class Policy(nn.Module):\n",
    "#     \"\"\"\n",
    "#     implements both actor and critic in one model\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         super(Policy, self).__init__()\n",
    "#         self.affine1 = nn.Linear(4, 128)\n",
    "\n",
    "#         # actor's layer\n",
    "#         self.action_head = nn.Linear(128, 2)\n",
    "\n",
    "#         # critic's layer\n",
    "#         self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "#         # action & reward buffer\n",
    "#         self.saved_actions = []\n",
    "#         self.rewards = []\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         forward of both actor and critic\n",
    "#         \"\"\"\n",
    "#         x = F.relu(self.affine1(x))\n",
    "\n",
    "#         # actor: choses action to take from state s_t\n",
    "#         # by returning probability of each action\n",
    "#         action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "#         # critic: evaluates being in the state s_t\n",
    "#         state_values = self.value_head(x)\n",
    "\n",
    "#         # return values for both actor and critic as a tupel of 2 values:\n",
    "#         # 1. a list with the probability of each action over the action space\n",
    "#         # 2. the value from state s_t\n",
    "#         return action_prob, state_values\n",
    "\n",
    "\n",
    "# model = Policy()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "# eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "# def select_action(state):\n",
    "#     state = torch.from_numpy(state).float()\n",
    "#     probs, state_value = model(state)\n",
    "\n",
    "#     # create a categorical distribution over the list of probabilities of actions\n",
    "#     m = Categorical(probs)\n",
    "\n",
    "#     # and sample an action using the distribution\n",
    "#     action = m.sample()\n",
    "\n",
    "#     # save to action buffer\n",
    "#     model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "#     # the action to take (left or right)\n",
    "#     return action.item()\n",
    "\n",
    "\n",
    "# def finish_episode():\n",
    "#     \"\"\"\n",
    "#     Training code. Calcultes actor and critic loss and performs backprop.\n",
    "#     \"\"\"\n",
    "#     R = 0\n",
    "#     saved_actions = model.saved_actions\n",
    "#     policy_losses = [] # list to save actor (policy) loss\n",
    "#     value_losses = [] # list to save critic (value) loss\n",
    "#     returns = [] # list to save the true values\n",
    "\n",
    "#     # calculate the true value using rewards returned from the environment\n",
    "#     for r in model.rewards[::-1]:\n",
    "#         # calculate the discounted value\n",
    "#         R = r + args.gamma * R\n",
    "#         returns.insert(0, R)\n",
    "\n",
    "#     returns = torch.tensor(returns)\n",
    "#     returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "#     for (log_prob, value), R in zip(saved_actions, returns):\n",
    "#         advantage = R - value.item()\n",
    "\n",
    "#         # calculate actor (policy) loss\n",
    "#         policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "#         # calculate critic (value) loss using L1 smooth loss\n",
    "#         value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "#     # reset gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # sum up all the values of policy_losses and value_losses\n",
    "#     loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "#     # perform backprop\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # reset rewards and action buffer\n",
    "#     del model.rewards[:]\n",
    "#     del model.saved_actions[:]\n",
    "\n",
    "# #########################################################################3\n",
    "# running_reward = 10\n",
    "\n",
    "# # run inifinitely many episodes\n",
    "# for i_episode in count(1):\n",
    "\n",
    "#     # reset environment and episode reward\n",
    "#     state = env.reset()\n",
    "#     ep_reward = 0\n",
    "\n",
    "#     # for each episode, only run 9999 steps so that we don't\n",
    "#     # infinite loop while learning\n",
    "#     for t in range(1, 10000):\n",
    "\n",
    "#         # select action from policy\n",
    "#         action = select_action(state)\n",
    "\n",
    "#         # take the action\n",
    "#         state, reward, done, _ = env.step(action)\n",
    "\n",
    "#         if args.render:\n",
    "#             env.render()\n",
    "\n",
    "#         model.rewards.append(reward)\n",
    "#         ep_reward += reward\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     # update cumulative reward\n",
    "#     running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "#     # perform backprop\n",
    "#     finish_episode()\n",
    "\n",
    "#     # log results\n",
    "#     if i_episode % args.log_interval == 0:\n",
    "#         print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "#               i_episode, ep_reward, running_reward))\n",
    "\n",
    "#     # check if we have \"solved\" the cart pole problem\n",
    "#     if running_reward > env.spec.reward_threshold:\n",
    "#         print(\"Solved! Running reward is now {} and \"\n",
    "#               \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn import Linear\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "#         self.conv2 = GCNConv(4, 4)\n",
    "#         self.conv3 = GCNConv(4, 2)\n",
    "#         self.classifier = Linear(2, dataset.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "#         h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "#         # Apply a final (linear) classifier.\n",
    "#         out = self.classifier(h)\n",
    "\n",
    "#         return out, h\n",
    "\n",
    "# model = GCN()\n",
    "# from torch_geometric.datasets import KarateClub\n",
    "\n",
    "# dataset = KarateClub()\n",
    "# data = dataset[0] \n",
    "# _, h = model(data.x, data.edge_index)\n",
    "# gnn_graph = trace(model, (data.x, data.edge_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import argparse\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')\n",
    "\n",
    "PRIMITIVES = [\n",
    "    'none',\n",
    "    'max_pool_3x3',\n",
    "    'avg_pool_3x3',\n",
    "    'skip_connect',\n",
    "    'sep_conv_3x3',\n",
    "    'sep_conv_5x5',\n",
    "    'dil_conv_3x3',\n",
    "    'dil_conv_5x5'\n",
    "]\n",
    "\n",
    "NASNet = Genotype(\n",
    "  normal = [\n",
    "    ('sep_conv_5x5', 1),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('sep_conv_5x5', 0),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ('skip_connect', 0),\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('sep_conv_3x3', 1),\n",
    "    ('skip_connect', 1),\n",
    "  ],\n",
    "  normal_concat = [2, 3, 4, 5, 6],\n",
    "  reduce = [\n",
    "    ('sep_conv_5x5', 1),\n",
    "    ('sep_conv_7x7', 0),\n",
    "    ('max_pool_3x3', 1),\n",
    "    ('sep_conv_7x7', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ('sep_conv_5x5', 0),\n",
    "    ('skip_connect', 3),\n",
    "    ('avg_pool_3x3', 2),\n",
    "    ('sep_conv_3x3', 2),\n",
    "    ('max_pool_3x3', 1),\n",
    "  ],\n",
    "  reduce_concat = [4, 5, 6],\n",
    ")\n",
    "    \n",
    "AmoebaNet = Genotype(\n",
    "  normal = [\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('max_pool_3x3', 1),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('sep_conv_5x5', 2),\n",
    "    ('sep_conv_3x3', 0),\n",
    "    ('avg_pool_3x3', 3),\n",
    "    ('sep_conv_3x3', 1),\n",
    "    ('skip_connect', 1),\n",
    "    ('skip_connect', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ],\n",
    "  normal_concat = [4, 5, 6],\n",
    "  reduce = [\n",
    "    ('avg_pool_3x3', 0),\n",
    "    ('sep_conv_3x3', 1),\n",
    "    ('max_pool_3x3', 0),\n",
    "    ('sep_conv_7x7', 2),\n",
    "    ('sep_conv_7x7', 0),\n",
    "    ('avg_pool_3x3', 1),\n",
    "    ('max_pool_3x3', 0),\n",
    "    ('max_pool_3x3', 1),\n",
    "    ('conv_7x1_1x7', 0),\n",
    "    ('sep_conv_3x3', 5),\n",
    "  ],\n",
    "  reduce_concat = [3, 4, 6]\n",
    ")\n",
    "\n",
    "DARTS_V1 = Genotype(normal=[('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('skip_connect', 0), ('sep_conv_3x3', 1), ('skip_connect', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('skip_connect', 2)], normal_concat=[2, 3, 4, 5], reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('skip_connect', 2), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('skip_connect', 2), ('skip_connect', 2), ('avg_pool_3x3', 0)], reduce_concat=[2, 3, 4, 5])\n",
    "DARTS_V2 = Genotype(normal=[('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 1), ('skip_connect', 0), ('skip_connect', 0), ('dil_conv_3x3', 2)], normal_concat=[2, 3, 4, 5], reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('skip_connect', 2), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 2), ('skip_connect', 2), ('max_pool_3x3', 1)], reduce_concat=[2, 3, 4, 5])\n",
    "\n",
    "DARTS = DARTS_V2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkImageNet(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes, layers, auxiliary, genotype):\n",
    "    super(NetworkImageNet, self).__init__()\n",
    "    self._layers = layers\n",
    "    self._auxiliary = auxiliary\n",
    "\n",
    "    self.stem0 = nn.Sequential(\n",
    "      nn.Conv2d(3, C // 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(C // 2),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(C // 2, C, 3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(C),\n",
    "    )\n",
    "\n",
    "    self.stem1 = nn.Sequential(\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(C, C, 3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(C),\n",
    "    )\n",
    "\n",
    "    C_prev_prev, C_prev, C_curr = C, C, C\n",
    "\n",
    "    self.cells = nn.ModuleList()\n",
    "    reduction_prev = True\n",
    "    for i in range(layers):\n",
    "      if i in [layers // 3, 2 * layers // 3]:\n",
    "        C_curr *= 2\n",
    "        reduction = True\n",
    "      else:\n",
    "        reduction = False\n",
    "      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
    "      reduction_prev = reduction\n",
    "      self.cells += [cell]\n",
    "      C_prev_prev, C_prev = C_prev, cell.multiplier * C_curr\n",
    "      if i == 2 * layers // 3:\n",
    "        C_to_auxiliary = C_prev\n",
    "\n",
    "    if auxiliary:\n",
    "      self.auxiliary_head = AuxiliaryHeadImageNet(C_to_auxiliary, num_classes)\n",
    "    self.global_pooling = nn.AvgPool2d(7)\n",
    "    self.classifier = nn.Linear(C_prev, num_classes)\n",
    "\n",
    "  def forward(self, input):\n",
    "    logits_aux = None\n",
    "    s0 = self.stem0(input)\n",
    "    s1 = self.stem1(s0)\n",
    "    for i, cell in enumerate(self.cells):\n",
    "      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\n",
    "      if i == 2 * self._layers // 3:\n",
    "        if self._auxiliary and self.training:\n",
    "          logits_aux = self.auxiliary_head(s1)\n",
    "    out = self.global_pooling(s1)\n",
    "    logits = self.classifier(out.view(out.size(0), -1))\n",
    "    return logits, logits_aux\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryHeadImageNet(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes):\n",
    "    \"\"\"assuming input size 14x14\"\"\"\n",
    "    super(AuxiliaryHeadImageNet, self).__init__()\n",
    "    self.features = nn.Sequential(\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.AvgPool2d(5, stride=2, padding=0, count_include_pad=False),\n",
    "      nn.Conv2d(C, 128, 1, bias=False),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(128, 768, 2, bias=False),\n",
    "      # NOTE: This batchnorm was omitted in my earlier implementation due to a typo.\n",
    "      # Commenting it out for consistency with the experiments in the paper.\n",
    "      # nn.BatchNorm2d(768),\n",
    "      nn.ReLU(inplace=True)\n",
    "    )\n",
    "    self.classifier = nn.Linear(768, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    x = self.classifier(x.view(x.size(0),-1))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(nn.Module):\n",
    "\n",
    "  def __init__(self, genotype, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
    "    super(Cell, self).__init__()\n",
    "    print(C_prev_prev, C_prev, C)\n",
    "\n",
    "    if reduction_prev:\n",
    "      self.preprocess0 = FactorizedReduce(C_prev_prev, C)\n",
    "    else:\n",
    "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0)\n",
    "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0)\n",
    "    \n",
    "    if reduction:\n",
    "      op_names, indices = zip(*genotype.reduce)\n",
    "      concat = genotype.reduce_concat\n",
    "    else:\n",
    "      op_names, indices = zip(*genotype.normal)\n",
    "      concat = genotype.normal_concat\n",
    "    self._compile(C, op_names, indices, concat, reduction)\n",
    "\n",
    "  def _compile(self, C, op_names, indices, concat, reduction):\n",
    "    assert len(op_names) == len(indices)\n",
    "    self._steps = len(op_names) // 2\n",
    "    self._concat = concat\n",
    "    self.multiplier = len(concat)\n",
    "\n",
    "    self._ops = nn.ModuleList()\n",
    "    for name, index in zip(op_names, indices):\n",
    "      stride = 2 if reduction and index < 2 else 1\n",
    "      op = OPS[name](C, stride, True)\n",
    "      self._ops += [op]\n",
    "    self._indices = indices\n",
    "\n",
    "  def forward(self, s0, s1, drop_prob):\n",
    "    s0 = self.preprocess0(s0)\n",
    "    s1 = self.preprocess1(s1)\n",
    "\n",
    "    states = [s0, s1]\n",
    "    for i in range(self._steps):\n",
    "      h1 = states[self._indices[2*i]]\n",
    "      h2 = states[self._indices[2*i+1]]\n",
    "      op1 = self._ops[2*i]\n",
    "      op2 = self._ops[2*i+1]\n",
    "      h1 = op1(h1)\n",
    "      h2 = op2(h2)\n",
    "      if self.training and drop_prob > 0.:\n",
    "        if not isinstance(op1, Identity):\n",
    "          h1 = drop_path(h1, drop_prob)\n",
    "        if not isinstance(op2, Identity):\n",
    "          h2 = drop_path(h2, drop_prob)\n",
    "      s = h1 + h2\n",
    "      states += [s]\n",
    "    return torch.cat([states[i] for i in self._concat], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data', type=str, default='../data/imagenet/', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1, help='init learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-5, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=100, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=250, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=48, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=14, help='total number of layers')\n",
    "parser.add_argument('--auxiliary', action='store_true', default=False, help='use auxiliary tower')\n",
    "parser.add_argument('--auxiliary_weight', type=float, default=0.4, help='weight for auxiliary loss')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "parser.add_argument('--arch', type=str, default='DARTS', help='which architecture to use')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='gradient clipping')\n",
    "parser.add_argument('--label_smooth', type=float, default=0.1, help='label smoothing')\n",
    "parser.add_argument('--gamma', type=float, default=0.97, help='learning rate decay')\n",
    "parser.add_argument('--decay_period', type=int, default=1, help='epochs between two learning rate decays')\n",
    "parser.add_argument('--parallel', action='store_true', default=False, help='data parallelism')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from plugins.darts.cnn.operations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48 48\n",
      "48 192 48\n",
      "192 192 48\n",
      "192 192 48\n",
      "192 192 96\n",
      "192 384 96\n",
      "384 384 96\n",
      "384 384 96\n",
      "384 384 96\n",
      "384 384 192\n",
      "384 768 192\n",
      "768 768 192\n",
      "768 768 192\n",
      "768 768 192\n"
     ]
    }
   ],
   "source": [
    "CLASSES = 1000\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = NetworkImageNet(args.init_channels, CLASSES, args.layers, args.auxiliary, DARTS)\n",
    "\n",
    "inputs = torch.randn(1, 3, 299, 299)\n",
    "inputs = Variable(inputs)\n",
    "\n",
    "dart_graph = trace(model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "# import argparse\n",
    "# import os\n",
    "# import random\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.parallel\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# import torch.optim as optim\n",
    "# import torch.utils.data\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')\n",
    "# # parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "# parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "# parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "# parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')\n",
    "# parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "# parser.add_argument('--ngf', type=int, default=64)\n",
    "# parser.add_argument('--ndf', type=int, default=64)\n",
    "# parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "# parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
    "# parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "# parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "# parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "# parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "# parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "# parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "# parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "# parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')\n",
    "\n",
    "# opt = parser.parse_args([])\n",
    "# print(opt)\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(opt.outf)\n",
    "# except OSError:\n",
    "#     pass\n",
    "\n",
    "# if opt.manualSeed is None:\n",
    "#     opt.manualSeed = random.randint(1, 10000)\n",
    "# print(\"Random Seed: \", opt.manualSeed)\n",
    "# random.seed(opt.manualSeed)\n",
    "# torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
    "# ngpu = int(opt.ngpu)\n",
    "# nz = int(opt.nz)\n",
    "# ngf = int(opt.ngf)\n",
    "# ndf = int(opt.ndf)\n",
    "# nc = 3\n",
    "\n",
    "# # custom weights initialization called on netG and netD\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         m.weight.data.normal_(0.0, 0.02)\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         m.weight.data.normal_(1.0, 0.02)\n",
    "#         m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, ngpu):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is Z, going into a convolution\n",
    "#             nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 8),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf*8) x 4 x 4\n",
    "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 4),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf*4) x 8 x 8\n",
    "#             nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 2),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf*2) x 16 x 16\n",
    "#             nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf),\n",
    "#             nn.ReLU(True),\n",
    "#             # state size. (ngf) x 32 x 32\n",
    "#             nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "#             nn.Tanh()\n",
    "#             # state size. (nc) x 64 x 64\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         if input.is_cuda and self.ngpu > 1:\n",
    "#             output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "#         else:\n",
    "#             output = self.main(input)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# netG = Generator(ngpu).to(device)\n",
    "# netG.apply(weights_init)\n",
    "# if opt.netG != '':\n",
    "#     netG.load_state_dict(torch.load(opt.netG))\n",
    "\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, ngpu):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf) x 32 x 32\n",
    "#             nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 2),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*2) x 16 x 16\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 4),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*4) x 8 x 8\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 8),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*8) x 4 x 4\n",
    "#             nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         if input.is_cuda and self.ngpu > 1:\n",
    "#             output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "#         else:\n",
    "#             output = self.main(input)\n",
    "\n",
    "#         return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "# netD = Discriminator(ngpu).to(device)\n",
    "# netD.apply(weights_init)\n",
    "# if opt.netD != '':\n",
    "#     netD.load_state_dict(torch.load(opt.netD))\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "# real_label = 1\n",
    "# fake_label = 0\n",
    "\n",
    "# # setup optimizer\n",
    "# optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "# optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "# ############################\n",
    "# # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "# ###########################\n",
    "# # train with real\n",
    "# netD.zero_grad()\n",
    "# real_cpu = data[0].to(device)\n",
    "# batch_size = real_cpu.size(0)\n",
    "# label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "\n",
    "# errD_real = criterion(output, label)\n",
    "\n",
    "\n",
    "# D_x = output.mean().item()\n",
    "\n",
    "# # train with fake\n",
    "# noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "# label.fill_(fake_label)\n",
    "\n",
    "# errD_fake = criterion(output, label)\n",
    "\n",
    "# D_G_z1 = output.mean().item()\n",
    "# errD = errD_real + errD_fake\n",
    "# optimizerD.step()\n",
    "\n",
    "# ############################\n",
    "# # (2) Update G network: maximize log(D(G(z)))\n",
    "# ###########################\n",
    "# netG.zero_grad()\n",
    "# label.fill_(real_label)  # fake labels are real for generator cost\n",
    "\n",
    "# errG = criterion(output, label)\n",
    "\n",
    "# D_G_z2 = output.mean().item()\n",
    "# optimizerG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph1 = trace(netD, real_cpu)\n",
    "# output = netD(real_cpu)\n",
    "# errD_real.backward()\n",
    "# graph2 = trace(netG, noise)\n",
    "# fake = netG(noise)\n",
    "# graph3 = trace(netD, fake.detach())\n",
    "# output = netD(fake.detach())\n",
    "# errD_fake.backward()\n",
    "# output = netD(fake)\n",
    "# graph4 = trace(netD, fake)\n",
    "# errG.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from generator import Generator, get_mem_props, get_compute_props\n",
    "from generator import *\n",
    "from utils.visualizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "\n",
    "from utils.logger import create_logger\n",
    "\n",
    "\n",
    "class Scheduling:\n",
    "    def __init__(self, hwfile=\"default.yaml\"):\n",
    "        base_dir = \"configs/\"\n",
    "        self.total_cycles = 0\n",
    "        self.technology = [1,1,40] \n",
    "        # maybe change this later to peripheral logic node or speed\n",
    "#     [wire_cap , sense_amp_time, plogic_node],\n",
    "        self.logger = create_logger(\"logs/stats.txt\")\n",
    "        self.config = self.create_config(\n",
    "            yaml.load(open(base_dir + hwfile), Loader=yamlordereddictloader.Loader)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(self, config):\n",
    "\n",
    "    self.logger.info(\"Config Statistics : \")\n",
    "\n",
    "    self.mle = config[\"memory_levels\"]\n",
    "    self.mem_energy = np.zeros((self.mle))\n",
    "    self.compute_energy = 0\n",
    "    self.mem_read_access = np.zeros((self.mle))\n",
    "    self.mem_write_access = np.zeros((self.mle))\n",
    "    self.mem_size = np.zeros((self.mle))\n",
    "    self.mem_util = np.zeros((self.mle))\n",
    "    self.mem_free = np.zeros((self.mle))\n",
    "    self.mem_read_bw = np.zeros((self.mle))\n",
    "    self.mem_write_bw = np.zeros((self.mle))\n",
    "    self.internal_bandwidth_time = 0\n",
    "    self.total_cycles = 0 \n",
    "    self.bandwidth_idle_time = 0\n",
    "    self.compute_idle_time = 0\n",
    "    self.mem_size_idle_time = 0\n",
    "\n",
    "    self.force_connectivity = False\n",
    "    mm_compute = config[\"mm_compute\"]\n",
    "    vector_compute = config[\"vector_compute\"]\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"systolic_array\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"]) ** 2) * mm_compute[\"N_PE\"] / (2*4)\n",
    "        )\n",
    "        config[\"comp_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "        self.logger.info(\n",
    "            \"MM Compute per cycle : %d\", config[\"mm_compute_per_cycle\"]\n",
    "        )\n",
    "        self.logger.info(\"Compute Bandwidth Required : %d\", config[\"comp_bw\"])\n",
    "\n",
    "    if config[\"mm_compute\"][\"class\"] == \"mac\":\n",
    "        config[\"mm_compute_per_cycle\"] = (\n",
    "            ((mm_compute[\"size\"])) * mm_compute[\"N_PE\"] / 2\n",
    "        )\n",
    "        config[\"comp_read_bw\"] = (\n",
    "            mm_compute[\"size\"] * mm_compute[\"N_PE\"] * mm_compute[\"frequency\"] * 2\n",
    "        )\n",
    "\n",
    "    for i in range(self.mle):\n",
    "        memory = config[\"memory\"][\"level\" + str(i)]\n",
    "        self.mem_read_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"read_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_write_bw[i] = (\n",
    "            memory[\"frequency\"]\n",
    "            * memory[\"banks\"]\n",
    "            * memory[\"write_ports\"]\n",
    "            * memory[\"width\"]\n",
    "        )\n",
    "        self.mem_size[i] = memory[\"size\"]\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Memory at Level %d, Read Bandwidth %d Write Bandwidth %d\",\n",
    "            i,\n",
    "            self.mem_read_bw[i],\n",
    "            self.mem_write_bw[i],\n",
    "        )\n",
    "    for i in range(self.mle - 1):\n",
    "        memory = config[\"memory\"][\"level\" + str(i)]\n",
    "        read_energy, write_energy, leakage_power = get_mem_props(\n",
    "            memory[\"size\"], memory[\"width\"], memory[\"banks\"]\n",
    "        )\n",
    "        config[\"memory\"][\"level\" + str(i)][\"read_energy\"] = str(read_energy)\n",
    "        config[\"memory\"][\"level\" + str(i)][\"write_energy\"] = str(write_energy)\n",
    "        config[\"memory\"][\"level\" + str(i)][\"leakage_power\"] = str(leakage_power)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, graph):\n",
    "\n",
    "    \"\"\"\n",
    "     Check both size, utilization and bandwidths at every node\n",
    "     What about memory size that can also get exhausted ?\n",
    "     So if memory size is exhausted, then have to go to a previous level and write there ?\n",
    "     if any level utilization is exhausted then only the immediate memory required will be kept.\n",
    "     if the memory is empty in size, but is not bandwidth, it is useless?\n",
    "     Cannot do prefetching\n",
    "     Read access of the next node will decrease\n",
    "     Bandwidth is available but size is not?, can do prefetching, but now the memory fetches have to check, \n",
    "     whether to do fetches of the same node or a different node\n",
    "     Say bandwidth at level0 is sufficient, at level1 is insufficient, then at level1 we have a bottlenecks\n",
    "     slower so it will take its own time\n",
    "     Do vector operations in the meantime perhaps ? \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config = self.config\n",
    "\n",
    "    read_bw_req = []\n",
    "    write_bw_req = []\n",
    "    read_bw_actual = []\n",
    "    write_bw_actual = []\n",
    "    cycles = []\n",
    "    free_cycles = []\n",
    "    transferable_checkpointed_edge = []\n",
    "    all_checkpointed_edge = []\n",
    "    self.mem_util_log=[]\n",
    "    self.mem_util_full=[]\n",
    "    # Mem Fetch time of the last Nodes\n",
    "#     print(self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "\n",
    "    mem_free = True\n",
    "    for n, node in enumerate(graph.nodes):\n",
    "\n",
    "        # These are last level read/write accesses\n",
    "        compute_expense, weights = node.get_stats()\n",
    "        read_access = node.mem_fetch\n",
    "        write_access = 0\n",
    "        self.mem_read_access[1]+=(weights)\n",
    "        \n",
    "        self.mem_util[0] += node.in_edge_mem\n",
    "        node.mem_util = node.out_edge_mem + node.mem_fetch\n",
    "        # Total Free memory\n",
    "        for i in range(self.mle - 1):\n",
    "            self.mem_free[i] = self.mem_size[i] - self.mem_util[i]\n",
    "            \n",
    "#         print(\"2\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        time_compute = compute_expense / config[\"mm_compute_per_cycle\"]\n",
    "        read_bw_ll = read_access / (time_compute)\n",
    "        write_bw_ll = write_access / (time_compute)\n",
    "        step_cycles = time_compute\n",
    "        read_bw_req.append(read_bw_ll)\n",
    "        write_bw_req.append(write_bw_ll)\n",
    "        free_cycles.append(step_cycles)\n",
    "#         print(\"bandwidth\",read_bw_ll, write_bw_ll, step_cycles) \n",
    "        \n",
    "        if self.mem_free[0] < node.mem_util:\n",
    "            mem_free = False\n",
    "            # node mem_util = output edge\n",
    "            self.logger.info(\"Memory size is too low/ Memory is Full\")\n",
    "            self.logger.info(\"Node or Node memory Requirements too high\")\n",
    "            # Rearrange the checkpointed_nodes\n",
    "            # rearrange = True\n",
    "\n",
    "            # Is it possible now : Otherwise update the last level memory bandwidth requirements\n",
    "            assert (self.mem_free[0]+node.in_edge_mem)>0\n",
    "            # Change this later with the number of solid total cycles\n",
    "            used_pe_ratio = (self.mem_free[0]+node.in_edge_mem)/(node.mem_util+node.in_edge_mem)\n",
    "            n_swaps = int(1/used_pe_ratio+1)\n",
    "            swap_time = max(config[\"mm_compute\"][\"size\"]*4,time_compute//n_swaps)\n",
    "            self.mem_size_idle_time += swap_time*n_swaps + (node.mem_util+node.in_edge_mem)//self.mem_read_bw[self.mle-1] \n",
    "            step_cycles+=self.mem_size_idle_time\n",
    "            self.mem_read_access[0]+=(node.mem_util+node.in_edge_mem)\n",
    "            self.mem_write_access[0]+=(node.mem_util+node.in_edge_mem)\n",
    "        else:\n",
    "            self.mem_util[0] += node.mem_util\n",
    "            self.mem_free[0] -= node.mem_util\n",
    "#         print(\"2.5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_log.append(self.mem_util[0])\n",
    "        self.mem_read_access[0]+=node.weights+node.out_edge_mem\n",
    "        self.mem_write_access[0]+=node.weights+node.out_edge_mem\n",
    "        assert(self.mem_free[0] < self.mem_size[0])\n",
    "        # Last level memory fetch takes more time, so that may be a bottleneck\n",
    "        bandwidth_available = read_bw_ll < self.mem_read_bw[self.mle - 1]\n",
    "        \n",
    "        # If Bandwidth is not available : Cannot Prefetch\n",
    "        if (bandwidth_available) == False:\n",
    "            step_cycles += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "            self.bandwidth_idle_time += (\n",
    "                read_bw_ll / self.mem_read_bw[self.mle - 1]\n",
    "            - 1) * time_compute\n",
    "\n",
    "        # If memory is not free for the next node and Bandwidth is available : Move nodes back and forth\n",
    "        # if(total_mem_free[0] == 0 and (bandwidth_available)):\n",
    "        # for(nodes in checkpointed_nodes):\n",
    "        # checkpointed but not immediate node\n",
    "\n",
    "        # Check if memory is free and Bandwidth available : From the Data Dependence Graph, Prefetch new node\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        if self.mem_free[0] > 0 and (bandwidth_available):\n",
    "            if n < len(graph.nodes) - 1:\n",
    "                if self.mem_free[0] > node.next.mem_fetch:\n",
    "                    read_access += node.next.mem_fetch\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        self.mem_util[0] += node.next.mem_fetch\n",
    "                        self.mem_free[0] -= node.next.mem_fetch\n",
    "                        node.next.mem_fetch = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_fetch = read_access - read_bw_ll * step_cycles\n",
    "\n",
    "                else:\n",
    "                    read_access += self.mem_free[0]\n",
    "                    if read_access / step_cycles < self.mem_read_bw[self.mle - 1]:\n",
    "                        node.next.mem_fetch = node.next.mem_fetch - self.mem_free[0]\n",
    "                        self.mem_util[0] = self.mem_size[0]\n",
    "                        self.mem_free[0] = 0\n",
    "                    else:\n",
    "                        read_access = self.mem_read_bw[self.mle - 1] * step_cycles\n",
    "                        self.mem_util[0] += read_access - read_bw_ll * step_cycles\n",
    "                        self.mem_free[0] -= read_access - read_bw_ll * step_cycles\n",
    "                        node.next.mem_fetch = read_access - read_bw_ll * step_cycles\n",
    "#         print(\"3\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        self.mem_util_full.append(self.mem_util[0])   \n",
    "        \n",
    "            # TODO Consider Write bandwidth for a block read memory or Write Bandwidth  for a endurance purposes\n",
    "        self.mem_util[0] -= node.in_edge_mem\n",
    "#         print(\"4\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "    \n",
    "        if mem_free:\n",
    "            self.mem_util[0] -= node.mem_util\n",
    "#         print(\"5\",self.mem_free[0], self.mem_util[0], self.mem_size[0])\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Node operator %r, Step Cycles %d, Read Accesses %d, Write Accesses %d \",\n",
    "            node.operator,\n",
    "            step_cycles,\n",
    "            read_access,\n",
    "            write_access,\n",
    "        )\n",
    "        self.total_cycles += step_cycles\n",
    "        cycles.append(step_cycles)\n",
    "        read_bw_actual.append(read_access / step_cycles)\n",
    "        write_bw_actual.append(write_access / step_cycles)\n",
    "#         print(\"actual\",read_access / step_cycles, write_access / step_cycles, step_cycles)\n",
    "#     print(\"The total cycles are \", self.total_cycles)\n",
    "    return read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduling.create_config = create_config\n",
    "Scheduling.run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Forward and Runner Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner_forward(graph_set):\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\" \n",
    "    generator = Generator()\n",
    "    bandwidth = [2,10,50,75,100]\n",
    "    num_iterations = 50\n",
    "    for graph in graph_set:\n",
    "#     for i in range(len(bandwidth)):\n",
    "        scheduler = Scheduling()\n",
    "#         scheduler.mem_read_bw[1] = 10*bandwidth[i]\n",
    "        read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "        read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "#         bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n",
    "#         cycles_bar_graph(\"cycles.png\", cycles, free_cycles, graph.nodes)\n",
    "#         mem_util_bar_graph(\"mem_util.png\",scheduler.mem_util_full/scheduler.mem_size[0],scheduler.mem_util_log/scheduler.mem_size[0], graph.nodes)\n",
    "        in_time, in_energy, design, tech = generator.save_stats(scheduler)\n",
    "#         in_time, in_energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        print(\"======Optimizing Design and Connectivity=========\")\n",
    "        for i in range(num_iterations):\n",
    "            config = generator.backward_pass(scheduler)\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            _,_,_,_, cycles, free_cycles = scheduler.run(graph)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "#             time, energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        print(in_time[0]//time[0], in_energy[0]//energy[0])\n",
    "        print(\"===============Optimizing Technology=============\")\n",
    "        for i in range(10):\n",
    "            config = generator.backward_pass_tech(scheduler,\"time\")\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            _,_,_,_, cycles, free_cycles = scheduler.run(graph)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "#             time, energy, design, tech = generator.save_stats(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        print(in_time[0]//time[0], in_energy[0]//energy[0])\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"=====================================VGG-Net=============================================\")\n",
    "# runner_forward([vgg11_graph])\n",
    "# print(\"=====================================ResNet===============================================\")\n",
    "# runner_forward([resnet_graph])\n",
    "# runner_forward([dlrm_graph])\n",
    "# runner_forward([gnn_graph])\n",
    "# runner_forward([dart_graph])\n",
    "# runner_forward([bert_graph])\n",
    "# runner_forward([gpt2_graph])\n",
    "# runner_forward([a3c_graph]) # Actor Critic\n",
    "# runner_forward([gan_graph]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(graph_set, scheduler):\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\" \n",
    "    time_list = []\n",
    "    energy_list=[]\n",
    "    bandwidth_time_list = []\n",
    "    mem_size_idle_time_list=[]\n",
    "    bank_list=[]\n",
    "    mem_size_list=[]\n",
    "    compute_list=[]\n",
    "    tech_params_list = []\n",
    "    num_iterations = 6\n",
    "    generator = Generator()\n",
    "    bandwidth = [2,10,50,75,100]\n",
    "    for graph in graph_set:\n",
    "#     for i in range(len(bandwidth)):\n",
    "#         scheduler.mem_read_bw[1] = 10*bandwidth[i]\n",
    "        read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "        read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "#         bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n",
    "#         cycles_bar_graph(\"cycles.png\", cycles, free_cycles, graph.nodes)\n",
    "#         mem_util_bar_graph(\"mem_util.png\",scheduler.mem_util_full/scheduler.mem_size[0],scheduler.mem_util_log/scheduler.mem_size[0], graph.nodes)\n",
    "#         generator.save_statistics(scheduler, True, get_backprop_memory(graph.nodes))\n",
    "        time, energy, design, tech = generator.save_stats(scheduler)\n",
    "        time_list.append(time[0])\n",
    "        energy_list.append(energy[0])\n",
    "        bandwidth_time_list.append(time[1])\n",
    "        mem_size_idle_time_list.append(time[2])\n",
    "        bank_list.append(design[0])\n",
    "        mem_size_list.append(design[1])\n",
    "        tech_params_list.append(tech)\n",
    "        #         print(scheduler.config)\n",
    "        for i in range(num_iterations):\n",
    "            config = generator.backward_pass(scheduler, \"time\")\n",
    "            generator.writeconfig(config, str(i) + \"hw.yaml\")\n",
    "            scheduler.create_config(config)\n",
    "            read_bw_req, write_bw_req, read_bw_actual, write_bw_actual, cycles, free_cycles = scheduler.run(graph)\n",
    "#             read_bw_limit, write_bw_limit = scheduler.mem_read_bw[scheduler.mle - 1], scheduler.mem_write_bw[scheduler.mle - 1]   \n",
    "# #             bandwidth_bar_graph(\"read_full.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes, cycles)\n",
    "            time, energy, design, tech = generator.save_stats(scheduler)\n",
    "            time_list.append(time[0])\n",
    "            energy_list.append(energy[0])\n",
    "            bandwidth_time_list.append(time[1])\n",
    "            mem_size_idle_time_list.append(time[2])\n",
    "            bank_list.append(design[0])\n",
    "            mem_size_list.append(design[1])\n",
    "            tech_params_list.append(tech)\n",
    "        return [time_list, bandwidth_time_list, mem_size_idle_time_list, bank_list, mem_size_list, compute_list, tech_params_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scheduler = Scheduling()\n",
    "# time_list, bandwidth_time_list, mem_size_idle_time_list, bank_list, mem_size_list, compute_list, tech_params = runner([vgg11_graph], scheduler)\n",
    "# plot_descent(time_list, bandwidth_time_list, mem_size_idle_time_list)\n",
    "# print(tech_params)\n",
    "# Plot Design Parameters change over time\n",
    "# plot_design_param_change(bank_list, mem_size_list, compute_list)\n",
    "# Plot Technology Parameters change over time\n",
    "# plot_tech_param_change(np.transpose(np.array(tech_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Sweep Memory Banks/Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax2 = ax.twinx()\n",
    "# base_dir = \"figures/\"\n",
    "# error_config = {\"ecolor\": \"0.3\"}\n",
    "# index = np.arange(6)\n",
    "# for j in range(5):\n",
    "#     scheduler = Scheduling()\n",
    "#     scheduler.config[\"memory\"][\"level1\"][\"frequency\"]=5\n",
    "#     scheduler.config[\"memory\"][\"level1\"][\"frequency\"]*=2**j\n",
    "#     time_list, bandwidth_time_list,_,_,_= runner([vgg11_graph],scheduler)\n",
    "#     plot_descent_multiple(ax, time_list, bandwidth_time_list)\n",
    "#     plot_parameter_change_multiple(ax2, bank_list)\n",
    "\n",
    "# ax.legend(fontsize=20)\n",
    "# ax.set_xticks(index)\n",
    "# plt.xticks(rotation=80)\n",
    "# plt.rc(\"xtick\", labelsize=20)  # fontsize of the tick labels\n",
    "# plt.rc(\"ytick\", labelsize=20)\n",
    "# ax.set_ylabel(\"Grad descent time\", fontsize=20, fontweight=\"bold\")\n",
    "# ax2.set_ylabel(\"Grad descent parameters\", fontsize=20, fontweight=\"bold\")\n",
    "# fig.tight_layout()\n",
    "# plt.yscale(\"log\")\n",
    "# plt.savefig(base_dir + \"multiple_bandwidth.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Sweep Memory Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax2 = ax.twinx()\n",
    "# base_dir = \"figures/\"\n",
    "# error_config = {\"ecolor\": \"0.3\"}\n",
    "# index = np.arange(6)\n",
    "# for i in range(5,10):\n",
    "#     scheduler = Scheduling()\n",
    "#     scheduler.config[\"memory\"][\"level0\"][\"size\"]=10**(i/2)\n",
    "#     time_list, bandwidth_time_list,mem_size_idle_time_list, bank_list, mem_size_list, compute_list = runner([vgg11_graph],scheduler)\n",
    "#     plot_descent_multiple(ax, time_list, mem_size_idle_time_list)\n",
    "#     plot_parameter_change_multiple(ax2, mem_size_list)\n",
    "\n",
    "# ax.legend(fontsize=20)\n",
    "# ax.set_xticks(index)\n",
    "# plt.xticks(rotation=80)\n",
    "# plt.rc(\"xtick\", labelsize=20)  # fontsize of the tick labels\n",
    "# plt.rc(\"ytick\", labelsize=20)\n",
    "# ax.set_ylabel(\"Grad descent time\", fontsize=20, fontweight=\"bold\")\n",
    "# ax2.set_ylabel(\"Grad descent parameters\", fontsize=20, fontweight=\"bold\")\n",
    "# fig.tight_layout()\n",
    "# plt.yscale(\"log\")\n",
    "# plt.savefig(base_dir + \"memory_size_multiple.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technology Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "d = pd.read_csv('tables/sram.csv')\n",
    "a = np.array(d)\n",
    "\n",
    "input = a[:,:3]\n",
    "# output = a[:,3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('plugins/cacti/bus_width.out')\n",
    "\n",
    "\n",
    "d = d.drop(d.columns[[0,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]], axis=1)\n",
    "\n",
    "d = d.drop(' Associativity',1)\n",
    "d = d.drop(' Dynamic search energy (nJ)',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array(d)\n",
    "np.savetxt('bus.csv',a, fmt='%.18e', delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph(\"read_dummy.png\", read_bw_req, read_bw_actual, read_bw_limit, graph.nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_table = np.array(pd.read_csv(\"tables/sram.csv\", header=None))\n",
    "a = mem_table[np.where(mem_table[:, 1] == 4)]\n",
    "a = a[np.where(a[:, 2] == 32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute:\n",
    "    name       : MACs\n",
    "        class      : mac\n",
    "        attributes :            \n",
    "        instances       : 256\n",
    "        meshX           : 16\n",
    "        word-bits       : 16\n",
    "\n",
    "    memory: \n",
    "        name       : DRAM\n",
    "        class      : DRAM\n",
    "        attributes :\n",
    "        instances       : 1\n",
    "        word-bits       : 16\n",
    "  \n",
    "    name       : OutputBuffer\n",
    "        class      : SRAM\n",
    "        attributes :\n",
    "        entries         : 1024  # 64 * 16 = 1024\n",
    "        instances       : 1\n",
    "        meshX           : 1\n",
    "        word-bits       : 16\n",
    "        block-size      : 16\n",
    "        read_bandwidth  : 16 # words/cycle\n",
    "        write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : InputBuffer\n",
    "            class      : SRAM\n",
    "            attributes :\n",
    "            entries         : 1024 # 64 * 16 = 1024\n",
    "            instances       : 1\n",
    "            meshX           : 1\n",
    "            word-bits       : 16\n",
    "            block-size      : 16\n",
    "            read_bandwidth  : 16 # words/cycle\n",
    "            write_bandwidth : 16 # words/cycle\n",
    "\n",
    "        name       : PsumRegFile\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 1\n",
    "            instances       : 16\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 16\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "            \n",
    "        name       : WeightBuffer\n",
    "            class      : regfile\n",
    "            attributes :\n",
    "            entries         : 64\n",
    "            instances       : 256\n",
    "            meshX           : 16\n",
    "            word-bits       : 16\n",
    "            cluster-size    : 256\n",
    "            read_bandwidth  : 1  # words/cycle\n",
    "            write_bandwidth : 1  # words/cycle\n",
    "    noc:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_rep = make_graph_from_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stanford_code]",
   "language": "python",
   "name": "conda-env-stanford_code-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
