{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaml Loaders - utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "import yaml\n",
    "from yaml import dump\n",
    "\n",
    "import yamlordereddictloader\n",
    "\n",
    "\n",
    "class accelergy_loader(yaml.SafeLoader):\n",
    "    \"\"\"\n",
    "    Accelergy yaml loader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "\n",
    "        self._root = os.path.split(stream.name)[0]\n",
    "        super(accelergy_loader, self).__init__(stream)\n",
    "\n",
    "def include_constructor(self, node):\n",
    "    \"\"\"\n",
    "    constructor:\n",
    "    parses the !include relative_file_path\n",
    "    loads the file from relative_file_path and insert the values into the original file\n",
    "    \"\"\"\n",
    "    filepath = self.construct_scalar(node)\n",
    "    if filepath[-1] == \",\":\n",
    "        filepath = filepath[:-1]\n",
    "    filename = os.path.join(self._root, filepath)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return yaml.load(f, accelergy_loader)\n",
    "\n",
    "\n",
    "def includedir_constructor(self, node):\n",
    "    \"\"\"\n",
    "    constructor:\n",
    "    parses the !includedir relative_file_path\n",
    "    loads the file from relative_file_path and insert the values into the original file\n",
    "    \"\"\"\n",
    "    filepath = self.construct_scalar(node)\n",
    "    if filepath[-1] == \",\":\n",
    "        filepath = filepath[:-1]\n",
    "    dirname = os.path.join(self._root, filepath)\n",
    "    yamllist = []\n",
    "    for filename in glob.glob(dirname + \"/*.yaml\"):\n",
    "        with open(filename, \"r\") as f:\n",
    "            yamllist.append(yaml.load(f, accelergy_loader))\n",
    "    return yamllist\n",
    "    \n",
    "    \n",
    "yaml.add_constructor(\"!include\", include_constructor, accelergy_loader)\n",
    "yaml.add_constructor(\"!includedir\", includedir_constructor, accelergy_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently I have to write the yaml files for constraints and run the schedulings and write equations of the memory handlers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class accelergy_dumper(yamlordereddictloader.SafeDumper):\n",
    "    \"\"\" Accelergy yaml dumper \"\"\"\n",
    "\n",
    "    def ignore_aliases(self, _data):\n",
    "        return True\n",
    "\n",
    "\n",
    "def create_folder(directory):\n",
    "    \"\"\"\n",
    "    Checks the existence of a directory, if does not exist, create a new one\n",
    "    :param directory: path to directory under concern\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"ERROR: Creating directory. \" + directory)\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    merge_dict = deepcopy(dict1)\n",
    "    merge_dict.update(dict2)\n",
    "    return merge_dict\n",
    "\n",
    "\n",
    "def write_yaml_file(filepath, content):\n",
    "    \"\"\"\n",
    "    if file exists at filepath, overwite the file, if not, create a new file\n",
    "    :param filepath: string that specifies the destination file path\n",
    "    :param content: yaml string that needs to be written to the destination file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "    create_folder(os.path.dirname(filepath))\n",
    "    out_file = open(filepath, \"a\")\n",
    "    out_file.write(dump(content, default_flow_style=False, Dumper=accelergy_dumper))\n",
    "\n",
    "\n",
    "def get_yaml_format(content):\n",
    "    return dump(content, default_flow_style=False, Dumper=accelergy_dumper)\n",
    "\n",
    "\n",
    "def write_file(filepath, content):\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "    create_folder(os.path.dirname(filepath))\n",
    "    out_file = open(filepath, \"a\")\n",
    "    out_file.write(content)\n",
    "\n",
    "\n",
    "def remove_quotes(filepath):\n",
    "    \"\"\"\n",
    "    :param filepath: file that needs to processed\n",
    "    :return: None\n",
    "    removes the quotes inside yaml files\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        new_content = \"\"\n",
    "        f = open(filepath, \"r\")\n",
    "\n",
    "        for line in f:\n",
    "            if \"'\" in line:\n",
    "                line = line.replace(\"'\", \"\")\n",
    "                new_content += line\n",
    "        f.close()\n",
    "        os.remove(filepath)\n",
    "        newf = open(filepath, \"w\")\n",
    "        newf.write(new_content)\n",
    "        newf.close()\n",
    "\n",
    "\n",
    "def ERROR_CLEAN_EXIT(*argv):\n",
    "    msg_str = \"ERROR: \"\n",
    "    for arg in argv:\n",
    "        if type(arg) is not str:\n",
    "            print(msg_str)\n",
    "            print(arg)\n",
    "            msg_str = \"\"\n",
    "        else:\n",
    "            msg_str += arg + \" \"\n",
    "    print(msg_str)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "def WARN(*argv):\n",
    "    msg_str = \"Warn: \"\n",
    "    for arg in argv:\n",
    "        if type(arg) is not str:\n",
    "            print(msg_str)\n",
    "            print(arg)\n",
    "            msg_str = \"\"\n",
    "        else:\n",
    "            msg_str += arg + \" \"\n",
    "    print(msg_str)\n",
    "\n",
    "\n",
    "def INFO(*argv):\n",
    "    msg_str = \"Info: \"\n",
    "    for arg in argv:\n",
    "        if type(arg) is not str:\n",
    "            print(msg_str)\n",
    "            print(arg)\n",
    "            msg_str = \"\"\n",
    "        else:\n",
    "            msg_str += arg + \" \"\n",
    "    print(msg_str)\n",
    "\n",
    "\n",
    "def ASSERT_MSG(expression, msg):\n",
    "    if not expression:\n",
    "        ERROR_CLEAN_EXIT(msg)\n",
    "\n",
    "\n",
    "def add_functions_as_methods(functions):\n",
    "    def decorator(Class):\n",
    "        for function in functions:\n",
    "            setattr(Class, function.__name__, function)\n",
    "        return Class\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def register_function(sequence, function):\n",
    "    sequence.append(function)\n",
    "    return function\n",
    "\n",
    "\n",
    "def remove_brackets(name):\n",
    "    \"\"\"Removes the brackets from a component name in a list\"\"\"\n",
    "    if \"[\" not in name and \"]\" not in name:\n",
    "        return name\n",
    "    if \"[\" in name and \"]\" in name:\n",
    "        start_idx = name.find(\"[\")\n",
    "        end_idx = name.find(\"]\")\n",
    "        name = name[:start_idx] + name[end_idx + 1 :]\n",
    "        name = remove_brackets(name)\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systemstate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemState():\n",
    "    def __init__(self):\n",
    "        self.cc_classes = {}\n",
    "        self.pc_classes = {}\n",
    "        self.arch_spec = None\n",
    "        self.hier_arch_spec = None\n",
    "        self.ccs = {}\n",
    "        self.pcs = {}\n",
    "        self.action_counts = None\n",
    "        self.plug_ins = []\n",
    "        self.ERT = None\n",
    "        self.ART = None\n",
    "        self.parser_version = None\n",
    "        self.flags = {}\n",
    "        self.energy_estimations = None\n",
    "\n",
    "    def set_flag_s(self, flag_name_val_dict):\n",
    "        self.flags.update(flag_name_val_dict)\n",
    "\n",
    "    def set_accelergy_version(self, version):\n",
    "        self.parser_version = version\n",
    "\n",
    "    def set_hier_arch_spec(self, arch_dict):\n",
    "        ASSERT_MSG(self.hier_arch_spec is None, 'interpreted input arch is set')\n",
    "        self.hier_arch_spec = arch_dict\n",
    "\n",
    "    def set_arch_spec(self, arch_spec):\n",
    "        ASSERT_MSG(self.arch_spec is None, 'architecture spec is already set')\n",
    "        self.arch_spec = arch_spec\n",
    "\n",
    "    def add_cc_class(self, cc_class):\n",
    "        cc_class_name = cc_class.get_name()\n",
    "        ASSERT_MSG(cc_class_name not in self.cc_classes, '%s compound class is already added'%(cc_class_name))\n",
    "        self.cc_classes[cc_class_name] = cc_class\n",
    "\n",
    "    def add_pc_class(self, pc_class):\n",
    "        pc_class_name = pc_class.get_name()\n",
    "        ASSERT_MSG(pc_class_name not in self.pc_classes, '%s primitive class is already added'%(pc_class_name))\n",
    "        self.pc_classes[pc_class_name] = pc_class\n",
    "\n",
    "    def add_cc(self, cc):\n",
    "        cc_name = cc.get_name()\n",
    "        ASSERT_MSG(cc_name not in self.ccs, '%s compound component is already added'%(cc_name))\n",
    "        self.ccs[cc_name] = cc\n",
    "\n",
    "    def add_pc(self, pc):\n",
    "        pc_name = pc.get_name()\n",
    "        ASSERT_MSG(pc_name not in self.ccs, '%s compound component is already added'%(pc_name))\n",
    "        self.pcs[pc_name] = pc\n",
    "\n",
    "    def add_plug_ins(self, plug_ins):\n",
    "        ASSERT_MSG(type(plug_ins) is list, 'plug in objects need to be passed in as a list')\n",
    "        self.plug_ins = plug_ins\n",
    "\n",
    "    def set_ERT(self, ERT):\n",
    "        self.ERT = ERT\n",
    "\n",
    "    def set_ART(self, ART):\n",
    "        self.ART = ART\n",
    "\n",
    "    def set_action_counts(self, action_counts):\n",
    "        self.action_counts = action_counts\n",
    "\n",
    "    def set_energy_estimations(self, energy_estimations):\n",
    "        self.energy_estimations = energy_estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class Action(object):\n",
    "    \"\"\"Action class\"\"\"\n",
    "    def __init__(self, action_def_dict):\n",
    "\n",
    "        self.action_def_dict = action_def_dict\n",
    "        self.name = action_def_dict['name']\n",
    "\n",
    "        self._arguments = None\n",
    "        self.set_arguments(action_def_dict)\n",
    "\n",
    "        # compound action contains action definitions in terms of subcomponents\n",
    "        self._subcomponents = None\n",
    "        self.set_subcomponents(action_def_dict)\n",
    "\n",
    "        # repeat has the same meaning as action share\n",
    "        if 'repeat' in action_def_dict:\n",
    "            self._action_share = action_def_dict['repeat']\n",
    "        elif 'action_share' in action_def_dict:\n",
    "            self._action_share = action_def_dict['action_share']\n",
    "        else:\n",
    "            self._action_share = None\n",
    "        # only compound actions will later set this property\n",
    "        self._primitive_list = None\n",
    "\n",
    "    def set_arguments(self, action_def_dict):\n",
    "        if 'arguments' in action_def_dict:\n",
    "            self._arguments = {}\n",
    "            for arg_name, arg_range in action_def_dict['arguments'].items():\n",
    "                self._arguments[arg_name] = arg_range\n",
    "\n",
    "    def set_subcomponents(self, action_def_dict):\n",
    "        if 'subcomponents' in action_def_dict:\n",
    "            self._subcomponents = {}\n",
    "            for subcomp in action_def_dict['subcomponents']:\n",
    "                subcompActions = []\n",
    "                for subcompAction in subcomp['actions']:\n",
    "                    subcompActions.append(Action(subcompAction))\n",
    "                self._subcomponents[subcomp['name']] = subcompActions\n",
    "\n",
    "    def set_primitive_list(self, primitive_list):\n",
    "        self._primitive_list = primitive_list\n",
    "\n",
    "    def set_action_share(self, new_action_share):\n",
    "        \"\"\"update the parsed repeat/action_share value\"\"\"\n",
    "        self._action_share = new_action_share\n",
    "\n",
    "    def set_argument(self, new_arg_dict):\n",
    "        \"\"\" update one or more argument name-val pairs\"\"\"\n",
    "        self._arguments.update(new_arg_dict)\n",
    "\n",
    "    def set_subcomps(self, defined_subcomps):\n",
    "        self._subcomponents = defined_subcomps\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_action_share(self):\n",
    "        return self._action_share\n",
    "\n",
    "    def get_arguments(self):\n",
    "        return self._arguments\n",
    "\n",
    "    def get_argument(self, arg_name):\n",
    "        return self._arguments[arg_name]\n",
    "\n",
    "    def get_subcomps(self):\n",
    "        ASSERT_MSG(self._subcomponents is not None, 'action does not have defined subcomponents')\n",
    "        return self._subcomponents\n",
    "\n",
    "    def get_primitive_list(self):\n",
    "        return self._primitive_list\n",
    "\n",
    "    def get_action_info_as_dict(self):\n",
    "        action_dict = {'name': self.name}\n",
    "        if self._subcomponents is not None: action_dict['subcomponents'] = self._subcomponents\n",
    "        if self._arguments is not None: action_dict['arguments'] = self._arguments\n",
    "        return action_dict\n",
    "\n",
    "    def get_subactions(self, subcompName):\n",
    "        ASSERT_MSG(self._subcomponents is not None and subcompName in self._subcomponents,\n",
    "                   'cannot find subactions associated with %s for action %s'%(subcompName, self.name))\n",
    "        return self._subcomponents[subcompName]\n",
    "\n",
    "    def get_arg_val(self, argName):\n",
    "        ASSERT_MSG(argName in self._arguments, 'argument name %s is not associated with action %s'%(argName, self.name))\n",
    "        return self._arguments[argName]\n",
    "\n",
    "    def set_arg(self, arg_dict):\n",
    "        self._arguments.update(arg_dict)\n",
    "\n",
    "    def flatten_action_args_into_list(self, mappingDict):\n",
    "        \"\"\" flatten an action into a list representing all possible argument value combinations\"\"\"\n",
    "\n",
    "        args = self.get_arguments()\n",
    "        if args is None:\n",
    "            return [self]  # no arguments, no need to flatten\n",
    "\n",
    "        # an action needs to be flattened into a list of actions with the same action name but different arg vals\n",
    "        total_entries = 1\n",
    "        argument_range_record = {}\n",
    "        for arg_name, arg_range in args.items():\n",
    "            ASSERT_MSG(type(arg_range) is str, '%s: argument value for action %s is not string, cannot parse range'%(arg_name,self.name))\n",
    "            ASSERT_MSG('..' in arg_range, '%s: argument value for action %s is not range, cannot parse range'%(arg_name,self.name))\n",
    "            new_arg_range = Action.map_arg_range_bounds(arg_range, mappingDict)[0]\n",
    "            startIdx, endIdx = Action.parse_arg_range(new_arg_range)\n",
    "            total_entries *= (endIdx - startIdx + 1)\n",
    "            argument_range_record[arg_name] = (startIdx, endIdx)\n",
    "\n",
    "        action_list = []\n",
    "        for entry_idx in range(total_entries):\n",
    "            offset = 1\n",
    "            arg_def = {}\n",
    "            for arg_name, range_record in argument_range_record.items():\n",
    "                arg_range = range_record[1] - range_record[0] + 1\n",
    "                arg_def[arg_name] = (entry_idx // offset) % arg_range + range_record[0]\n",
    "                offset *= arg_range\n",
    "            subcomp_list = []\n",
    "            new_action = deepcopy(self); new_action._arguments = arg_def\n",
    "            action_list.append(new_action)\n",
    "        return action_list\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_arg_range(arg_range):\n",
    "        \"\"\" Parse the start index and end index for an argument range\"\"\"\n",
    "        if type(arg_range) is not str or '..' not in arg_range:\n",
    "            ERROR_CLEAN_EXIT('cannot parse the argument range specification: ', arg_range)\n",
    "        split_sub_string = arg_range.split('..')\n",
    "        start_idx = int(split_sub_string[0])\n",
    "        end_idx = int(split_sub_string[1])\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_action_to_list_with_arg_values(action_info):\n",
    "        \"\"\"flatten actions with arguments into list\n",
    "           1) input action is fully defined with numerical ranges\n",
    "           2) output list contains a list of actions each with a possible set of argument values\n",
    "        \"\"\"\n",
    "\n",
    "        action_name = action_info['name']\n",
    "        total_entries = 1\n",
    "        argument_range_record = {}\n",
    "        for argument_name, argument_range in action_info['arguments'].items():\n",
    "            start_idx, end_idx = Action.parse_arg_range(argument_range)\n",
    "            total_entries *= (end_idx - start_idx + 1)\n",
    "            argument_range_record[argument_name] = (start_idx, end_idx)\n",
    "        expanded_list = [{'name': action_name, 'arguments':{}} for i in range(total_entries)]\n",
    "        # construct list of dictionaries that contain all the possible combination of argument values\n",
    "        for entry_idx in range(total_entries):\n",
    "            offset = 1\n",
    "            for argument_name, range_record in argument_range_record.items():\n",
    "                arg_range = range_record[1] - range_record[0] + 1\n",
    "                expanded_list[entry_idx]['arguments'][argument_name] = \\\n",
    "                    (entry_idx // offset) % arg_range + range_record[0]\n",
    "                offset *= arg_range\n",
    "        return expanded_list\n",
    "\n",
    "    @staticmethod\n",
    "    def map_arg_range_bounds(arg_range_str, attributes_dict):\n",
    "        \"\"\"\n",
    "        arguments for actions might have ranges that are specified in terms of it attributes\n",
    "        parses the argument ranges in the format int/str..int/str, where str can be arithmetic operation\n",
    "\n",
    "        :param arg_range_str: string that decribes the range of a compound action\n",
    "        :param attributes_dict: attribute name-value pairs of the compound component\n",
    "        :return: parsed argument range, whether there was binding\n",
    "        \"\"\"\n",
    "        split_sub_string = arg_range_str.split('..')\n",
    "        detect_arg_range_binding = False\n",
    "        # process the start index\n",
    "        try:\n",
    "            start_idx = int(split_sub_string[0])\n",
    "        except ValueError:\n",
    "            op_type, op1, op2 = parse_expression_for_arithmetic(split_sub_string[0], attributes_dict)\n",
    "            if op_type is not None:\n",
    "                start_idx = process_arithmetic(op1, op2, op_type)\n",
    "            else:\n",
    "                if split_sub_string[0] not in attributes_dict:\n",
    "                    ERROR_CLEAN_EXIT('cannot find mapping from', arg_range_str, 'to', attributes_dict)\n",
    "                start_idx = attributes_dict[split_sub_string[0]]\n",
    "            detect_arg_range_binding = True\n",
    "\n",
    "        # process the end index\n",
    "        try:\n",
    "            end_idx = int(split_sub_string[1])\n",
    "        except ValueError:\n",
    "            op_type, op1, op2 = parse_expression_for_arithmetic(split_sub_string[1], attributes_dict)\n",
    "            if op_type is not None:\n",
    "                end_idx = process_arithmetic(op1, op2, op_type)\n",
    "            else:\n",
    "                if split_sub_string[1] not in attributes_dict:\n",
    "                    ERROR_CLEAN_EXIT('cannot find mapping from', arg_range_str, 'to', attributes_dict)\n",
    "                end_idx = attributes_dict[split_sub_string[1]]\n",
    "            detect_arg_range_binding = True\n",
    "\n",
    "        new_arg_range_str = str(start_idx) + '..' + str(end_idx)\n",
    "\n",
    "        return new_arg_range_str, detect_arg_range_binding\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_arg_range(arg_range):\n",
    "        if type(arg_range) is not str or '..' not in arg_range:\n",
    "            ERROR_CLEAN_EXIT('cannot parse the argument range specification: ', arg_range)\n",
    "        split_sub_string = arg_range.split('..')\n",
    "        start_idx = int(split_sub_string[0])\n",
    "        end_idx   = int(split_sub_string[1])\n",
    "        return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subcomponent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subcomponent:\n",
    "    def __init__(self, comp_def):\n",
    "        self.dict_reprsentation = comp_def\n",
    "        if 'attributes' not in self.dict_reprsentation:\n",
    "            self.dict_reprsentation['attributes'] = {}\n",
    "\n",
    "    def set_name(self, name):\n",
    "        self.dict_reprsentation['name'] = name\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.dict_reprsentation['name']\n",
    "\n",
    "    def get_class_name(self):\n",
    "        return self.dict_reprsentation['class']\n",
    "\n",
    "    def get_attributes(self):\n",
    "        return self.dict_reprsentation['attributes']\n",
    "\n",
    "\n",
    "    def get_area_share(self):\n",
    "        return self.dict_reprsentation['area_share']\n",
    "\n",
    "    def add_new_attr(self, attr_dict):\n",
    "        self.dict_reprsentation['attributes'].update(attr_dict)\n",
    "\n",
    "    def set_area_share(self, area_share):\n",
    "        self.dict_reprsentation['area_share'] = area_share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "componenclass.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ComponentClass:\n",
    "    def __init__(self, class_dict):\n",
    "        self._name = class_dict['name']\n",
    "        self._default_attributes = deepcopy(class_dict['attributes'])\n",
    "\n",
    "        self._actions = {}\n",
    "        self.set_actions(class_dict['actions'])\n",
    "\n",
    "        self._subcomponents = None\n",
    "        if 'subcomponents' in class_dict:\n",
    "            self._subcomponents = {}\n",
    "            self.type = 'compound'\n",
    "            for scomp in class_dict['subcomponents']: self._subcomponents[scomp['name']] = Subcomponent(scomp)\n",
    "        else:\n",
    "            self.type = 'primitive'\n",
    "        self._primitive_type = class_dict['primitive_type'] if 'primitive_type' in class_dict else None\n",
    "\n",
    "    def set_actions(self, action_list):\n",
    "        ASSERT_MSG(type(action_list) is list,\n",
    "                   '%s class description must specify its actions in list format'%(self.get_name()))\n",
    "        for action in action_list:\n",
    "            ASSERT_MSG('name' in action, '%s class actions must contain \"name\" keys'%(self.get_name()))\n",
    "            self._actions[action['name']] = Action(action)\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Getters\n",
    "    #-----------------------------------------------------\n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def get_default_attr_to_apply(self, obj_attr_name_list):\n",
    "        attr_to_be_applied = OrderedDict()\n",
    "        for attr_name, attr_val in self._get_default_attrs().items():\n",
    "            if attr_val == \"must_specify\":\n",
    "                ASSERT_MSG(attr_name in obj_attr_name_list,\n",
    "                           \"attributes %s for compound class %s must be specified in architecture description\"\n",
    "                           %(attr_name, self.get_name()))\n",
    "            if attr_name not in obj_attr_name_list:\n",
    "                attr_to_be_applied[attr_name] = attr_val\n",
    "        return attr_to_be_applied\n",
    "\n",
    "    def _get_default_attrs(self):\n",
    "        return self._default_attributes\n",
    "\n",
    "    def _get_attr_name_list(self):\n",
    "        return list(self._default_attributes.keys())\n",
    "\n",
    "    def _get_attr_default_val(self, attrName):\n",
    "        ASSERT_MSG(attrName in self._default_attributes, 'Attribute %s cannot be found in class %s'%(attrName, self.get_name()))\n",
    "        return self._default_attributes[attrName]\n",
    "\n",
    "    def get_action_name_list(self):\n",
    "        return list(self._actions.keys())\n",
    "\n",
    "    def get_action(self, actionName):\n",
    "        ASSERT_MSG(actionName in self._actions, '%s does not exist in class %s'%(actionName, self.get_name()))\n",
    "        return self._actions[actionName]\n",
    "\n",
    "    def get_subcomponents_as_dict(self):\n",
    "        ASSERT_MSG(self._subcomponents is not None, 'component class %s does not have subcomponents' % self.get_name())\n",
    "        return self._subcomponents\n",
    "\n",
    "    def get_primitive_type(self):\n",
    "        return self._primitive_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "import yaml\n",
    "from yaml import dump\n",
    "\n",
    "import yamlordereddictloader\n",
    "\n",
    "\n",
    "class constraintyaml_loader(yaml.SafeLoader):\n",
    "    \"\"\"\n",
    "    Constraints yaml loader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "\n",
    "        self._root = os.path.split(stream.name)[0]\n",
    "        super(constraintyaml_loader, self).__init__(stream)\n",
    "\n",
    "def include_constructor(self, node):\n",
    "    \"\"\"\n",
    "    constructor:\n",
    "    parses the !include relative_file_path\n",
    "    loads the file from relative_file_path and insert the values into the original file\n",
    "    \"\"\"\n",
    "    filepath = self.construct_scalar(node)\n",
    "    if filepath[-1] == \",\":\n",
    "        filepath = filepath[:-1]\n",
    "    filename = os.path.join(self._root, filepath)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return yaml.load(f, constraintyaml_loader)\n",
    "yaml.add_constructor(\"!include\", include_constructor, constraintyaml_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action_counts:\n",
    "  local:\n",
    "  - action_counts:\n",
    "    - counts: 31306\n",
    "      name: idle\n",
    "    - arguments:\n",
    "        address_delta: 2\n",
    "        data_delta: 1\n",
    "      counts: 1152\n",
    "      name: fill\n",
    "    - arguments:\n",
    "        address_delta: 0\n",
    "        data_delta: 0\n",
    "      counts: 1150\n",
    "      name: read\n",
    "    - arguments:\n",
    "        address_delta: 2\n",
    "        data_delta: 1\n",
    "      counts: 2\n",
    "      name: read\n",
    "    name: eyeriss_like.weights_glb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  subtree:\n",
    "    - name: eyeriss_like\n",
    "      attributes:\n",
    "        technology: 40nm\n",
    "      local:\n",
    "        - name: weights_glb\n",
    "          class: smartbuffer_SRAM\n",
    "          attributes:\n",
    "            memory_width: 64\n",
    "            memory_depth: 1024\n",
    "            n_banks: 2\n",
    "        - name: shared_glb\n",
    "          class: smartbuffer_SRAM\n",
    "          attributes:\n",
    "            memory_width: 64\n",
    "            n_banks: 25\n",
    "            bank_depth: 512\n",
    "            memory_depth: bank_depth * n_banks\n",
    "            n_buffets: 2\n",
    "            update_fifo_depth: 2\n",
    "        - name: ifmap_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 16\n",
    "            col_id_width: 5\n",
    "        - name: weights_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 64\n",
    "        - name: psum_write_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 64\n",
    "        - name: psum_read_NoC\n",
    "          class: XY_NoC\n",
    "          attributes:\n",
    "            datawidth: 64\n",
    "            Y_X_wire_avg_length: 4mm\n",
    "      subtree:\n",
    "      - name: PE[0..167]\n",
    "        attributes:\n",
    "          memory_width: 16\n",
    "        local:\n",
    "          - name: ifmap_spad\n",
    "            class: smartbuffer_RF\n",
    "            attributes:\n",
    "              memory_depth: 12\n",
    "              buffet_manager_depth: 0\n",
    "          - name: weights_spad\n",
    "            class: smartbuffer_SRAM\n",
    "            attributes:\n",
    "              memory_depth: 224\n",
    "              buffet_manager_depth: 0\n",
    "          - name: psum_spad\n",
    "            class: smartbuffer_RF\n",
    "            attributes:\n",
    "              memory_depth: 24\n",
    "              buffet_manager_depth: 24\n",
    "              update_fifo_depth: 2\n",
    "          - name: mac\n",
    "            class: intmac\n",
    "            attributes:\n",
    "              datawidth: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hardware_primitives.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multipliers \n",
    "\n",
    "---\n",
    "\n",
    "Sytolic Array (of Standard Sizes)\n",
    "\n",
    "---\n",
    "SRAM Arrays of Some Standard Sizes (A dictionary with different Bank Depth and Number of Banks)\n",
    "\n",
    "---\n",
    "\n",
    "ORION NOC (XY_NOC)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def model(x, p):\n",
    "    return x ** (2 * p + 1) / (1 + x ** (2 * p))\n",
    "\n",
    "with plt.style.context(['science','no-latex']):\n",
    "    x = np.linspace(0.75, 1.25, 201)\n",
    "    fig, ax = plt.subplots()\n",
    "    for p in [10, 15, 20, 30, 50, 100]:\n",
    "        ax.plot(x, model(x, p), label=p)\n",
    "    ax.legend(title='Order')\n",
    "    ax.set(xlabel='Voltage (mV)')\n",
    "    #     ax.set(ylabel='Current ($\\mu$A)')\n",
    "    ax.autoscale(tight=True)\n",
    "    plt.show()\n",
    "#     fig.savefig('figures/fig1.pdf')\n",
    "#     fig.savefig('figures/fig1.jpg', dpi=300)\n",
    "\n",
    "# with plt.style.context(['science', 'ieee']):\n",
    "#     x = np.linspace(0.75, 1.25, 201)\n",
    "#     fig, ax = plt.subplots()\n",
    "#     for p in [10, 20, 50]:\n",
    "#         ax.plot(x, model(x, p), label=p)\n",
    "#     ax.legend(title='Order')\n",
    "#     ax.set(xlabel='Voltage (mV)')\n",
    "#     ax.set(ylabel='Current ($\\mu$A)')\n",
    "#     ax.autoscale(tight=True)\n",
    "# #     fig.savefig('figures/fig2.pdf')\n",
    "# #     fig.savefig('figures/fig2.jpg', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "with plt.style.context(['science', 'high-vis','no-latex']):\n",
    "    x = np.linspace(0.75, 1.25, 201)\n",
    "    fig, ax = plt.subplots()\n",
    "    for p in [10, 15, 20, 30, 50, 100]:\n",
    "        ax.plot(x, model(x, p), label=p)\n",
    "    ax.legend(title='Order')\n",
    "    ax.set(xlabel='Voltage (mV)')\n",
    "#     ax.set(ylabel='Current ($\\mu$A)')\n",
    "    ax.autoscale(tight=True)\n",
    "    plt.show()\n",
    "#     fig.savefig('figures/fig4.pdf')\n",
    "#     fig.savefig('figures/fig4.jpg', dpi=300)\n",
    "\n",
    "with plt.style.context(['dark_background', 'high-vis']):\n",
    "    x = np.linspace(0.75, 1.25, 201)\n",
    "    fig, ax = plt.subplots()\n",
    "    for p in [10, 15, 20, 30, 50, 100]:\n",
    "        ax.plot(x, model(x, p), label=p)\n",
    "    ax.legend(title='Order')\n",
    "    ax.set(xlabel='Voltage (mV)')\n",
    "#     ax.set(ylabel='Current ($\\mu$A)')\n",
    "    ax.autoscale(tight=True)\n",
    "    plt.show()\n",
    "# #     fig.savefig('figures/fig5.pdf')\n",
    "# #     fig.savefig('figures/fig5.jpg', dpi=300)\n",
    "\n",
    "# # Plot different color cycles \n",
    "\n",
    "with plt.style.context(['science', 'bright','no-latex']):\n",
    "    x = np.linspace(0.75, 1.25, 201)\n",
    "    fig, ax = plt.subplots()\n",
    "    for p in [5, 10, 15, 20, 30, 50, 100]:\n",
    "        ax.plot(x, model(x, p), label=p)\n",
    "    ax.legend(title='Order')\n",
    "    ax.set(xlabel='Voltage (mV)')\n",
    "#     ax.set(ylabel='Current ($\\mu$A)')\n",
    "    ax.autoscale(tight=True)\n",
    "#     fig.savefig('figures/fig6.pdf')\n",
    "#     fig.savefig('figures/fig6.jpg', dpi=300)\n",
    "\n",
    "with plt.style.context(['science', 'vibrant','no-latex']):\n",
    "    x = np.linspace(0.75, 1.25, 201)\n",
    "    fig, ax = plt.subplots()\n",
    "    for p in [5, 10, 15, 20, 30, 50, 100]:\n",
    "        ax.plot(x, model(x, p), label=p)\n",
    "    ax.legend(title='Order')\n",
    "    ax.set(xlabel='Voltage (mV)')\n",
    "#     ax.set(ylabel='Current ($\\mu$A)')\n",
    "    ax.autoscale(tight=True)\n",
    "#     fig.savefig('figures/fig7.pdf')\n",
    "#     fig.savefig('figures/fig7.jpg', dpi=300)\n",
    "\n",
    "with plt.style.context(['science', 'muted','no-latex']):\n",
    "    x = np.linspace(0.75, 1.25, 201)\n",
    "    fig, ax = plt.subplots()\n",
    "    for p in [5, 7, 10, 15, 20, 30, 38, 50, 100, 500]:\n",
    "        ax.plot(x, model(x, p), label=p)\n",
    "    ax.legend(title='Order', fontsize=7)\n",
    "    ax.set(xlabel='Voltage (mV)')\n",
    "#     ax.set(ylabel='Current ($\\mu$A)')\n",
    "    ax.autoscale(tight=True)\n",
    "#     fig.savefig('figures/fig8.pdf')\n",
    "# #     fig.savefig('figures/fig8.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import matplotlib as mpl\n",
    "# mpl.rcParams['text.usetex'] = False?\n",
    "# mpl.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,6))\n",
    "index = np.arange(n_groups)\n",
    "print(index)\n",
    "bar_width = 0.05\n",
    "ax.set_yscale('linear')\n",
    "# opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "index = np.arange(n_groups)\n",
    "# print(index)\n",
    "bar_width = 0.1\n",
    "# ax.set_yscale('log')\n",
    "opacity = 0.1\n",
    "error_config = {'ecolor': '0.3'}\n",
    "#\n",
    "\n",
    "n_groups = 15\n",
    "rects1 = ax.bar(index+bar_width,read_dram, bar_width, color='blue', error_kw=error_config,label='DRAM')\n",
    "rects2 = ax.bar(index + 2.2*bar_width,read_nvm, bar_width, color='green', error_kw=error_config,label='NVM')\n",
    "\n",
    "# ax.set_ylabel('SpeedUp (GraphMat=1)',fontsize=14,fontweight='bold')\n",
    "ax.set_xticks(index + bar_width*2)\n",
    "ax.set_xticklabels(('<1,.256,4>', '<2,.128,2>', '<5,.05,0.8>','<10,.025,0.4>','<20,.012,0.2>','<1,.256,4>', '<2,.128,2>', '<5,.05,0.8>','<10,.025,0.4>','<20,.012,0.2>','<1,.256,4>', '<2,.128,2>', '<5,.05,0.8>','<10,.025,0.4>','<20,.012,0.2>'))\n",
    "ax.set_ylabel('Average Read Size \\n (in KB)',fontsize=20,fontweight='bold')\n",
    "ax.legend(fontsize=20)\n",
    "ax.set_xlabel('Zipf\\'s Parameter =0.9,                                                         Zipf\\'s Parameter =0.8,                                                                                        Zipf\\'s Parameter =0.7',fontsize=20,fontweight='bold')\n",
    "fig.tight_layout()\n",
    "plt.rc('xtick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.savefig('read_avg.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import matplotlib as mpl\n",
    "# mpl.rcParams['text.usetex'] = False?\n",
    "# mpl.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n",
    "\n",
    "n_groups = 15\n",
    "fig, ax = plt.subplots(figsize=(30,6))\n",
    "index = np.arange(n_groups)\n",
    "print(index)\n",
    "bar_width = 0.05\n",
    "ax.set_yscale('linear')\n",
    "# opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "index = np.arange(n_groups)\n",
    "# print(index)\n",
    "bar_width = 0.1\n",
    "# ax.set_yscale('log')\n",
    "opacity = 0.1\n",
    "error_config = {'ecolor': '0.3'}\n",
    "#\n",
    "\n",
    "\n",
    "rects1 = ax.bar(index+bar_width,h0_writes[:,0], bar_width, color='red', error_kw=error_config,label='DRAM')+ax.bar(index+bar_width, h0_writes[:,1], bar_width, bottom=h0_writes[:,0], color='gold', error_kw=error_config,label='NVM')+ax.bar(index+bar_width, h0_writes[:,2], bar_width,bottom=h0_writes[:,0]+h0_writes[:,1],color='aqua', error_kw=error_config,label='Misses')\n",
    "rects1 = ax.bar(index+bar_width, h0_writes[:,0]+h0_writes[:,1]+h0_writes[:,2], bar_width, color='none', error_kw=error_config,label='h=2',hatch='o')\n",
    "rects2 = axpgf_with_rc_fonts = {\"pgf.texsystem\": \"pdflatex\"}\n",
    "matplotlib.rcParams.update(pgf_with_rc_fonts).bar(index + 2.2*bar_width,h1_writes[:,0]  , bar_width, color='red', error_kw=error_config)+ax.bar(index + 2.2*bar_width, h1_writes[:,1] , bar_width,bottom=h1_writes[:,0], color='gold', error_kw=error_config)+ax.bar(index+2.2*bar_width,h1_writes[:,2] , bar_width,bottom=h1_writes[:,0]+h1_writes[:,1],color='aqua', error_kw=error_config)\n",
    "rects2 = ax.bar(index + 2.2*bar_width, h1_writes[:,0]+h1_writes[:,1]+h1_writes[:,2], bar_width, color='none',hatch='x', error_kw=error_config,label='h=1')\n",
    "rects3 = ax.bar(index + 3.4*bar_width, h2_writes[:,0] , bar_width, color='red',hatch='/',error_kw=error_config)+ax.bar(index + 3.4*bar_width, h2_writes[:,1] , bar_width, bottom=h2_writes[:,0], color='gold',hatch='/',error_kw=error_config)+ax.bar(index+3.4*bar_width, h2_writes[:,2], bar_width,bottom= h2_writes[:,0]+h2_writes[:,1],color='aqua', error_kw=error_config)\n",
    "rects3 = ax.bar(index + 3.4*bar_width, h2_writes[:,0]+h2_writes[:,1]+h2_writes[:,2], bar_width, color='none',hatch='/',error_kw=error_config,label='h=0')\n",
    "\n",
    "# ax.set_ylabel('SpeedUp (GraphMat=1)',fontsize=14,fontweight='bold')\n",
    "ax.set_xticks(index + bar_width*2)\n",
    "ax.set_xticklabels(('<1,.256,4>', '<2,.128,2>', '<5,.05,0.8>','<10,.025,0.4>','<20,.012,0.2>','<1,.256,4>', '<2,.128,2>', '<5,.05,0.8>','<10,.025,0.4>','<20,.012,0.2>','<1,.256,4>', '<2,.128,2>', '<5,.05,0.8>','<10,.025,0.4>','<20,.012,0.2>'))\n",
    "ax.set_ylabel('Fraction of Number of Writes  \\n in DRAM/NVM',fontsize=20,fontweight='bold')\n",
    "ax.set_xlabel('Zipf\\'s Parameter =0.9,                                                         Zipf\\'s Parameter =0.8,                                                                                        Zipf\\'s Parameter =0.7',fontsize=20,fontweight='bold')\n",
    "ax.legend(fontsize=20)\n",
    "# ax.set_ylim( (pow(10,-2),pow(10,3)))\n",
    "fig.tight_layout()\n",
    "plt.rc('xtick', labelsize=16)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.savefig('data_writes.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture:\n",
    "# ============================================================\n",
    "# Architecture Description\n",
    "# ============================================================\n",
    "  version: 0.3\n",
    "  subtree:\n",
    "    - name: simple              # top-level name key has the value as your design name, there can be only one component is the top level list, which is the design\n",
    "      attributes:               # shared attributes for all subcomponents in design\n",
    "        technology: 65nm\n",
    "        voltage: 1\n",
    "        num_glbs: 5\n",
    "      local:                     # list of nodes under design, this design only has one node\n",
    "        - name: glb[0..num_glbs-1]\n",
    "          class: shared_SRAM      # use the primitive component type specification of the compound component class\n",
    "          attributes:            # hardware attributes that are different from default values\n",
    "            nbanks: 1\n",
    "            bank_depth: 224\n",
    "            bank_width: 16\n",
    "            n_rd_ports: 2        # number of architectural read ports (same as default value, can be omitted)\n",
    "            n_wr_ports: 2        # number of architectural write ports (same as default value, can be omitted)\n",
    "            n_bank_rd_ports: 1   # (same as default value, can be omitted)\n",
    "            n_bank_wr_ports: 1   # (same as default value, can be omitted)\n",
    "            n_bank_rdwr_ports: n_bank_rd_ports + n_bank_wr_ports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data = OrderedDict([\n",
    "    ('key1', 'val1'),\n",
    "    ('key2', OrderedDict([('key21', 'val21'), ('key22', 'val22')]))\n",
    "])\n",
    "yaml.dump(\n",
    "    data,\n",
    "    open('myfile.yml', 'w'),\n",
    "    Dumper=yamlordereddictloader.Dumper,\n",
    "    default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-8-bba73270b77b>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-bba73270b77b>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class defconstraints():\n",
    "    def __init__(self, yamlfile):\n",
    "        self.yamlfile = yamlfile \n",
    "\n",
    "        self.parse_and_set_props(yamlfile)\n",
    "    def parse_and_set_props():\n",
    "        \"\"\"\n",
    "        Constraints of Hardware Primitives \n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import yamlordereddictloader\n",
    "from torchvision import models\n",
    "from yaml import dump\n",
    "\n",
    "from ir.handlers import handlers\n",
    "from ir.trace import trace\n",
    "\n",
    "from logger import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just preload the graph for fast experiments \n",
    "# Create Graph\n",
    "\n",
    "for name, model in models.__dict__.items():\n",
    "    if not name.islower() or name.startswith(\"__\") or not callable(model):\n",
    "        continue\n",
    "    model = model().eval()\n",
    "    if \"resnet50\" in name:\n",
    "        inputs = torch.randn(1, 3, 299, 299)\n",
    "        graph = trace(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_node_characterstics(graph):\n",
    "    for node in graph.nodes:\n",
    "        for operators, func in handlers:\n",
    "            if isinstance(operators, str):\n",
    "                operators = [operators]\n",
    "            if node.operator in operators:\n",
    "                if func is not None:\n",
    "                    print(node.operator)\n",
    "                    node.compute_expense, node.read_access, node.write_access = func(\n",
    "                        node\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class primitve():    \n",
    "#     systolic_array \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduling:\n",
    "    \n",
    "    def __init__(self, primitive=\"primtive.yaml\", constraintfiles=[\"max.yaml\",\"min.yaml\"], hwfile = \"default.yaml\",opts=None):\n",
    "        total_cycles = 0\n",
    "        base_dir = \"configs/\"\n",
    "        self.maxval=yaml.load(open(base_dir+constraintfiles[0]), Loader=yamlordereddictloader.Loader)\n",
    "        self.minval = yaml.load(open(base_dir+constraintfiles[1]),Loader=yamlordereddictloader.Loader)\n",
    "        self.primitives = yaml.load(open(base_dir+primitive_file), Loader=yamlordereddictloader.Loader)   \n",
    "    \n",
    "    def run(self, graph):\n",
    "        \"\"\"\n",
    "        Scheduling works in the following way :\n",
    "        1. Start with a given/random Hardware point -> Nodes of the graph are scheduled (prefetching)\n",
    "        2. Do the Scheduling with that Point -> Mapping stops here -> Further evaluation is done using accelergy \n",
    "        (with values taken from ERT/ART) -> If values not available -> Use plugins for generating these values \n",
    "        3. Log bottlenecks and work on a different Hardware point -> do this till some realistically\n",
    "        max, min values are not violated -> Values/Analyses for a different/unavailable point will require full \n",
    "        integration of plugins -> Currently using a table at 40nm.\n",
    "        \"\"\" \n",
    "        for node in graph.nodes:\n",
    "            read_access, write_access, compute_expense = node.get_stats()\n",
    "            execution_logger.info(\"Execution Node %d\", node)\n",
    "            if (bw_req < self.maxval['']):\n",
    "                execution_logger.info(\"Node has Memory Bottleneck %b\", True )\n",
    "                step_cycles = get_number_cycles(node, True)\n",
    "            else:\n",
    "                step_cycles = get_number_cycles(node,False)\n",
    "                    \n",
    "            stats_logger.info(\"%d %d %d %d %d\", node, step_cycles, read_access, write_access, bw_req)\n",
    "            total_cycles += step_cycles\n",
    "        \n",
    "        stats_logger.info(\"No of cycles %d - \",total_cycles)\n",
    "\n",
    "    def get_number_cycles(self, membtlnck, *args, **kwargs):\n",
    "        if membtlnck != True:\n",
    "            return (node.computational_expense // maxval[self.compute_primitive][\"latency\"])\n",
    "        else:\n",
    "            return total_memory_accesses // memory_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runner():\n",
    "    \"\"\"\n",
    "    Runs the Input Graph\n",
    "    \"\"\"\n",
    "#     add_plug_ins = [\n",
    "#         \"accelergy_ART\",\n",
    "#         \"accelergy_ERT\",\n",
    "#         \"cacti_memory\",\n",
    "#         \"orion_noc\",\n",
    "#         \"aladdin_compute\"]\n",
    "    \n",
    "#     plugins.instatiate_plugins(add_plug_ins)\n",
    "\n",
    "\n",
    "    # Set Node Characterstics\n",
    "    set_node_characterstics(graph)\n",
    "    \n",
    "    \n",
    "    # For the Inverse Problem\n",
    "    stats_logger = create_logger(\"logs/stats.txt\")\n",
    "    scheduler = Scheduling(graph)\n",
    "    scheduler.set_primitives(\"systolic array\",\"\")\n",
    "    scheduler.run(graph)\n",
    "    \n",
    "    # For the Forward Problem\n",
    "    \n",
    "    # executer = Mapper(opts, hwdesc)\n",
    "    # logger.save_statistics(area)\n",
    "    # logger.save_statistics(energy)\n",
    "    # logger.save_statistics(timing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::_convolution\n",
      "aten::batch_norm\n",
      "aten::adaptive_avg_pool2d\n",
      "aten::addmm\n",
      "OrderedDict([('sytolic_array', OrderedDict([('size', 512), ('area', 50000), ('latency', 300), ('energy', 2000)])), ('sram_memory', OrderedDict([('size', 25000), ('area', 100000), ('latency', 200), ('energy', 6000), ('connectivity', 300)])), ('dram_memory', OrderedDict([('size', 30000000), ('area', None), ('latency', 400), ('connectivity', 800)])), ('registerfile', OrderedDict([('size', None), ('area', None), ('latency', None), ('connectivity', None)]))])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'bandwidth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2a03703c532d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-b6d7ba7fd6da>\u001b[0m in \u001b[0;36mrunner\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScheduling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m# For the Forward Problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# executer = Mapper(opts, hwdesc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d1353deb4717>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Also check how to schedule next node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mread_access\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_access\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_bandwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmemory_bandwidth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mstep_cycles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_number_cycles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             logger.save(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'bandwidth'"
     ]
    }
   ],
   "source": [
    "runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit ('dl': conda)",
   "language": "python",
   "name": "python36464bitdlconda74ce50dd77ab450dad146f0ee6a8041f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
